[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Earth-Economy Modeling: Fine resolution ecosystems in general equilibrium",
    "section": "",
    "text": "Introduction\nThis is a a work in progress that combines code from many different aspects of my reserach, including from land-use change modeling (SEALS), ecosystem services modeling (InVEST), general equilibrium modeling (GTAP), APEC 8222 (Big Data for Economists), APEC 8601 (Natural Resource Economics) and several sources.\nIt will eventually grow to form the core textbook content for new iterations of theses courses as well as the documentation for the respective models."
  },
  {
    "objectID": "01_Python_Introduction/01_jupyter_intro.html#jupyter-has-two-kinds-of-cells",
    "href": "01_Python_Introduction/01_jupyter_intro.html#jupyter-has-two-kinds-of-cells",
    "title": "1  Jupyter Intro",
    "section": "1.1 Jupyter has two kinds of cells",
    "text": "1.1 Jupyter has two kinds of cells\n\nMarkdown cells (like this one)\nCode cells (like the next one)\n\nAbove the editor window here, you can click +Code or +Markdown to add new ones.\nAlternatively, right-click on a cell for more options (like splitting a cell into two)\nMarkdown cells are meant for formatted content, pictures, lecture notes etc. and follows the same notation as R-markdown.\nIf you want to edit a markdown cell, double left-click it. To finalize the edits, click the checkmark above (or type ctrl-enter)."
  },
  {
    "objectID": "01_Python_Introduction/01_jupyter_intro.html#code-runs-in-the-notebook.",
    "href": "01_Python_Introduction/01_jupyter_intro.html#code-runs-in-the-notebook.",
    "title": "1  Jupyter Intro",
    "section": "1.2 Code runs IN the notebook.",
    "text": "1.2 Code runs IN the notebook.\nSelect the python cell below. You can edit it freely. To run it, you can click the triangle button (“play button”) to the upper right of that cell. Alternatively, you can ctrl-enter.\n\na = 5\nb = 4\nsimple_summation = a + b\n\nYou know the cell has run successfully if it gets a green check at the bottom. Also notice that there is a number now in the [ ] box at the bottom left. This indicates which cell this was run in order of all the cells run.\nNotice that it doesn’t output anything, but note that the values are now stored in the Python Kernel (Jupyter Server) and are available to other parts of this notebook.\nIf you want to see a variable outputted, you can just type the variable name.\n\nsimple_summation\n\nYou can also use the print command, but this will supress non-printed variables.\n\nprint('Rounded: ', simple_summation)"
  },
  {
    "objectID": "01_Python_Introduction/01_jupyter_intro.html#order-matters",
    "href": "01_Python_Introduction/01_jupyter_intro.html#order-matters",
    "title": "1  Jupyter Intro",
    "section": "1.3 Order matters",
    "text": "1.3 Order matters\nThe second python cell would fail if the first one wasn’t run.\nYou can run the whole program via the double-play Run icon above."
  },
  {
    "objectID": "01_Python_Introduction/01_jupyter_intro.html#in-class-exercise-1.1",
    "href": "01_Python_Introduction/01_jupyter_intro.html#in-class-exercise-1.1",
    "title": "1  Jupyter Intro",
    "section": "1.4 In-class exercise 1.1",
    "text": "1.4 In-class exercise 1.1\nBelow, add two cells to this notebook.\nFirst, create a markdown cell where you have a header and some paragraph text. To make something a header in Markdown language, just use a hashtag and a space before the title. To make it a paragraph, just separate it with a blank line in between. Finally, add a bulletted list with a few entries. To do this, just have a dash at the beginning of each new bullet.\nSecond, create a python cell. Save a variable that is the sum of all primes between 3 and 10. Print that sum."
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#python-basics",
    "href": "01_Python_Introduction/02_python_basics.html#python-basics",
    "title": "2  Python Basics",
    "section": "2.1 Python Basics",
    "text": "2.1 Python Basics\n\n# Comments: The hashtag makes the rest of the line a comment. The more programming you do, the more you focus on making good comments.\n# Jupyter lets you write formatted text, but you'll still want to put comments in the raw python.\n\n# Assign some text (a string) to a variable\nsome_text = 'This is the text.'\n\n# Assign some numbers to variables\na = 5  # Here, we implicitly told python that a is an integer\nb = 4.6  # Here, we told python that b is a floating point number (a decimal)\n\n\nEven though nothing is outputted above, our Python “Kernel” has the values to each variable stored for later use.\n\n\n2.1.1 Important note: Python is not a “typed” language\n\nNotice that above, we added an integer and the float (a floating point number, i.e., one with a decimal point). Python “smartly” redefines variables so that they work together.\nThis is different from other languages which require you to manually manage the “types” of your variables.\n\n\n# Python as a calculator. \nsum_of_two_numbers = a + b\n\n# Printing output to the console\nprint('Our output was', sum_of_two_numbers)\n\n\nIn the above, you’ll notice the result was a float.\nIf needed, you can demand that python specify something as a certain type, as below.\n\n\nsum_as_int = int(sum_of_two_numbers)\nsum_as_int_back_to_float = float(sum_as_int)\n\nprint('We lost some precision in this operation:', sum_as_int_back_to_float)"
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#other-python-types",
    "href": "01_Python_Introduction/02_python_basics.html#other-python-types",
    "title": "2  Python Basics",
    "section": "2.2 Other python types",
    "text": "2.2 Other python types\n\n# Reminder, this assumes you have setup an envioronment with conda using:\nlist_1 = [4, 5, 6]\nprint('list_1', list_1)\n\n\n# You can embed lists in lists in lists, etc.\nlist_2 = [[5, 3, 5], [6, 6, 5]]\nprint(list_2)\n\n\n# Dictionaries\ndictionary_1 = {23: \"Favorite number\", 24: \"Second favorite number\"}\nprint('dictionary_1', dictionary_1)\n\n# Here is a multi line string: (also discusses improved capabilities of an IDE editor)\n\nthings_you_can_do_in_vs_code_that_you_cant_do_without_an_ide = \"\"\"\n1.) Move back and forth in your history of cursor positions (using your mouse forward and back buttons)\n2.) Edit on multiple lines at the same time (hold alt and click new spots)\n3.) Smartly paste DIFFERENT values\n4.) Duplicate lines (ctrl-d)\n5.) Introspection (e.g., jump between function definition and usages)\n6.) Debugging (Interactively walk through your code one line at a time)\n7.) Profiling your code (see which lines take the most time to compute.)\n8.) Keep track of a history of copy-paste items and paste from past copies. (ctrl-shift-v)\n\"\"\""
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#looping",
    "href": "01_Python_Introduction/02_python_basics.html#looping",
    "title": "2  Python Basics",
    "section": "2.3 Looping",
    "text": "2.3 Looping\n\n\n\nsmall_range = range(0, 10)\nprint('small_range:', small_range)\n\nsmall_range_as_list = list(range(0, 10))\nprint('small_range_as_list:', small_range_as_list)\n\n# Here is a for loop. Also note that python EXPLICITLY USES TAB-LEVEL to denote nested things.\n# I.e., the inner part of the loop is tabbed 1 level up. Python does not use { like  R.\n# I LOVE this notation and it's a big part of why python is so pretty and readable.\nsum = 0 # Set the initial variable values\nnum = 0\nsum_with_some = 0\nfor i in range(100, 136, 3):\n    sum = sum + i\n    num = num + 1\n\n    # loop within a loop\n    for j in range(200, 205):\n        sum_with_some = sum + j\n\nmean = sum / num\nprint('mean', mean)"
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#defining-functions",
    "href": "01_Python_Introduction/02_python_basics.html#defining-functions",
    "title": "2  Python Basics",
    "section": "2.4 Defining functions",
    "text": "2.4 Defining functions\n\n# Functions\ndef my_function(input_parameter_1, input_parameter_2):\n    product = input_parameter_1 * input_parameter_2\n    return product\n\n# Use the function\nvalue_returned = my_function(2, 7)\nprint(value_returned)\n\n\n# In-class exercise workspace"
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#importing-packages",
    "href": "01_Python_Introduction/02_python_basics.html#importing-packages",
    "title": "2  Python Basics",
    "section": "2.5 Importing packages",
    "text": "2.5 Importing packages\n\n\n# Built-in packages via the Python Standard Library\nimport math\nimport os, sys, time, random\n\n# Using imported modules\nnumber_rounded_down = math.floor(sum_of_two_numbers)\nprint(number_rounded_down)"
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#using-packages-from-elsewhere",
    "href": "01_Python_Introduction/02_python_basics.html#using-packages-from-elsewhere",
    "title": "2  Python Basics",
    "section": "2.6 Using packages from elsewhere",
    "text": "2.6 Using packages from elsewhere\nWhen we used Mambaforge, we installed a ton of packages. These were not “built-in” to python like the ones above. Here we will import them into our notebook to use.\nThis will also illustrate the use of numpy. We’ll use it so much we us the as code to name it something shorter.\n\nimport numpy as np # The as just defines a shorter name\n\n# Create an 2 by 3 array of integers\nsmall_array = np.array([[5, 3, 5], [6, 6, 5]])\n\nprint('Here\\'s a small numpy array\\n', small_array)\n\n# Sidenote: from above backspace \\ put in front of a character is the\n# \"escapce character,\" which makes python interpret the next thing as a string or special text operator. \\n makes a line break"
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#discussion-point",
    "href": "01_Python_Introduction/02_python_basics.html#discussion-point",
    "title": "2  Python Basics",
    "section": "2.7 Discussion point",
    "text": "2.7 Discussion point\nThe array above looks identical to the nested lists we made. It IS NOT! It is a numpy array that is ridiculously fast and can scale up to massive, massive data questions. The optional reading for today (Harris et al. 2020, Nature) discusses how these arrays have formed the backbone of modern scientific computing.\n\nlow = 3\nhigh = 8\nshape = (1000, 1000)\n\nsmallish_random_array = np.random.randint(low, high, shape)\n\nprint('Here\\'s a slightly larger numpy array\\n', smallish_random_array)"
  },
  {
    "objectID": "01_Python_Introduction/02_python_basics.html#in-class-exercise-2.1",
    "href": "01_Python_Introduction/02_python_basics.html#in-class-exercise-2.1",
    "title": "2  Python Basics",
    "section": "2.8 In-class exercise 2.1",
    "text": "2.8 In-class exercise 2.1\nParticipation points note! I will call on a random table to show me their answer via their table’s monitor.\nMake a function that returns the square of a number. Combine the function with a loop to calculate the Sum of Squared Numbers from 1 to 100.\nHINT, ** is the exponent operator in python.\nBONUS: Make sure you’re actually right by inserting a print statement in each step.\nBONUS-bonus: Store each stage of the results in a list using your_list = [] and your_list.append(thing_to_add_to_your_list)"
  },
  {
    "objectID": "02_Machine_Learning/01_linear_regression.html#in-class-exercise",
    "href": "02_Machine_Learning/01_linear_regression.html#in-class-exercise",
    "title": "3  Linear Regression",
    "section": "3.1 In-Class Exercise:",
    "text": "3.1 In-Class Exercise:\nPrint out the mean BMI in the dataset, the sum BMI, and the sum of the squared BMI values. Explain why the sum of the squared BMI is what it is. To do this, you will need to access the right parts of the data array and slice out the right column.\nHINT: You will need to read the DESCR to understand which column the BMI is stored in.\nHINT2: To create a new variable with just the desired column of the array, you can use Array slicing notation like a = data_array[:, n] where the : means you want ALL rows, and the n means you want just column n.\nHINT3: You may want to use np.sum(), np.mean(), and the ** exponent operator.\n\nbmi = data_array[:, 2]\nprint(bmi)\n\n\n# For conveinence, sklearn also just has an option to get the key parts for the regression ready to use.\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n\n\n# Look at diabetes_X and notice there are lots of independent variables. Rather than printing the whole\n# Array, which would be messy, just look at the .shape attribute.l\nprint('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\nFor now, we’re just going to use a single variable (a single column) for simplicity. The following line extracts just the second column from the array. The colon was necessary because we access arrays using the ROW, COLUMN notation, so we sliced out all ROWS (the colon indicates all) and the second COLUMN.\n\ndiabetes_X = diabetes_X[:, 2]\n# diabetes_X = np.array([diabetes_X])\n# diabetes_X = diabetes_X[:, np.newaxis, 2]\nprint('diabetes_X', diabetes_X.shape)\nprint('diabetes_X', diabetes_X)\n\n# diabetes_X = diabetes_X.reshape((:, 1))\ndiabetes_X = diabetes_X.reshape((diabetes_X.shape[0], 1))\n\nprint('diabetes_X', diabetes_X.shape)\nprint('diabetes_X', diabetes_X)\n\n\n3.1.1 Split into training and testing arrays (the manual way)\nNext we are going to do a very rudimentary split of the data into training and testing sets using array slice notation. The following lines assigns the last all but the last 20 lines to the TRAIN set and the remaining 20 to the test set.\n\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\nprint(diabetes_X_test)\n\nCreate an empty LinearRegression object.\nIn the lines below, we will follow a relatively standardized process for running a model:\n\nCreate the model object.\nFit the model.\nPredict with the model\n\nThe basic notation for sklearn below first creates a regression model object using the linear_model that we imported above. This model is “empty” in the sense that it has no coefficients identified. Just like othertimes we’ve encountered objects (like numpy array objects), this object has many functions (called methods) and attributes which can be accessed by the dot operator.\n\nregression_object = linear_model.LinearRegression()\nprint('regression_object', regression_object)\n\n\n3.1.1.1 Use the fit method\nUse the fit method from our regression object. It takes two inputs, the independent variables (X) and dependent variables (y).\nBelow, we will ONLY use the training subset of the data we created above.\n\nregression_object.fit(diabetes_X_train, diabetes_y_train)\nprint(regression_object)\n\n\n\n3.1.1.2 Use the fitted model to predict values\nNow the regression_object is “trained,” which means we can also call it’s predict() method which will take some other observations and (in the case of OLS), multiple the new observations against our trained coefficients to make a prediciton.\nThe predict method returned an array of numerical predictions, which we will look at.\n\ndiabetes_y_pred = regression_object.predict(diabetes_X_test)\nprint(diabetes_y_pred)\n\n\n\n3.1.1.3 Look at the coefficients\nMore interesting might be to look at the coefficients. Once the model has been fit, it has a new attribute .coef_ which stores an array of coefficients. In this case it will only be an array of length 1 because we just have one input.\n\nprint('Coefficients: \\n', regression_object.coef_)\n\nYou might be wondering why we are looking at the coefficients as a raw array rather than at a nicely formatted regression table. The reason is in cross-validation approaches, these coefficients might just be one step towards the final model performance check on unseen data.\n\n\n3.1.1.4 Evaluating the fit\nWe can use sklearn’s built in evaluation functions, such as for the mean squared error or other metrics.\n\nmse = mean_squared_error(diabetes_y_test, diabetes_y_pred)\nprint('Mean squared error on the TEST data:',  mse)\n\n\n\n# Or perhaps we want the r2 for the second independent variable (which is the only one we used)\nr2_score_value = r2_score(diabetes_y_test, diabetes_y_pred)\nprint('r2 calculated on TEST data: ', r2_score_value)\n\n\n# Finally, to prove to ourselves that we know what we are doing, let's plot this.\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()"
  },
  {
    "objectID": "02_Machine_Learning/01_linear_regression.html#exercise-2.1-machine-learning-ols-mashup.",
    "href": "02_Machine_Learning/01_linear_regression.html#exercise-2.1-machine-learning-ols-mashup.",
    "title": "3  Linear Regression",
    "section": "3.2 Exercise 2.1: Machine Learning OLS Mashup.",
    "text": "3.2 Exercise 2.1: Machine Learning OLS Mashup.\nUse loops to find which TWO variables best describe the data, as measured by R-squared. This is a hilariously brute-force approach to OLS model selection, but it is similar in some senses to Machine Learning and will be relevant to the cross-validation approaches we discuss next.\n\n# Exercise 2.1 workspace and starter code\n\n\nfull_dataset = datasets.load_diabetes() # Load the full dataset\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True) # Get just the data arrays\n\n# Split into training and testing\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\nhighest_score = 0\nfor i in range(len(full_dataset['feature_names'])):\n    for j in range(len(full_dataset['feature_names'])):\n        \n        diabetes_current_X_train = diabetes_X_train[:, [i, j]]\n        diabetes_current_X_test = diabetes_X_test[:, [i, j]]\n\n        # MISSING STUFF HERE.       \n        \n        if r2_score_value > highest_score:\n            highest_score = r2_score_value\n            best_option = [i, j, r2_score_value]\n        \nprint('best_option', best_option)"
  },
  {
    "objectID": "02_Machine_Learning/01_linear_regression.html#just-for-completeness-lets-look-at-this-the-way-an-econometritian-would",
    "href": "02_Machine_Learning/01_linear_regression.html#just-for-completeness-lets-look-at-this-the-way-an-econometritian-would",
    "title": "3  Linear Regression",
    "section": "3.3 Just for completeness, let’s look at this the way an econometritian would",
    "text": "3.3 Just for completeness, let’s look at this the way an econometritian would\nSklearn doesn’t report summary statistics in the classic, econometric sense because it focuses on the train, test paradigm, which is not equivilent to a model performance report (which in the classic case is only reporting performance of the TRAINING data).\nNonetheless, Here’s how I do it, using an alternative, more econometrics-focused package. You will need to conda install statsmodel if you want to uncomment this line and have it work. Note that because we’re not splitting our data into training and testing, the r-squareds are not really comparable.\n\nimport statsmodels\nfrom statsmodels.api import OLS\n\ndata_with_constant = statsmodels.api.add_constant(full_dataset.data)\nresult = OLS(full_dataset.target, data_with_constant).fit().summary()\nprint(result)"
  },
  {
    "objectID": "02_Machine_Learning/02_support_vector_machines.html",
    "href": "02_Machine_Learning/02_support_vector_machines.html",
    "title": "4  Support Vector Machines",
    "section": "",
    "text": "In this section, we will use what we learned about fitting models and apply it to a very useful machine-learning algorithm.\nFirst let’s start with imports.\n\nimport numpy as np\nimport scipy\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport os\n\n\n4.0.0.1 Load in some digit image data\nOne of the canonical datasets in sklearn is a series of images of handwritten digits. We’ve imported the datasets above, but now lets load it.\n\n\ndigits = datasets.load_digits()\n\n# First, take a look at the raw python object:\nprint('digits\\n', digits)\n\ndigits\n {'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n       ...,\n       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'frame': None, 'feature_names': ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7'], 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n        ...,\n        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n        ...,\n        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n\n       ...,\n\n       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n        ...,\n        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n\n       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n        ...,\n        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n\n       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n        ...,\n        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}\n\n\nNot super helpful unless you’re very good at reading python dictionary notation. Fortunately, one of the entries in this dataset is a description. Let’s read that.\n\n\nprint('DESCR\\n', digits['DESCR'])\n\nDESCR\n .. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n\n\n\n\n4.0.0.2 Extract one of the digits to inspect\nNow that we’re oriented, also look at one particular image of a digit, just so you know what it actually looks like. Below, we print just the first (index = 0) numeral of the 5620 they provide.\n\n\nprint('digits.images[0]\\n', digits.images[0])\n\ndigits.images[0]\n [[ 0.  0.  5. 13.  9.  1.  0.  0.]\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n\n\n\n\n# If you squint, maybe you can tel what image it is, but let's plot it to be sure.\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.imshow(digits.images[0])\nplt.show()\n\n\n\n\nNotice also in the dataset that there is a ‘targets’ attribute in the dataset. This is the correct numeral that we are trying to make the model predict.\n\nprint('target', digits.target)\n\ntarget [0 1 2 ... 8 9 8]\n\n\nOur task now is to train a model that inputs the digit images and predicts the digit numeral. For this, we’re going to use SVM, as discussed in lecture.\n\n\n4.0.0.3 Import SVM and create a new (unfitted) model with it.\nFor now, the parameters are going to be manually set (gamme) but we’ll address how to choose them later. Here, I want to illustrate the basic approach used in sklearn to Load, train, fit and predict the model\n\nfrom sklearn import svm\n\n# Create the model object\nclassifier = svm.SVC(gamma=0.001)\n\nAt this point, classifier is not yet “trained”, ie. not yet fit to the model. All ML algorithms in SKLEARN have a .fit() method, which we will use here, passing it the images and the targets.\nBefore we train it, we want to split the data into testing and training splits.\nClass question: Remind me WHY are we splitting it here? What is the bad thing that would happen if we just trained it on all of them?\nBefore we can even split the data, however, we need to reshape it to be in the way the regression model expects.\nIn particular, the SVM model needs a 1-dimensional, 64 element array. BUT, the input digits we saw were 2-dimensional, 8 by 8 arrays.\nThis actually leads to a somewhat mind-blown example of how computers “think” differently than we do. We clearly think about a numeral in 2 dimensional space, but here we see that the computer doesn’t are about the spatial relationship ship at all. It sees each individual pixel as it’s own “Feature” to use the classification parlance. You could even reshuffle the order of those 64 digits and as long as you kept it consistent across the data, it would result in identical predictions.\nLater on, we will talk about machine learning techniques that leverage rather than ignore this 2 dimensional, spatial nature of the data.\nFor now, let’s just look at the data again. Rather than print it out, I really just want the shape so that i don’t get inundated with text.\n\nprint('digits.images shape', digits.images.shape)\n\ndigits.images shape (1797, 8, 8)\n\n\n\n\nn_samples = len(digits.images)\nn_features = digits.images[0].size\n\nprint('n_samples', n_samples)\nprint('n_features', n_features)\n\ndata = digits.images.reshape((n_samples, n_features))\n\n# Now check the shame again to see that it's right.\nprint('data shape', data.shape)\n\nn_samples 1797\nn_features 64\ndata shape (1797, 64)\n\n\n\n\n# Now that we've arranged our data in this shape, we can split it into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)\n\nprint('X_train', X_train)\nprint('y_train', y_train)\n\nX_train [[ 0.  0.  5. ...  0.  0.  0.]\n [ 0.  0.  0. ... 10.  0.  0.]\n [ 0.  0.  0. ... 16.  9.  0.]\n ...\n [ 0.  0.  2. ... 14.  0.  0.]\n [ 0.  1. 12. ...  0.  0.  0.]\n [ 0.  0.  0. ...  3.  0.  0.]]\ny_train [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9\n 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\n 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2\n 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 3 1 3 9 1\n 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 5 4 8 8 4 9 0 8 9 8 0 1 2\n 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9\n 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8\n 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2\n 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0\n 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2\n 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7\n 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1\n 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8\n 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2\n 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7\n 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9\n 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1\n 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1\n 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0\n 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5\n 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4\n 7 2 8 2 2 5 7 9 5 4]\n\n\n\n\n4.0.0.4 Fit the model\nFinally, now that we’ve split it, we can call the classifier’s fit method which takes the TRAINING data as input.\n\nclassifier.fit(X_train, y_train)\n\nSVC(gamma=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma=0.001)\n\n\nNow, our classifier object has it’s internal parameters fit so that when we give it new input, it predicts what it thinks the correct classification is.\n\npredicted = classifier.predict(X_test)\n\n# Looking at the predicted won't be very intuitive, but you could glance.\nprint('predicted', predicted)\n\npredicted [8 8 4 9 0 8 9 8 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 9 6 7 8 9\n 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 9 1 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9\n 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1\n 7 5 4 4 7 2 8 2 2 5 7 9 5 4 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 7 8 2 0\n 1 2 6 3 3 7 3 3 4 6 6 6 9 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 5 4 6 3 1 7 9\n 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9\n 5 2 8 2 0 0 1 7 6 3 2 2 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 8 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 9 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\n 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2\n 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 6 2 8 3 0 0 1 7 6 3 2 1 7 4 6 3 1 3\n 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 0\n 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5\n 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4\n 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 1 7 8 9 0 1 2 3 4 5 6 9 0\n 1 2 3 4 5 6 7 8 9 4 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2\n 6 8 7 7 7 3 4 6 6 6 9 9 1 5 0 9 5 2 8 0 1 7 6 3 2 1 7 9 6 3 1 3 9 1 7 6 8\n 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 2 5 7 3 5 9 4 5 0 8 9 8 0 1 2 3 4 5 6 7\n 8 9 0 1 2 8 4 5 6 7 8 9 0 1 2 5 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7\n 7 5 1 0 0 2 2 7 8 2 0 1 2 6 8 8 7 5 8 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7\n 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 5 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7\n 9 5 4 8 8 4 9 0 8 9 8]\n\n\n\n\n4.0.0.5 Plot some results\nLet’s plot a few of them in nicer format. Don’t worry about learning the plotting code but it’s a useful example to show the power.\n\n\n_, axes = plt.subplots(2, 4)\nimages_and_labels = list(zip(digits.images, digits.target))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: %i' % label)\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Prediction: %i' % prediction)\nplt.show()\n\n\n\n\n\n\nfrom sklearn import metrics\n\nprint(\"Classification report:\\n\", metrics.classification_report(y_test, predicted))\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99        88\n           1       0.99      0.97      0.98        91\n           2       0.99      0.99      0.99        86\n           3       0.98      0.87      0.92        91\n           4       0.99      0.96      0.97        92\n           5       0.95      0.97      0.96        91\n           6       0.99      0.99      0.99        91\n           7       0.96      0.99      0.97        89\n           8       0.94      1.00      0.97        88\n           9       0.93      0.98      0.95        92\n\n    accuracy                           0.97       899\n   macro avg       0.97      0.97      0.97       899\nweighted avg       0.97      0.97      0.97       899\n\n\n\n\n\n4.0.0.6 Confusion matrix\nA more convenient way of looking at the results is t the confusion matrix. This is a built in metric for sklearn. It plots the predicted labels vs. the true labels.\n\ndisp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\ndisp.figure_.suptitle(\"Confusion Matrix\")\n\n# print(\"Confusion matrix:\\n\", disp.confusion_matrix)\n\n# Finally, show it so that you can look at it and see how good we did.\nplt.show()\n\nc:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\nQUESTION: Which digit was hardest to categorize and what was it most frequently confused as?"
  },
  {
    "objectID": "Appendices/hosting_notes.html#pasting-images",
    "href": "Appendices/hosting_notes.html#pasting-images",
    "title": "5  Hosting notes",
    "section": "5.1 Pasting images",
    "text": "5.1 Pasting images\nNOTE: This is easiest to do in ipynb NOT in qmd because of the built-in options for pasting images into notebooks.\nNote that there are two methods: paste (ctrl-v) directly into a markdown cell of a ipynb. This adds it as an “attachment” where the raw binary image code is written into text in the cell’s attribute (and thus there is no external file saved). The other is the use the Paste Image VS Code plug in, which lets you actually write a file in a gnerated location for the PNG. This is called by ctrl-alt-v\nOption 1\n\n\n\nimage.png\n\n\nOption 2"
  },
  {
    "objectID": "Appendices/quarto_notes.html",
    "href": "Appendices/quarto_notes.html",
    "title": "6  Cross References",
    "section": "",
    "text": "For tables produced by executable code cells, include a label with a tbl- prefix to make them cross-referenceable. For example:\n\nfrom IPython.display import Markdown\n# from tabulate import tabulate\n# table = [[\"Sun\",696000,1989100000],\n#          [\"Earth\",6371,5973.6],\n#          [\"Moon\",1737,73.5],\n#          [\"Mars\",3390,641.85]]\n# Markdown(tabulate(\n#   table, \n#   headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n# ))\n\n\n?(caption)"
  }
]