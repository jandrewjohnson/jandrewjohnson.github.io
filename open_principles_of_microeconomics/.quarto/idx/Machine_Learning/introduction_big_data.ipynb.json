{"title":"Machine Learning Intro","markdown":{"yaml":{"title":"Machine Learning Intro"},"headingText":"Cross-Validation, Python for Linear Regression, and Support Vector Machines","containsRefs":false,"markdown":"\n\n\n\n\n\n\n\n\n\n\n\nReminder as always: Pull the latest code (I probably made some minor updates)\n\n\n\nRelated fun OSS product of the day: QUARTO\\!\n\n\n\n# Syllabus check-in\n\n\n\n![](img\\\\1219.png)\n\n\n\nI've also added some optional journal articles to Canvas that exemplify the approaches we're using\n\n\n\n# Agenda\n\n\n\nFinal word on Git: pushing and pulling\\.\n\n\n\nDiscuss cross\\-validation \\(CV\\) vs\\. model performance\n\n\n\nIntroduce Scikit\\-learn via regular linear regression\\, use it to talk about CV\n\n\n\nIntroduce Support Vector Machines for classification\n\n\n\n# Git push and pull\n\n\n\n# What happens when I make a change and how does everyone stay synced.\n\n\n\n# Suppose I make an edit on my Workstation\n\n\n\nSuppose I ad some extremely important new code to our workbook from last lecture\\.\n\n\n\n![](img\\\\1220.png)\n\n\n\n# Git realizes that my file now doesn't match the repository\n\n\n\nFirst thing you'll notice is the file changes to brown/orange and has an \"M\" for modified by it\\.\n\n\n\n# How do I get this into the online repository?\n\n\n\n* Click on the Source Control tab and you will see this file listed in the \"Change List\"\n\n* Because I'm in charge of this repository\\, I want to \"commit\" this file and then \"push\" it to the repository\\.\n\n  * I type a commit message \\(REQUIRED and will silently fail if not\\)\\, and then select Commit and Push\\.\n\n* Now my Source Control tab is clean\\. That means my local code matches the remote repository\\.\n\n\n\n![](img\\\\1221.png)\n\n\n\n![](img\\\\1222.png)\n\n\n\n# But wait, what happens to other people who also have edited the file?\n\n\n\nGitting into a predicament\\.\n\n\n\n# Suppose your instructor says \"okay, now pull the latest code from the course repository�\"\n\n\n\n* If you do that\\, VS Code might scold you\\.\n\n* What does this mean?\n\n* It means YOU have changes on your computer that are different from what's on GitHub\\.\n\n  * Git can't pull because it doesn't know how to resolve the conflict\\.\n\n\n\n![](img\\\\1223.png)\n\n\n\n# Source control tab shows new code is available.\n\n\n\nGo to the Source Control tab\n\n\n\nIf your files are different than the repository\\, you will see those files listed here\\.\n\n\n\n# How do we resolve this \"merge conflict\"?\n\n\n\n![](img\\\\1224.png)\n\n\n\nIf you do not want to keep your changes \\(simplest case\\) we can discard them and then pull\\.\n\n\n\nTo do this\\, go to the Source Control tab and you will see the Change List\\.\n\n\n\nRight\\-click\\, and select discard changes\\.\n\n\n\nNow you can happily Git Pull\\.\n\n\n\n![](img\\\\1225.png)\n\n\n\n# You can also keep both sets of changes by \"merging\" them\n\n\n\n![](img\\\\1226.png)\n\n\n\nHere's an example where I might want to keep them\\.\n\n\n\nI clicked on the change list and opened it\\. You can see where I modified the In\\-class exercise\\.\n\n\n\nYou can research more about this on your own\\, but for now we're just going to avoid it\\.\n\n\n\n# I think the best way to do this is to just move this to your own folder outside the repository.\n\n\n\n![](img\\\\1227.png)\n\n\n\nThere you can preserve all your notes\\.\n\n\n\nOnce you moved the file \\(not copied\\)\\, Git Pull will succeed at getting the newest course code\\.\n\n\n\n# Cross-validation\n\n\n\nIn my opinion\\, this is the most important advance in all of Machine Learning from the perspective of an applied economist\\.\n\n\n\n# Cross-validation vs model performance\n\n\n\n* In either econometrics or ML\\, there are two tasks in building a model\n\n  * Estimating parameters of the model\n\n  * Evaluating how well that model does\n\n* Different approaches for these steps between Econometrics and ML:\n\n  * Econometrics the above steps involve:\n\n    * t\\- and p\\-statistics\\, hypothesis testing\\, analyzing specific coefficients\n\n    * R\\-values\\, AIC/BIC\\, etc\n\n  * In ML\\, the emphasis is different\\. The above steps in ML are:\n\n    * Mostly absent in isolation\\, but included in step 2\\.\n\n    * Determined by cross\\-validation of the model\\.\n\n\n\n# Returning to the complexity tradeoff.\n\n\n\n* Recall: Overfitting a model is making the model overly complex to that accuracy falls on the test data\\.\n\n  * We will talk about ways to methodologically hit the \"sweet spot\" of model complexity\\.\n\n* How do we find this sweet\\-spot? Cross validation\n\n* _First though\\, let's illustrate why overly\\-complex models can UNDER\\-perform\\._\n\n\n\n![](img\\\\1228.png)\n\n\n\nThe source of the \"sweet spot\" in model complexity\\.\n\n\n\n![](img\\\\1229.png)\n\n\n\nThe complex model is super accurate on the training data\\.\n\n\n\n![](img\\\\1230.png)\n\n\n\nWith new data\\, the complex model is much worse\\. Notice that the simple model performs about the same\\.\n\n\n\nImage source: statquest\\.org\n\n\n\n# Operationalizing Cross-Validation\n\n\n\n# Splitting data and Cross-Validation\n\n\n\n![](img\\\\1231.png)\n\n\n\n* In this course\\, we will use scikit\\-learn to illustrate this\\.\n\n* Scikit\\-learn has nice built\\-in functions to split our data into training and test data\\.\n\n  * This is the first step of cross\\-validation approaches\\.\n\n* We are going to set aside the data and make sure we never use it again until the very end\\.\n\n\n\n# We train the model on a second split of the data\n\n\n\n![](img\\\\1232.png)\n\n\n\n* To train our model\\, cross\\-validation creates a second split of the training data\\.\n\n  * ML algorithms will iteratively try different models/coefficients on this second spit\\, using whichever performs best on the training\\-test data\\.\n\n  * But we can do MORE than that\\!\n\n\n\n![](img\\\\1233.png)\n\n\n\n# Splitting into MANY Splits and Folds\n\n\n\n![](img\\\\1234.png)\n\n\n\n# Final model performance analysis\n\n\n\n![](img\\\\1235.png)\n\n\n\n* Once the best set of parameters are found\\, the model is compared against the test data from the first split\\.\n\n* Final performance assessment then is done with calculating the MSE of the model prediction of Test\\_X for Test\\_Y\n\n* This method is SUPER FLEXIBLE\n\n  * It could compare totally dissimilar models in a rigorous way\\.\n\n  * Will helps us choose \"tuning\\-\" or \"hyper\\-parameters\"\n\n\n\n# Switch to VS Code\n\n\n\nOpen Lectures\\\\02\\_Machine\\_Learning\\\\01\\_Linear\\_Regression\\.ipynb\n\n\n\n# Support Vector Machines\n\n\n\n# Support vector machines\n\n\n\nMotivation: classify inputs into categories\n\n\n\nApproach: draw a line \\(hyperplane\\) separating observations from different categories\n\n\n\nIssue 1: Many lines might work\\. Which one should we choose?\n\n\n\nIssue 1: Many lines might work\\. Which one should we choose?\n\n\n\nA: Pick the line with the largest 'margin' � distance to nearest points on either side\n\n\n\n# SVMs: naming digression\n\n\n\nThose nearest points determine the 'support vectors'\\. I think of it as a little person heroically holding up the margin lines\\.\n\n\n\n![](img\\\\1236.png)\n\n\n\n![](img\\\\1237.png)\n\n\n\n# SVM: math preliminaries\n\n\n\n![](img\\\\1238.png)\n\n\n\n# SVM: objective\n\n\n\n# Support vector machines\n\n\n\nBut\\, maybe no lines work perfectly\\. What can we do?\n\n\n\n# SVM: objective\n\n\n\nwhere add some sort of penalty on misclassifications\\.\n\n\n\nMany forms of this penalty term are possible\\. Here is the simplest one that just says limit sum of misclassification to be below some threshold gamma\\.\n\n\n\nYou might be wondering\\, wouldn't this all depend on the gamma value?\n\n\n\nYes it does\\. And we will use Cross\\-validation to find the best value for this \"hyperparameter\"\\.\n\n\n\n# We choose gamma via CV!\n\n\n\n![](img\\\\1239.png)\n\n\n\n* Iteratively try all values of gamma\\.\n\n* Whichever one predicts best across the many splits is what we will use\\.\n\n* Thus\\, we have systematically determined exactly how many outliers we  __should__  ignore\n\n  * From the perspective of out\\-of\\-sample performance\\.\n\n\n\n# Switch to VS Code\n\n\n\nOpen Lectures\\\\02\\_Machine\\_Learning\\\\02\\_SVM\\.ipynb\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\n\n\n# Cross-Validation, Python for Linear Regression, and Support Vector Machines\n\n\n\nReminder as always: Pull the latest code (I probably made some minor updates)\n\n\n\nRelated fun OSS product of the day: QUARTO\\!\n\n\n\n# Syllabus check-in\n\n\n\n![](img\\\\1219.png)\n\n\n\nI've also added some optional journal articles to Canvas that exemplify the approaches we're using\n\n\n\n# Agenda\n\n\n\nFinal word on Git: pushing and pulling\\.\n\n\n\nDiscuss cross\\-validation \\(CV\\) vs\\. model performance\n\n\n\nIntroduce Scikit\\-learn via regular linear regression\\, use it to talk about CV\n\n\n\nIntroduce Support Vector Machines for classification\n\n\n\n# Git push and pull\n\n\n\n# What happens when I make a change and how does everyone stay synced.\n\n\n\n# Suppose I make an edit on my Workstation\n\n\n\nSuppose I ad some extremely important new code to our workbook from last lecture\\.\n\n\n\n![](img\\\\1220.png)\n\n\n\n# Git realizes that my file now doesn't match the repository\n\n\n\nFirst thing you'll notice is the file changes to brown/orange and has an \"M\" for modified by it\\.\n\n\n\n# How do I get this into the online repository?\n\n\n\n* Click on the Source Control tab and you will see this file listed in the \"Change List\"\n\n* Because I'm in charge of this repository\\, I want to \"commit\" this file and then \"push\" it to the repository\\.\n\n  * I type a commit message \\(REQUIRED and will silently fail if not\\)\\, and then select Commit and Push\\.\n\n* Now my Source Control tab is clean\\. That means my local code matches the remote repository\\.\n\n\n\n![](img\\\\1221.png)\n\n\n\n![](img\\\\1222.png)\n\n\n\n# But wait, what happens to other people who also have edited the file?\n\n\n\nGitting into a predicament\\.\n\n\n\n# Suppose your instructor says \"okay, now pull the latest code from the course repository�\"\n\n\n\n* If you do that\\, VS Code might scold you\\.\n\n* What does this mean?\n\n* It means YOU have changes on your computer that are different from what's on GitHub\\.\n\n  * Git can't pull because it doesn't know how to resolve the conflict\\.\n\n\n\n![](img\\\\1223.png)\n\n\n\n# Source control tab shows new code is available.\n\n\n\nGo to the Source Control tab\n\n\n\nIf your files are different than the repository\\, you will see those files listed here\\.\n\n\n\n# How do we resolve this \"merge conflict\"?\n\n\n\n![](img\\\\1224.png)\n\n\n\nIf you do not want to keep your changes \\(simplest case\\) we can discard them and then pull\\.\n\n\n\nTo do this\\, go to the Source Control tab and you will see the Change List\\.\n\n\n\nRight\\-click\\, and select discard changes\\.\n\n\n\nNow you can happily Git Pull\\.\n\n\n\n![](img\\\\1225.png)\n\n\n\n# You can also keep both sets of changes by \"merging\" them\n\n\n\n![](img\\\\1226.png)\n\n\n\nHere's an example where I might want to keep them\\.\n\n\n\nI clicked on the change list and opened it\\. You can see where I modified the In\\-class exercise\\.\n\n\n\nYou can research more about this on your own\\, but for now we're just going to avoid it\\.\n\n\n\n# I think the best way to do this is to just move this to your own folder outside the repository.\n\n\n\n![](img\\\\1227.png)\n\n\n\nThere you can preserve all your notes\\.\n\n\n\nOnce you moved the file \\(not copied\\)\\, Git Pull will succeed at getting the newest course code\\.\n\n\n\n# Cross-validation\n\n\n\nIn my opinion\\, this is the most important advance in all of Machine Learning from the perspective of an applied economist\\.\n\n\n\n# Cross-validation vs model performance\n\n\n\n* In either econometrics or ML\\, there are two tasks in building a model\n\n  * Estimating parameters of the model\n\n  * Evaluating how well that model does\n\n* Different approaches for these steps between Econometrics and ML:\n\n  * Econometrics the above steps involve:\n\n    * t\\- and p\\-statistics\\, hypothesis testing\\, analyzing specific coefficients\n\n    * R\\-values\\, AIC/BIC\\, etc\n\n  * In ML\\, the emphasis is different\\. The above steps in ML are:\n\n    * Mostly absent in isolation\\, but included in step 2\\.\n\n    * Determined by cross\\-validation of the model\\.\n\n\n\n# Returning to the complexity tradeoff.\n\n\n\n* Recall: Overfitting a model is making the model overly complex to that accuracy falls on the test data\\.\n\n  * We will talk about ways to methodologically hit the \"sweet spot\" of model complexity\\.\n\n* How do we find this sweet\\-spot? Cross validation\n\n* _First though\\, let's illustrate why overly\\-complex models can UNDER\\-perform\\._\n\n\n\n![](img\\\\1228.png)\n\n\n\nThe source of the \"sweet spot\" in model complexity\\.\n\n\n\n![](img\\\\1229.png)\n\n\n\nThe complex model is super accurate on the training data\\.\n\n\n\n![](img\\\\1230.png)\n\n\n\nWith new data\\, the complex model is much worse\\. Notice that the simple model performs about the same\\.\n\n\n\nImage source: statquest\\.org\n\n\n\n# Operationalizing Cross-Validation\n\n\n\n# Splitting data and Cross-Validation\n\n\n\n![](img\\\\1231.png)\n\n\n\n* In this course\\, we will use scikit\\-learn to illustrate this\\.\n\n* Scikit\\-learn has nice built\\-in functions to split our data into training and test data\\.\n\n  * This is the first step of cross\\-validation approaches\\.\n\n* We are going to set aside the data and make sure we never use it again until the very end\\.\n\n\n\n# We train the model on a second split of the data\n\n\n\n![](img\\\\1232.png)\n\n\n\n* To train our model\\, cross\\-validation creates a second split of the training data\\.\n\n  * ML algorithms will iteratively try different models/coefficients on this second spit\\, using whichever performs best on the training\\-test data\\.\n\n  * But we can do MORE than that\\!\n\n\n\n![](img\\\\1233.png)\n\n\n\n# Splitting into MANY Splits and Folds\n\n\n\n![](img\\\\1234.png)\n\n\n\n# Final model performance analysis\n\n\n\n![](img\\\\1235.png)\n\n\n\n* Once the best set of parameters are found\\, the model is compared against the test data from the first split\\.\n\n* Final performance assessment then is done with calculating the MSE of the model prediction of Test\\_X for Test\\_Y\n\n* This method is SUPER FLEXIBLE\n\n  * It could compare totally dissimilar models in a rigorous way\\.\n\n  * Will helps us choose \"tuning\\-\" or \"hyper\\-parameters\"\n\n\n\n# Switch to VS Code\n\n\n\nOpen Lectures\\\\02\\_Machine\\_Learning\\\\01\\_Linear\\_Regression\\.ipynb\n\n\n\n# Support Vector Machines\n\n\n\n# Support vector machines\n\n\n\nMotivation: classify inputs into categories\n\n\n\nApproach: draw a line \\(hyperplane\\) separating observations from different categories\n\n\n\nIssue 1: Many lines might work\\. Which one should we choose?\n\n\n\nIssue 1: Many lines might work\\. Which one should we choose?\n\n\n\nA: Pick the line with the largest 'margin' � distance to nearest points on either side\n\n\n\n# SVMs: naming digression\n\n\n\nThose nearest points determine the 'support vectors'\\. I think of it as a little person heroically holding up the margin lines\\.\n\n\n\n![](img\\\\1236.png)\n\n\n\n![](img\\\\1237.png)\n\n\n\n# SVM: math preliminaries\n\n\n\n![](img\\\\1238.png)\n\n\n\n# SVM: objective\n\n\n\n# Support vector machines\n\n\n\nBut\\, maybe no lines work perfectly\\. What can we do?\n\n\n\n# SVM: objective\n\n\n\nwhere add some sort of penalty on misclassifications\\.\n\n\n\nMany forms of this penalty term are possible\\. Here is the simplest one that just says limit sum of misclassification to be below some threshold gamma\\.\n\n\n\nYou might be wondering\\, wouldn't this all depend on the gamma value?\n\n\n\nYes it does\\. And we will use Cross\\-validation to find the best value for this \"hyperparameter\"\\.\n\n\n\n# We choose gamma via CV!\n\n\n\n![](img\\\\1239.png)\n\n\n\n* Iteratively try all values of gamma\\.\n\n* Whichever one predicts best across the many splits is what we will use\\.\n\n* Thus\\, we have systematically determined exactly how many outliers we  __should__  ignore\n\n  * From the perspective of out\\-of\\-sample performance\\.\n\n\n\n# Switch to VS Code\n\n\n\nOpen Lectures\\\\02\\_Machine\\_Learning\\\\02\\_SVM\\.ipynb\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"introduction_big_data.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","bibliography":["../references.bib"],"theme":"cosmo","title":"Machine Learning Intro"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}