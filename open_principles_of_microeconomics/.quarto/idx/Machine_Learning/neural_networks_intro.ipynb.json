{"title":"Neural networks","markdown":{"yaml":{"title":"Neural networks"},"headingText":"Logistical comments and agenda","containsRefs":false,"markdown":"\n\n\n\n\n\n\n\nMulti\\-layer perceptrons and convolutional neural nets\\.\n\n\n\n![](img\\\\1254.png)\n\n\n\n\n\n\n* After today\\, Ali will resume lectures for 2 more days\n\n* Agenda:\n\n  * Neural\\-nets example\n\n  * NN theory\n\n  * NN in python\n\n\n\n# \n\n\n\nhttps://www\\.youtube\\.com/watch?v=aQwqD5cB2ck\n\n\n\n# What is this magic?\n\n\n\n![](img\\\\1255.png)\n\n\n\nBut first\\, a historical comic\\.\n\n\n\nThis comic is > 5 years old and the research team was successful\\.\n\n\n\n# Image analysis: from SVMs to Neural Nets to CONVOLUTIONAL Neural Nets\n\n\n\n![](img\\\\1256.png)\n\n\n\n* Recall that we used Support Vector Machines to categorize these digits\\.\n\n  * These were 8x8 images\\, but we flattened them into 64 independent features\\.\n\n  * We've also made great progress using Neural Nets to expand beyond just SVM\\.\n\n\n\nConvolutional Neural Net\\, conceptual diagram:\n\n\n\n![](img\\\\1257.png)\n\n\n\n# Neurons - Introduction\n\n\n\n* Our brain makes models of problems through networks of neurons\\.\n\n  * Neuroscience in 6 words: \"Neurons that fire together wire together\\.\"\n\n* Each network takes a set of inputs and fires under certain conditions\\.\n\n  * What if we mimic that process in ML?\n\n\n\n# Neural networks for regression\n\n\n\nConnect multiple layers of neurons\\. Often the covariates are considered an input 'layer\\.'\n\n\n\nDeterministic function\n\n\n\n# Neural networks: learning\n\n\n\n# Graphical mapping of the functions\n\n\n\n* Let's start with an example of relating dosage to efficacy\n\n* We define a simple NN with 1 hidden layer and two perceptrons\\.\n\n  * Each perceptron is defined as:\n\n\n\n![](img\\\\1258.png)\n\n\n\n![](img\\\\1259.png)\n\n\n\n# How is this fit? Start with some random choices and iterate via CV to find the best coefficients\n\n\n\n![](img\\\\1260.png)\n\n\n\n![](img\\\\1261.png)\n\n\n\n# More specifically how is this fit? Back propagation (sec 11.4 in text)\n\n\n\n# Intuitively, what is gradient descent?\n\n\n\n* We have a multidimensional parameter space\\. Calculate where the slope downwards is the steepest\\.\n\n  * Traverse part of the way down that slope\\, then recalculate derivatives\\.\n\n  * Iteratively do this until you find the bottom\\.\n\n\n\n![](img\\\\1262.png)\n\n\n\n# Simplified example of a neural net\n\n\n\n![](img\\\\1263.png)\n\n\n\n![](img\\\\1264.png)\n\n\n\n![](img\\\\1265.png)\n\n\n\nOnce trained\\, the neural net basically \"combines squiggles\" by adding up the fitted curves\n\n\n\n# More complexity can be added to improve predictions\n\n\n\n![](img\\\\1266.png)\n\n\n\n# Code Example: Analyzing imagery features\n\n\n\nBased on 500\\+ images of potentially malignant tumors\\, extracted variables such as radius\\, concavity\\, spacing\\, etc\\.\n\n\n\nCompared this to a medical test that identified if the tumor was in fact malignant\\.\n\n\n\nCODE TIME: Switch to VS Code\\.\n\n\n\n![](img\\\\1267.png)\n\n\n\n# What did we just do?\n\n\n\n![](img\\\\1268.png)\n\n\n\nThe image below is the coefficients of the neurons in our network for different variables \\(vertical\\) and network layers \\(horizontal\\)\n\n\n\n![](img\\\\1269.png)\n\n\n\n# Neural networks: challenges\n\n\n\n* Throwing a bunch of nodes in layers won't necessarily work well:\n\n  * If you have lots of layers\\, it will take a long time to train\n\n  * Simply adding more nodes doesn't always increase predictive accuracy\n\n* Use problem knowledge to intelligently construct structure of network\n\n  * What about problems that require very large data?\n\n    * Maybe we want to impose the same weights in different parts e\\.g\\. \"Convolutional Neural Networks\"\n\n\n\n# Convolutional Neural Nets\n\n\n\n# In our SVM, we were \"throwing away\" the spatial information by flattening it.\n\n\n\n![](img\\\\1270.png)\n\n\n\nWhat if we try to keep that spatial information? Neural Nets apply can do this\\!\n\n\n\nHowever\\, traditional Neural Nets like we've just discussed in Code may  break down under the huge information of images\\.\n\n\n\n# Image analysis and CNNs.\n\n\n\nImagine an input image\\, where we're trying to classify the image \\(rather than a pixel\\)\\.\n\n\n\nSay we have 3 bands for that image\\, and the image is 100x100 pixels\\. That's 30\\,000 inputs\\.\n\n\n\nSuppose we have one hidden layer with 10 neurons\\.\n\n\n\nIf we use a fully connected network \\(like in the code we just made\\)\\, we would have 300\\,000 weights and 10 offsets to learn for that hidden layer\\. Too many\\.\n\n\n\n![](img\\\\1271.png)\n\n\n\n# What are convolutions?\n\n\n\n![](img\\\\1272.gif)\n\n\n\n![](img\\\\1273.png)\n\n\n\nhttps://miro\\.medium\\.com/max/2400/1\\*ciDgQEjViWLnCbmX\\-EeSrA\\.gif\n\n\n\n# Layer types in CNNs\n\n\n\n* Convolution layer:\n\n  * Construct one or more \"filters\" that get passed over the pixel\n\n  * Each filter has a window of say 5x5 pixels\\. It moves with some \"stride\" length over the image\\.\n\n  * Need to chose activation function \\(sign\\, linear\\, logistic\\)\n\n\n\n# Fitting convolutions into neural nets\n\n\n\nNow perceptrons have convolution kernels as their definition\n\n\n\nSame back\\-propagation gradient descent method works to solve this\n\n\n\n![](img\\\\1274.png)\n\n\n\n![](img\\\\1275.png)\n\n\n\n![](img\\\\1276.png)\n\n\n\n![](img\\\\1277.png)\n\n\n\n![](img\\\\1278.png)\n\n\n\n![](img\\\\1279.png)\n\n\n\n![](img\\\\1280.png)\n\n\n\n![](img\\\\1281.png)\n\n\n\n![](img\\\\1282.png)\n\n\n\n![](img\\\\1283.png)\n\n\n\n![](img\\\\1284.png)\n\n\n\n![](img\\\\1285.png)\n\n\n\n# Layer types in CNNs\n\n\n\n![](img\\\\1286.jpg)\n\n\n\nThe convolutions can also be added to the rest of a fully\\-connected Neural Net\n\n\n\n# Research advance I'm currently working on: Creating \"reduced-form\" spatial regression that can work on billion+ observations.\n\n\n\n* Gridded data preserve spatial structure\n\n  * Nearby cells are highly correlated\n\n  * The actual pattern may be a good predictor\n\n* Can use 2\\-dimensional convolutions to express this structure\n\n  * E\\.g\\.\\, identify what is the relationship between two variables as their distance increases via a flexible parametric form\n\n\n\n__B\\. __ Expression in 2\\- and 3\\-dimensions\n\n\n\n__A\\. __ Example 1\\-dimensional adjacency relationships\n\n\n\n![](img\\\\1287.png)\n\n\n\nThis is one of the parametric relationships we solve for\n\n\n\n![](img\\\\1288.png)\n\n\n\n# Gaussian kernel\n\n\n\nSize = 21\\, Sigma 4\n\n\n\nApply this definition of spatial effect to each of the different land\\-use classes\\.\n\n\n\n![](img\\\\1289.png)\n\n\n\n# Preliminary results\n\n\n\n* Greatly outperforms the look\\-up table approach \\(IPCC method\\)\n\n* Edge effects definitely exist\n\n  * But still working to identify exact structure\n\n\n\n# Appendix\n\n\n\nAdditional slides on own research\n\n\n\n![](img\\\\1290.png)\n\n\n\n# Spatial results\n\n\n\n* Increases precision over linear model\n\n* Can test specific edge\\-type hypotheses\n\n  * Note that marginal impact analysis of edge type is tricky\n\n  * Note also these use the LassoLarsCV method\n\n* Preliminary conclusions:\n\n  * Intensive crops \\(class 10\\) have negative impact on carbon\\.\n\n  * Non\\-fragmented forests \\(class 50\\) has positive impact\\.\n\n  * Cities \\(190\\) have negative impact\\.\n\n\n\n# Linear model\n\n\n\n![](img\\\\1291.png)\n\n\n\n# Spatial model\n\n\n\n![](img\\\\1292.png)\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\n\nMulti\\-layer perceptrons and convolutional neural nets\\.\n\n\n\n![](img\\\\1254.png)\n\n\n\n# Logistical comments and agenda\n\n\n\n* After today\\, Ali will resume lectures for 2 more days\n\n* Agenda:\n\n  * Neural\\-nets example\n\n  * NN theory\n\n  * NN in python\n\n\n\n# \n\n\n\nhttps://www\\.youtube\\.com/watch?v=aQwqD5cB2ck\n\n\n\n# What is this magic?\n\n\n\n![](img\\\\1255.png)\n\n\n\nBut first\\, a historical comic\\.\n\n\n\nThis comic is > 5 years old and the research team was successful\\.\n\n\n\n# Image analysis: from SVMs to Neural Nets to CONVOLUTIONAL Neural Nets\n\n\n\n![](img\\\\1256.png)\n\n\n\n* Recall that we used Support Vector Machines to categorize these digits\\.\n\n  * These were 8x8 images\\, but we flattened them into 64 independent features\\.\n\n  * We've also made great progress using Neural Nets to expand beyond just SVM\\.\n\n\n\nConvolutional Neural Net\\, conceptual diagram:\n\n\n\n![](img\\\\1257.png)\n\n\n\n# Neurons - Introduction\n\n\n\n* Our brain makes models of problems through networks of neurons\\.\n\n  * Neuroscience in 6 words: \"Neurons that fire together wire together\\.\"\n\n* Each network takes a set of inputs and fires under certain conditions\\.\n\n  * What if we mimic that process in ML?\n\n\n\n# Neural networks for regression\n\n\n\nConnect multiple layers of neurons\\. Often the covariates are considered an input 'layer\\.'\n\n\n\nDeterministic function\n\n\n\n# Neural networks: learning\n\n\n\n# Graphical mapping of the functions\n\n\n\n* Let's start with an example of relating dosage to efficacy\n\n* We define a simple NN with 1 hidden layer and two perceptrons\\.\n\n  * Each perceptron is defined as:\n\n\n\n![](img\\\\1258.png)\n\n\n\n![](img\\\\1259.png)\n\n\n\n# How is this fit? Start with some random choices and iterate via CV to find the best coefficients\n\n\n\n![](img\\\\1260.png)\n\n\n\n![](img\\\\1261.png)\n\n\n\n# More specifically how is this fit? Back propagation (sec 11.4 in text)\n\n\n\n# Intuitively, what is gradient descent?\n\n\n\n* We have a multidimensional parameter space\\. Calculate where the slope downwards is the steepest\\.\n\n  * Traverse part of the way down that slope\\, then recalculate derivatives\\.\n\n  * Iteratively do this until you find the bottom\\.\n\n\n\n![](img\\\\1262.png)\n\n\n\n# Simplified example of a neural net\n\n\n\n![](img\\\\1263.png)\n\n\n\n![](img\\\\1264.png)\n\n\n\n![](img\\\\1265.png)\n\n\n\nOnce trained\\, the neural net basically \"combines squiggles\" by adding up the fitted curves\n\n\n\n# More complexity can be added to improve predictions\n\n\n\n![](img\\\\1266.png)\n\n\n\n# Code Example: Analyzing imagery features\n\n\n\nBased on 500\\+ images of potentially malignant tumors\\, extracted variables such as radius\\, concavity\\, spacing\\, etc\\.\n\n\n\nCompared this to a medical test that identified if the tumor was in fact malignant\\.\n\n\n\nCODE TIME: Switch to VS Code\\.\n\n\n\n![](img\\\\1267.png)\n\n\n\n# What did we just do?\n\n\n\n![](img\\\\1268.png)\n\n\n\nThe image below is the coefficients of the neurons in our network for different variables \\(vertical\\) and network layers \\(horizontal\\)\n\n\n\n![](img\\\\1269.png)\n\n\n\n# Neural networks: challenges\n\n\n\n* Throwing a bunch of nodes in layers won't necessarily work well:\n\n  * If you have lots of layers\\, it will take a long time to train\n\n  * Simply adding more nodes doesn't always increase predictive accuracy\n\n* Use problem knowledge to intelligently construct structure of network\n\n  * What about problems that require very large data?\n\n    * Maybe we want to impose the same weights in different parts e\\.g\\. \"Convolutional Neural Networks\"\n\n\n\n# Convolutional Neural Nets\n\n\n\n# In our SVM, we were \"throwing away\" the spatial information by flattening it.\n\n\n\n![](img\\\\1270.png)\n\n\n\nWhat if we try to keep that spatial information? Neural Nets apply can do this\\!\n\n\n\nHowever\\, traditional Neural Nets like we've just discussed in Code may  break down under the huge information of images\\.\n\n\n\n# Image analysis and CNNs.\n\n\n\nImagine an input image\\, where we're trying to classify the image \\(rather than a pixel\\)\\.\n\n\n\nSay we have 3 bands for that image\\, and the image is 100x100 pixels\\. That's 30\\,000 inputs\\.\n\n\n\nSuppose we have one hidden layer with 10 neurons\\.\n\n\n\nIf we use a fully connected network \\(like in the code we just made\\)\\, we would have 300\\,000 weights and 10 offsets to learn for that hidden layer\\. Too many\\.\n\n\n\n![](img\\\\1271.png)\n\n\n\n# What are convolutions?\n\n\n\n![](img\\\\1272.gif)\n\n\n\n![](img\\\\1273.png)\n\n\n\nhttps://miro\\.medium\\.com/max/2400/1\\*ciDgQEjViWLnCbmX\\-EeSrA\\.gif\n\n\n\n# Layer types in CNNs\n\n\n\n* Convolution layer:\n\n  * Construct one or more \"filters\" that get passed over the pixel\n\n  * Each filter has a window of say 5x5 pixels\\. It moves with some \"stride\" length over the image\\.\n\n  * Need to chose activation function \\(sign\\, linear\\, logistic\\)\n\n\n\n# Fitting convolutions into neural nets\n\n\n\nNow perceptrons have convolution kernels as their definition\n\n\n\nSame back\\-propagation gradient descent method works to solve this\n\n\n\n![](img\\\\1274.png)\n\n\n\n![](img\\\\1275.png)\n\n\n\n![](img\\\\1276.png)\n\n\n\n![](img\\\\1277.png)\n\n\n\n![](img\\\\1278.png)\n\n\n\n![](img\\\\1279.png)\n\n\n\n![](img\\\\1280.png)\n\n\n\n![](img\\\\1281.png)\n\n\n\n![](img\\\\1282.png)\n\n\n\n![](img\\\\1283.png)\n\n\n\n![](img\\\\1284.png)\n\n\n\n![](img\\\\1285.png)\n\n\n\n# Layer types in CNNs\n\n\n\n![](img\\\\1286.jpg)\n\n\n\nThe convolutions can also be added to the rest of a fully\\-connected Neural Net\n\n\n\n# Research advance I'm currently working on: Creating \"reduced-form\" spatial regression that can work on billion+ observations.\n\n\n\n* Gridded data preserve spatial structure\n\n  * Nearby cells are highly correlated\n\n  * The actual pattern may be a good predictor\n\n* Can use 2\\-dimensional convolutions to express this structure\n\n  * E\\.g\\.\\, identify what is the relationship between two variables as their distance increases via a flexible parametric form\n\n\n\n__B\\. __ Expression in 2\\- and 3\\-dimensions\n\n\n\n__A\\. __ Example 1\\-dimensional adjacency relationships\n\n\n\n![](img\\\\1287.png)\n\n\n\nThis is one of the parametric relationships we solve for\n\n\n\n![](img\\\\1288.png)\n\n\n\n# Gaussian kernel\n\n\n\nSize = 21\\, Sigma 4\n\n\n\nApply this definition of spatial effect to each of the different land\\-use classes\\.\n\n\n\n![](img\\\\1289.png)\n\n\n\n# Preliminary results\n\n\n\n* Greatly outperforms the look\\-up table approach \\(IPCC method\\)\n\n* Edge effects definitely exist\n\n  * But still working to identify exact structure\n\n\n\n# Appendix\n\n\n\nAdditional slides on own research\n\n\n\n![](img\\\\1290.png)\n\n\n\n# Spatial results\n\n\n\n* Increases precision over linear model\n\n* Can test specific edge\\-type hypotheses\n\n  * Note that marginal impact analysis of edge type is tricky\n\n  * Note also these use the LassoLarsCV method\n\n* Preliminary conclusions:\n\n  * Intensive crops \\(class 10\\) have negative impact on carbon\\.\n\n  * Non\\-fragmented forests \\(class 50\\) has positive impact\\.\n\n  * Cities \\(190\\) have negative impact\\.\n\n\n\n# Linear model\n\n\n\n![](img\\\\1291.png)\n\n\n\n# Spatial model\n\n\n\n![](img\\\\1292.png)\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"neural_networks_intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","bibliography":["../references.bib"],"theme":"cosmo","title":"Neural networks"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}