{"title":"Regularization with Lasso","markdown":{"yaml":{"title":"Regularization with Lasso"},"headingText":"Automatic feature selection with LASSO regression","containsRefs":false,"markdown":"\n\n\n\n\n\nIn this notebook we will learn how LASSO (Least Absolute Shrinkage and Selection Operator) regression works and how it can assist in automatically selecting which variables should be included using a **Cross-Validation** perspective.\n\n#### Start by importing packages\n\n\n\n#### Load dataset and inspect it\n\nAgain we're going to use our diabetes dataset. Inspect it again just to remind yourself\nwhat is in it.\n\n#### Select subset of data\n\nTo speed up calculation, we're going to just use the first 150 observations\nusing numpy slice notation to grab them out of the X, y\n\n#### Run OLS first (for comparison)\n\nRemember the standard Sklearn model steps:\n\n1. create the model object\n2. call the object's fit method.\n3. use the fitted model to predict something.\n4. assess the predictions.\n\n#### Do it again in the econometrics style\n\nRecall that the package statsmodels is closer to the econometrician's way of doing things. We're going to quickly repeat the steps above but with Statsmodels so we can view it in a nice table form.\n\n#### Plot y and y_hat\n\nLet's also plot y and y_hat compared to one of the most important variables, BMI. We'll see both y and y_hat resemble each other.\n\n## Switch to LASSO\n\nNow that we've spent all this time setting up our python environment and getting sklearn, it's almost a trivial step in many cases to try out the latest-and-greatest model.\n\n#### Create a LASSO model object\n\nToday's goal, however, is to do Lasso on this same dataset.\nTo start, lets create a Lasso object. Notice that we are not\nsetting the alpha/gamma value when we create it.\n\n#### Fit the LASSO\n\nCall the lasso.fit() method. \n\n#### Plot it too to compare it with the OLS plot from above\n\nWhat do you see. Is this expected?\n\n#### Compare the actual coefficients created\n\nClass question: How are they different? And how are they similar?\n\n## Exercise 1\n\nUse a loop to identify the best value of alpha, as measured by r-squared. \n\nWrite all of the alphas and associated r2 into a dictionary\n\nDiscussion question for once you're done: what was the optimal alpha and why does this make sense? How does this compare to OLS? Why is it that way?\n\n```python\n# Starter code: keyt parts omitted.\nscores = {}\nalphas = np.logspace(-5, -0.05, 30)\nfor SOMETHING in SOMETHING_ELSE:\n    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n    # LINE OMIITTED\n    # LINE OMIITTED\n    r2 = r2_score(y, y_hat_lasso)\n    print('R2 for alpha ' + str(alpha) + ': ' + str(r2))\n    scores.append(r2)\n\n# Quick way to get the value from the highest-valued dictionary entry\nbest_alpha = max(scores, key=scores.get)\n```\n\n\n## Exercise 1 Answer\n\n## Operationalizing CV with GridSearch\n\nIt seems a little weird to be automatically finding the best model. If we were just applying this to the dataset a single time, this would indeed be p-hacking to the extreme. However, showing its performance on UNSEEN data is quite the opposite of p-hacking.\n\nHere, we're going to operationalize our method for finding th ebest model by using GridSearch. We are going to test a variety of different alphas, similar to above. Define them here using numpy logspace:\n\nWe are going to be passing this range of tuning parameters to a GridSearch function\nthat will test which works best when cross-validation methods are applied.\nFirst though, we have to put the alphas into the form the GridSearchCV funciton\nExpects, which is a list of dictionaries.\n\nRecall that CV works by calculating the fit quality of different folds of the training data. Here we will just use 5 folds. GridSearchCV will automatically implement the folding and testing logic.\n\n#### Create the lasso_cv object from the lasso object\n\nFinally, we have all our objects ready to pass to the GridSearchVC function which will Give us back a classifier object. Notice that we're reusing that model_lasso objectg we created above. The difference is that we will be systematically  handing different parameters from the tuning_parameters list into the model_lasso object.\n\n#### Fit the lasso_cv object\n\nWhen we call the model_lasso_cv.fit() method, we will iteratively be calling the Lasso.fit() with different permutations of\ntuned parameters and then will return the classifier with the best CV fit.\n\nThe classifier object now has a variety of diagnostic metrics, reporting back on different folds within the Cross Validation. Take a look at them below.\n\nSome relevant results are as below, which we'll extract and assign to lists.\n\n## Exercise 2: \n\nWith your table, explore the scores and alphas lists we've created. Identify which alpha is the best, based on the MSE score returned. A challenge here is that sklearn gave us the scores as a list rather than a dictionary (as we built above), so you will need to use the list to create the dictionary.\n\nOne way to consider doing this would be to create a for loop to iterate through a range(len(scores)): object, saving the alphas and scores to a new dictionary, as in the starter code below. \n\nSave the optimal alpha as a new variable called chosen_alpha.\n```python\noutput_dict = {}\nfor i in OMITTED_CODE:\n    output_dict[alphas[i]] = scores[i]\n    \nbest_alpha = max(output_dict, key=output_dict.get)\n\nprint('best_alpha', best_alpha)\n```\n\n## Exercise 2 Answer\n\n#### Use the built-in attributes to get the best alpha\n\nFortunately, the authors provide a useful  best_params_ attribute.\n\nExtract the best alpha, which we will use later.\n\n#### Rerun LASSO with the best alpha\n\nNow we can rerun a vanilla (no CV) version of Lasso with that specific alpha.\nThis will return, for instance, a .coef_ list.\n\nSimply looking at the coefficients tells us which are to be included.\nQuestion: How will we know just by looking?\n\n#### Extract the feature names and colum indices of the features that Lasso has selected.\n\nThis process led us to the following selected_coefficient_labels:\n\n#### Plot the scores versus the alphas\n\nFor fun, let's plot the alphas, scores and a confidence range.\nWhat does this show us about the optimal alpha and how it varies with score?\n\nA fun aspect of the k-fold approach is you can get a measure of the std_errors involved. Plot those below. \n\n#### Plot the confidence band and the maximum score\n\nalpha=0.2 controls the translucency of the fill color\n\n## Switch back to slides\n\n\n\n\n## Post-LASSO\n\nFinally, now that we have our selected labels, we can use them to select the numpy array columns that we want to use for a post-LASSO run.\n\nPlug this new x matrix into our statsmodels OLS function and print that out.\n\n## Class discussion\n\nHow does the r-squared of this model compare to the one we did at the start of the lecture?\n\nGiven the above, how is the LASSO approach better than a vanilla OLS?\n\nLook at the adjusted R-squared. How does that compare across models. In what ways is the adjusted R-squared similar the CV approach?\n\n\n\n","srcMarkdownNoYaml":"\n\n\n\n\n## Automatic feature selection with LASSO regression\n\nIn this notebook we will learn how LASSO (Least Absolute Shrinkage and Selection Operator) regression works and how it can assist in automatically selecting which variables should be included using a **Cross-Validation** perspective.\n\n#### Start by importing packages\n\n\n\n#### Load dataset and inspect it\n\nAgain we're going to use our diabetes dataset. Inspect it again just to remind yourself\nwhat is in it.\n\n#### Select subset of data\n\nTo speed up calculation, we're going to just use the first 150 observations\nusing numpy slice notation to grab them out of the X, y\n\n#### Run OLS first (for comparison)\n\nRemember the standard Sklearn model steps:\n\n1. create the model object\n2. call the object's fit method.\n3. use the fitted model to predict something.\n4. assess the predictions.\n\n#### Do it again in the econometrics style\n\nRecall that the package statsmodels is closer to the econometrician's way of doing things. We're going to quickly repeat the steps above but with Statsmodels so we can view it in a nice table form.\n\n#### Plot y and y_hat\n\nLet's also plot y and y_hat compared to one of the most important variables, BMI. We'll see both y and y_hat resemble each other.\n\n## Switch to LASSO\n\nNow that we've spent all this time setting up our python environment and getting sklearn, it's almost a trivial step in many cases to try out the latest-and-greatest model.\n\n#### Create a LASSO model object\n\nToday's goal, however, is to do Lasso on this same dataset.\nTo start, lets create a Lasso object. Notice that we are not\nsetting the alpha/gamma value when we create it.\n\n#### Fit the LASSO\n\nCall the lasso.fit() method. \n\n#### Plot it too to compare it with the OLS plot from above\n\nWhat do you see. Is this expected?\n\n#### Compare the actual coefficients created\n\nClass question: How are they different? And how are they similar?\n\n## Exercise 1\n\nUse a loop to identify the best value of alpha, as measured by r-squared. \n\nWrite all of the alphas and associated r2 into a dictionary\n\nDiscussion question for once you're done: what was the optimal alpha and why does this make sense? How does this compare to OLS? Why is it that way?\n\n```python\n# Starter code: keyt parts omitted.\nscores = {}\nalphas = np.logspace(-5, -0.05, 30)\nfor SOMETHING in SOMETHING_ELSE:\n    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n    # LINE OMIITTED\n    # LINE OMIITTED\n    r2 = r2_score(y, y_hat_lasso)\n    print('R2 for alpha ' + str(alpha) + ': ' + str(r2))\n    scores.append(r2)\n\n# Quick way to get the value from the highest-valued dictionary entry\nbest_alpha = max(scores, key=scores.get)\n```\n\n\n## Exercise 1 Answer\n\n## Operationalizing CV with GridSearch\n\nIt seems a little weird to be automatically finding the best model. If we were just applying this to the dataset a single time, this would indeed be p-hacking to the extreme. However, showing its performance on UNSEEN data is quite the opposite of p-hacking.\n\nHere, we're going to operationalize our method for finding th ebest model by using GridSearch. We are going to test a variety of different alphas, similar to above. Define them here using numpy logspace:\n\nWe are going to be passing this range of tuning parameters to a GridSearch function\nthat will test which works best when cross-validation methods are applied.\nFirst though, we have to put the alphas into the form the GridSearchCV funciton\nExpects, which is a list of dictionaries.\n\nRecall that CV works by calculating the fit quality of different folds of the training data. Here we will just use 5 folds. GridSearchCV will automatically implement the folding and testing logic.\n\n#### Create the lasso_cv object from the lasso object\n\nFinally, we have all our objects ready to pass to the GridSearchVC function which will Give us back a classifier object. Notice that we're reusing that model_lasso objectg we created above. The difference is that we will be systematically  handing different parameters from the tuning_parameters list into the model_lasso object.\n\n#### Fit the lasso_cv object\n\nWhen we call the model_lasso_cv.fit() method, we will iteratively be calling the Lasso.fit() with different permutations of\ntuned parameters and then will return the classifier with the best CV fit.\n\nThe classifier object now has a variety of diagnostic metrics, reporting back on different folds within the Cross Validation. Take a look at them below.\n\nSome relevant results are as below, which we'll extract and assign to lists.\n\n## Exercise 2: \n\nWith your table, explore the scores and alphas lists we've created. Identify which alpha is the best, based on the MSE score returned. A challenge here is that sklearn gave us the scores as a list rather than a dictionary (as we built above), so you will need to use the list to create the dictionary.\n\nOne way to consider doing this would be to create a for loop to iterate through a range(len(scores)): object, saving the alphas and scores to a new dictionary, as in the starter code below. \n\nSave the optimal alpha as a new variable called chosen_alpha.\n```python\noutput_dict = {}\nfor i in OMITTED_CODE:\n    output_dict[alphas[i]] = scores[i]\n    \nbest_alpha = max(output_dict, key=output_dict.get)\n\nprint('best_alpha', best_alpha)\n```\n\n## Exercise 2 Answer\n\n#### Use the built-in attributes to get the best alpha\n\nFortunately, the authors provide a useful  best_params_ attribute.\n\nExtract the best alpha, which we will use later.\n\n#### Rerun LASSO with the best alpha\n\nNow we can rerun a vanilla (no CV) version of Lasso with that specific alpha.\nThis will return, for instance, a .coef_ list.\n\nSimply looking at the coefficients tells us which are to be included.\nQuestion: How will we know just by looking?\n\n#### Extract the feature names and colum indices of the features that Lasso has selected.\n\nThis process led us to the following selected_coefficient_labels:\n\n#### Plot the scores versus the alphas\n\nFor fun, let's plot the alphas, scores and a confidence range.\nWhat does this show us about the optimal alpha and how it varies with score?\n\nA fun aspect of the k-fold approach is you can get a measure of the std_errors involved. Plot those below. \n\n#### Plot the confidence band and the maximum score\n\nalpha=0.2 controls the translucency of the fill color\n\n## Switch back to slides\n\n\n\n\n## Post-LASSO\n\nFinally, now that we have our selected labels, we can use them to select the numpy array columns that we want to use for a post-LASSO run.\n\nPlug this new x matrix into our statsmodels OLS function and print that out.\n\n## Class discussion\n\nHow does the r-squared of this model compare to the one we did at the start of the lecture?\n\nGiven the above, how is the LASSO approach better than a vanilla OLS?\n\nLook at the adjusted R-squared. How does that compare across models. In what ways is the adjusted R-squared similar the CV approach?\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"regularization_with_lasso.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","bibliography":["../references.bib"],"theme":"cosmo","title":"Regularization with Lasso"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}