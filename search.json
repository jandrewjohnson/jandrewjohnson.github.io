[
  {
    "objectID": "teaching/undergraduate_environmental_economics.html",
    "href": "teaching/undergraduate_environmental_economics.html",
    "title": "Undergraduate Environmental Economics",
    "section": "",
    "text": "Undergraduate Environmental Economics\nLast offered Fall 2023."
  },
  {
    "objectID": "teaching/natural_resource_economics.html",
    "href": "teaching/natural_resource_economics.html",
    "title": "APEC 8601: Natural Resource Economics",
    "section": "",
    "text": "Location: B36 Ruttan Hall\nMeeting Times: Tuesday/Thursday 8:45 - 10:00 am\nCanvas link: https://canvas.umn.edu/courses/412194\nCourse repository: https://github.com/jandrewjohnson/apec_8601_2024\n\n\nThis course has two main objectives:\n\nTo acquaint you with the major issues and seminal literature in natural resource economics. We will cover issues related to the use of renewable resources such as fisheries, aquifers, and timber, and non-renewable resources such as oil and minerals. We will also cover issues related to the conservation of biodiversity and provision of ecosystem services, climate change, and sustainability. We will analyze the issue of efficient use of resources over time and under what conditions market equilibrium achieves an efficient outcome, intergenerational equity and discounting, common property resources, imperfect competition, spatial modeling, uncertainty, and irreversible decisions. We will also consider new research on global integrated assessment modeling and creation of earth-economy models.\nTo increase your ability to do economic research. We will do a set of activities to increase your ability to think critically and formulate specific researchable questions, as well as improving your modeling and analytical skills.\n\n\n\n\nAPEC 8000-8004 or equivalent (graduate level microeconomic theory). It is possible to take microeconomic theory concurrently. If you haven’t had such classes (or are taking them concurrently) please talk to me about what you will need to do to keep up. We will adjust the course based on how familiar students are with the techniques of dynamic optimization (optimal control theory and dynamic programming)."
  },
  {
    "objectID": "teaching/natural_resource_economics.html#course-content-and-objectives",
    "href": "teaching/natural_resource_economics.html#course-content-and-objectives",
    "title": "APEC 8601: Natural Resource Economics",
    "section": "",
    "text": "This course has two main objectives:\n\nTo acquaint you with the major issues and seminal literature in natural resource economics. We will cover issues related to the use of renewable resources such as fisheries, aquifers, and timber, and non-renewable resources such as oil and minerals. We will also cover issues related to the conservation of biodiversity and provision of ecosystem services, climate change, and sustainability. We will analyze the issue of efficient use of resources over time and under what conditions market equilibrium achieves an efficient outcome, intergenerational equity and discounting, common property resources, imperfect competition, spatial modeling, uncertainty, and irreversible decisions. We will also consider new research on global integrated assessment modeling and creation of earth-economy models.\nTo increase your ability to do economic research. We will do a set of activities to increase your ability to think critically and formulate specific researchable questions, as well as improving your modeling and analytical skills."
  },
  {
    "objectID": "teaching/natural_resource_economics.html#prerequisites",
    "href": "teaching/natural_resource_economics.html#prerequisites",
    "title": "APEC 8601: Natural Resource Economics",
    "section": "",
    "text": "APEC 8000-8004 or equivalent (graduate level microeconomic theory). It is possible to take microeconomic theory concurrently. If you haven’t had such classes (or are taking them concurrently) please talk to me about what you will need to do to keep up. We will adjust the course based on how familiar students are with the techniques of dynamic optimization (optimal control theory and dynamic programming)."
  },
  {
    "objectID": "teaching/big_data_ml_and_ai_for_economists.html",
    "href": "teaching/big_data_ml_and_ai_for_economists.html",
    "title": "Big Data, Machine Learning and AI for Economists",
    "section": "",
    "text": "Big Data, Machine Learning and AI for Economists\nNext scheduled offering: 2025 Fall."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research links economic models and ecosystem service models in order to inform environmental decision-making. Towards this goal, I am working to launch a new model combining the Global Trade Analysis Project (GTAP) computable general equilibrium model with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model from the Natural Capital Project. This new model, GTAP-InVEST, builds on our new work published in Science that is able to calculate ecosystem service flows globally, at very high resolution (30m to 300m grid-cells), under a variety of future scenarios. We include this ability to calculate global ecosystem services in GTAP-InVEST to analyze how changes in future scenarios would shock the global economy through a computable general equilibrium. This allows us to identify the impact on indicators such as real GDP, trade flows, employment and commodity prices. Although this project started very recently, we have already seen exponential growth in the number of organizations interested in working with us and raised significant new funding.\nThe reason we made GTAP-InVEST is because there is a whole class of questions that can only be answered in an integrated, global framework that considers both landscape-scale processes and global dynamics such as supply and demand. This class of questions includes:\n\nWhat does it take to attain the sustainable development goals? How do we even measure that?\nWhat would broad implementation of commodity certification standards do to economic performance (and this would allow us to more precisely establish when and where the conservation benefit might be offset by changes in other countries or sectors.\nWhat does an optimized global payments for ecosystem services policy look like?\n\nThinking about economics more broadly, we've developed macroeconomic tools that have been massively successful at explaining how markets operate, how economies grow and how we consume goods. But as growth has accelerated, we have not had similar success at understanding how and where our economy will cause damage to the basic environmental processes that enable our growth. And conversely, we do not have good tools to assess how our economy will be affected by massive environmental risks, such as climate change or land-use change. My research agenda aims to provide new models that give answers on these challenges. I want to provide models that ministries of finance, offices of land-use planning, multinational corporations or others a set of tools that can handle these questions. Although many have linked environment and economy models before, our work is the first of its kind that links these with detailed, global models at both global and local extents. This is important because environmental processes have incredibly detailed spatial heterogeneity and pursuing the wrong action in the wrong location can lead to undesired consequences.\nIn addition to my current GTAP-InVEST research, I explore many other related research questions, such as:\n\nHow can we meet food security goals while minimizing the loss of ecosystem services? (Published in the Proceedings of the National Academy of Sciences as Global Agriculture and Carbon Trade-Offs)\nHow does the spatial configuration of a landscape affect how economic agents choose between ecosystem services and market goods?\nWhen and where do commodity certification standards (such as Bonsucro) change equilibrium outcomes for improved environmental quality?\nHow can we modify traditional economic models of rational utility maximization to account for reciprocal behavior (and how can these models be applied to understand better human interaction in environmental commons dilemmas)?\n\nFinally, my job market paper, \"Food for Nature Payments: Cost-Effective Payments for Ecosystem Services Using Global, High-Resolution Data,\" defines a spatially-explicit payments for ecosystem services (PES) policy. This paper presents two versions of the PES policy: one based only on public information and one that uses a sealed-bid auction to elicit private information from potential participants on their opportunity cost and the value of their land. Both versions increase the cost-efficiency of conservation investments while minimizing the private loss from forgone private production. The auction version, however, is more cost effective and works even with high degrees of information asymmetry while maintaining the truthful-revelation aspect of sealed-bid auctions. For additional information, see my preview description of Food for Nature Payments. This work is now being incorporated into the GTAP-InVEST model as a specific policy to assess.\nFor a full statement of research, please contact me directly."
  },
  {
    "objectID": "mesh/index.html",
    "href": "mesh/index.html",
    "title": "MESH: Mapping Ecosystem Services to Human well-being",
    "section": "",
    "text": "Mapping Ecosystem Services to Human well-being – MESH – is an ecosystem service assessment, mapping and reporting toolkit to enable sustainable decision making among governments, non-profits and businesses. MESH is an integrative modelling framework that calculates ecosystem service production functions and maps ecosystem service provision under different landscape management scenarios. The base model of MESH integrates and extends ecosystem service models from the Natural Capital Project's 'InVEST' toolkit into a graphical framework (Figure 1) and includes methods to automatically create input data, define scenarios and visualize outputs (without the need to use, e.g., ArcGIS).\nMESH is hosted by The Natural Capital Project at Stanford University were you can download the most recent stable release. Please see forums.naturalcapitalproject.org to discuss MESH or request support. The most recent development release of MESH and links to data can be found on this page.\n\nMESH 5-Minute Tutorial\nHere's a horribly low-quality demo video I  created. Note that it's on a slightly dated version of MESH but still illustrates the basic work-flow."
  },
  {
    "objectID": "gtap_invest/user_guide/scenarios.html",
    "href": "gtap_invest/user_guide/scenarios.html",
    "title": "3. Scenarios and Policy specification",
    "section": "",
    "text": "3. Scenarios and Policy specification\nTo understand different aspects of the earth-economy linkage, we compare multiple scenarios on different policies, economic assumptions, and conceptions of “business as usual” (BAU). In this section, we report how we used the created model with multiple scenarios to assess various aspects of the earth-economy linkage.\nBefore running policy-relevant scenarios, our first step was to update the baseline economy from 2014 to 2021 to be closer to the present year. To do this, we ran a “Base Year Calibration” scenario that projects the global economy from Y2014 – the latest reference year of the GTAP v.10 database – to Y2021, which is the base year used in this paper. We then apply all policies and other shocks to this 2021 snapshot of the economy. To generate the 2021 economy, we specified how the exogenous factors in our model would evolve from 2014 to 2021 and then applied them as a shock to the 2014 equilibrium. We use a similar approach for scenarios that proceed to 2030 (represented in Figure S.3.1 as the “Economic Drivers” arrow on the right-hand side).\nWith the 2021 baseline economy defined, we then define two sets of future scenarios that project from Y2021 to Y2030. Below, we discuss these two sets of scenarios, namely the BAU scenarios (right-hand side of Figure S.3.1) and the Policy Scenarios (left-hand side).\n\nFigure S.3.1: Summary of scenarios and policies analyzed\n\n3.1. BAU scenarios\nOur BAU scenarios include “no Ecosystem Services”, “Ecosystem Services”, “Economic Rigidities” and “Ecosystem Collapse” scenarios. To create Figure 1 in the main text, we calculated the difference between the “no Ecosystem Services” scenario, which projects basic economic activity to 2030 but ignores all changes in ecosystem services, and the “Ecosystems Services” scenario, which incorporates shocks to the economy from changes in pollination services, timber carbon stock and fisheries stock. The difference between these scenarios identifies a subset of estimates on “how much nature matters” and gives us as sense of how wrong our calculations can be if we ignore earth-economy linkages.\n3.1.1. Economic Rigidity\nOur BAU scenarios also consider two important sensitivity analyses. First, in the “Economic Rigidities” scenario, we assess what happens to the economy if it exhibits more rigidity and less substitutability. Specifically, this scenario builds on the “Ecosystem Services” scenario and uses smaller elasticity values for key economic parameters in GTAP-AEZ model. These include: (a) Elasticity of transformation between land cover types, (b) Elasticity of cropland transformation among crops, (c) Constant elasticity of substitution (CES) between primary factors in production, (d) Armington CES for regional allocation of imports and (e) Armington CES for domestic/imported allocation. We obtain lowers bound of the 95% confidence intervals for the Armington CES for regional allocation of imports and Armington CES for domestic/imported allocation based on adjusted estimates from Hertel et al (2007) and Hertel et al (2006) respectively. For the other parameters, standard values are deflated by 50%.\n3.1.2. Ecosystem Collapse\nWe also analyzed what happens to the economy under an “Ecosystem Collapse” scenario, which analyzes very large changes in ecosystems services rather than just small shocks from currently projected land-use change, as analyzed in the main manuscript. These ecosystem collapse shocks are a highly simplified representation of what might happen when key relationships in the ecosystem pass ecological tipping-points. We draw from the tipping-point scenarios defined in the Dasgupta Review of the Economics of Biodiversity (2021, pp. 375, Box 14.3), which used an earlier version of the GTAP-InVEST model. The three specific elements shocked in the collapse scenario are widespread dieback of tropical forests, collapse of wild pollinator populations, and severe losses in marine fishery populations.\nA large body of literature suggests that regime changes may happen because large parts of the biosphere are close to tipping points (Rockström et al. 2009; Steffen et al. 2015). According to these studies, it is possible that when some (probably unknown) ecological thresholds are passed, it might trigger large, non-linear, systemic change in the health of entire ecosystems.  It is extremely challenging to predict when tipping points might be crossed. Instead, these results are not predicted to happen at some specific point in time, but are presented to explore scenarios and economic implications of large-scale ecological change. We suggest that our readers interpret these results with some caution because the exact thresholds are unknown and what happens beyond the threshold remains poorly understood, especially at large spatial scales (Lenton 2013).\nConceptually, it is important to consider such non-linear changes and estimate how they might have further, possibly non-linear, effects in the economy. Traditional CGE models ameliorate many negative impacts to some degree by substitution away from affected sectors. However, it might be the case that such substitution is limited in overall quantity and that very large changes could cause decreased flexibility within the economy, amplifying negative effects. We discussed this possibility above in our section on economic rigidity.\nTo determine which tipping points we wanted to assess, we reviewed the “Regime Changes” database produced by the Stockholm Resilience Center[1]. We identified three scenarios that we were able to evaluate in GTAP-InVEST. These include assessing wide-spread collapse of tropical forests that results in forests converting into grasslands and shrubs, global pollinator collapse, and climate-related reductions in fisheries output.\nWe define the specific shocks that we impose on the GTAP-InVEST model in Table S.3.1, reproduced with permission from Johnson et al. (2021).\n\n\n\n\n\n\n\nScenario\nMethod to calculate shock\n\n\nWild pollination collapse\nThe BAU and policy scenario considers how pollination services would change when different levels of pollinator habitat are present near pollinator-dependent crops. It does not consider what happens if broad-scale reductions in pollinator colony health result in additional changes unrelated to LULC configuration. To model this extended pollinator collapse scenario, we modified the pollination scenario to also contain a 90 percent reduction in pollination sufficiency. The 90 percent reduction is less severe than other attempts to model pollinator collapse, e.g., Bauer and Wang (2016) evaluate a scenario where most, but not all, species of wild pollinators cease to provide service. Note that this shock means crops only partially dependent on pollination services will not see yield reductions as large as the pollinator collapse. Assessing the extent to which markets shift, consumers substitute to non-pollinator crops, and producers substitute to non-pollinator intermediate goods will test the global market’s ability to absorb such a large shock.\n\n\nMarine fisheries collapse\nThe model relies on the Fisheries and Marine Ecosystem Model Intercomparison Project data (Lotze et al. 2019). To simulate the regime shift, the model assumes a severe climate change scenario (8.5 instead of RCP4.5) and further takes the worst-case outcome in terms of climate change impact reported in the uncertainty bounds and sensitivity analysis. The model simulates severe disruptions of fish migration that lead to a reduced total catch biomass, which in turn impacts the economic model.\nThis type of collapse would reflect when, for example, fish populations are blocked from migrating north or south to keep a constant habitat. The reduced fisheries impact the model by lowering Total Catch Biomass in the projections, which registers as a technology-neutral productivity change in the fisheries sector.\n\n\nWidespread conversion of tropical forests to savannah\nTo create this regime-change shock, we used the SEALS model to project a landscape where 88% of the forests in tropical regions (specifically AEZs 5 and 6) were converted to grassland or shrubland. The landscape generated by this calculation had much less forest cover, so when it was used as an input to InVEST, the relative sizes of ecosystem service impacts were much larger.\nAs with all of these tipping-point scenarios, the precise magnitude of forest dieback is unknown, and instead we aimed to provide illustrative values that can then be processed through the rest of the GTAP-InVEST model. l\n\n\n\nTable S.3.1: Shocks applied to the GTAP-InVEST model\nNote that the definition of this ecosystem-collapse scenario is subtly different than in Johnson et al. (2021). In the previous study, the shocks to update the global economy from 2021 to 2030 (see Figure S.3.1) were imposed along with the ecosystem-collapse shocks. With this approach, the contractionary impacts of ecosystem-collapse interacted with economic growth, thereby resulting in higher losses ($2.7 trillion in the previous study versus $2.0 trillion here). In this article, we applied the ecosystem-collapse shock after the economy grew to the 2030 level. We did this to isolate the impacts of ecosystem-collapse on direct economic activity rather than in combination with growth effects, which makes attribution to ecosystem services more challenging. Ecosystem-service and economic growth interactions remain important, however, so future research is needed to further identify this interrelation.\n3.2. Policy Scenarios\nThe second set of scenarios are policy scenarios that define several nature-smart policies to see how they impact the outcomes of the GTAP-InVEST model. Our approach draws on four different policies: (i) removing agricultural subsidies, (ii) domestic carbon forest payments, (iii) global carbon forest payments, and (iv) agricultural research and development (R&D). Removing agricultural subsidies, sometimes referred to as “decoupling support to farmers”, requires that certain agricultural subsidies are replaced with a direct payment to landowners using the “savings” obtained from not paying the subsidy. Carbon forest payments can be either domestically or globally managed. In the domestic case, governments compensate landowners to preserve land instead of converting it for agriculture or other purposes. For globally managed forest payments, wealthier countries contribute to a global pool based on their historical emissions. The pool is allocated to developing countries, compensating them to limit land use in a manner equivalent to the payment received. Research and development investments focus on increasing the efficiency of land already converted for agricultural purposes, meaning that supply can rise without expansions in land use.\nReferring back to Figure S.3.1, the “Policies” scenarios include subsidy repurposing and payments to ecosystem services. Input and output subsidies paid to the agricultural sector are reallocated as land input subsidies (“Land Payments”) or are reinvested into public agricultural R&D (“Agricultural R&D”). Calculations under “Agricultural R&D” scenario relies heavily on the framework from Baldos et al. (2019), which estimates the public R&D spending increase required to offset agricultural productivity losses from climate change. The payment schemes to ecosystem services include “Local PES” and “Global PES”. Under the “Global PES” scenario, high-income countries fund a global budget via income transfers. The budget compensates income losses in countries that set aside land for natural use. In the “Local PES” scheme, the budget is based on the amount of input and output subsidies in agriculture for each region. Finally, the “Combined Policies” scenario incorporates the methods from the “Agricultural R&D” and “Global PES” scenarios",
    "crumbs": [
      "3. Scenarios and Policy specification"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/invest_details.html",
    "href": "gtap_invest/user_guide/invest_details.html",
    "title": "5. InVEST model details",
    "section": "",
    "text": "5. InVEST model details\nThe InVEST model suite is thoroughly documented in its user guide (Sharp et al. 2020). This source identifies how each of the 24 ecosystem service models are calculated, including how to obtain input data. However, the models officially included in InVEST do not always work at the global scale. Thus, we use modified versions of them that have been rewritten to run in parallel at the global scale. These modified versions of the models were first documented in Kim et al. (2018) and more fully in Chaplin-Kramer et al. (2019). In this section, we summarize the modifications made to enable global coverage.\nFor the pollination model, the global version assumed pollinators are fully abundant on any natural land covers (specifically classes 50 to 180 in our underlying ESACCI LULC database) and that all pollinators had the same relative efficacy and distance decay parameters for their pollination activity. Region-specific applications of InVEST often calibrate the model based on which species of pollinators are present and what their pollination efficiency is (which we did not do). We then calculated the proportional area of natural land covers around every plot of agricultural land (classes 10-20). In brief, the model estimates pollinator sufficiency based on the abundance of pollinator species given the two limiting resources on the landscape: pollinator nesting habitat and pollinator nectar sources (provided here by pollination-dependent crops). We calculated this globally following Chaplin-Kramer et al. (2019), which provided a global map of pollination sufficiency. This was an input to the algorithm described in SI Section 6 that connects InVEST biophysical outputs to GTAP inputs.\nWe perform the carbon storage calculations by applying a custom version of the InVEST carbon storage model. This version uses a globally-extended lookup table for IPCC Tier-1 (Ruesch and Gibbs 2008) values of carbon content defined for regions breaking out each continent, ecofloristic region and frontier forest status. The default InVEST model takes as an input a biophysical table with one of these regions.",
    "crumbs": [
      "5. InVEST model details"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtap_details.html",
    "href": "gtap_invest/user_guide/gtap_details.html",
    "title": "4. GTAP model details",
    "section": "",
    "text": "4. GTAP model details\nThe GTAP (Global Trade Analysis Project) model is a multi-commodity, multiregional computable general equilibrium (CGE) model that tracks bilateral trade flows between all countries in the world and explicitly models the consumption and production for all commodities of each national economy (Corong et al., 2017; Hertel, 1997). The standard version of the model is a comparative static CGE model that shows differences between possible states of the global economy for a given year – with and without policy or with respect to base year and future year. At the core of the GTAP model is an input-output accounting framework that accounts for all sources and uses of each market good and economic endowment. Figure S.4.1 is a stylized framework of the GTAP model and summarizes the key flows across economic agents in the model.\n\nFigure S.4.1: A stylized framework of the GTAP model. Source: the figure is adapted from Corong et al. 2017\nConsumption framework. The model has a single representative household for each region (the Regional household). The red line in Figure S.4.1 represents income flows to the household. The household receives all gross factor payments net of the capital depreciation allowance (VOA - Payments of Factors of Production), plus the receipts from all indirect taxes (including Export and Import Taxes – XTAX, MTAX). Regional income is distributed across three broad categories—Private Household, Government Expenditures, and Savings— by maximizing a top-level Cobb-Douglas utility function. Saving is a unitary good, while Private Household and Government Expenditures utilize sub-level utility functions to determine consumption of each domestic product (Value of Domestic purchases of Private Household and Government - VDPA, VDGA - black lines in Figure S.4.1) and imported commodities (Value of Imported purchases of Private Household and Government - VIPA, VIGA - blue lines in Figure B.1). The sub-level utility function for the Private Household is based on a constant differences of elasticities (CDE) function (Hanoch, 1975). This function is less demanding to solve than the flexible functional forms and permits calibration of income elasticities and own-price elasticities independently, and importantly, it is non-homothetic. The sub-utility function for public expenditure is based on a constant elasticity (CES) utility function (Arrow et al., 1961).\nProduction framework. GTAP uses nested CES functions to model producer behavior for each region. At the top level of the production framework, producers combine aggregate value-added and intermediate inputs, according to a single CES function. Sub-level CES functions produce aggregated value-added from each primary factor commodities and aggregated intermediate input from each purchased inputs. Factors of production, or endowments, are of three types: perfectly mobile (e.g., labor and capital), partially mobile or sluggish (e.g., land) and sector-specific factors (natural resources). Each purchased input can be sourced either domestically or internationally, and this is modelled using another sub-level CES function (Value of Domestic and Imports of Firms – VDFA and VIFA in Figure S.4.1.).\nInternational trade. The most notable restriction on trade in the GTAP model is that commodity sourcing is at the border: for each product, all domestic agents (i.e., Private Household, Government, Producers) in an economy use the same mix of imports from different countries, though each agent chooses their own combination of import and domestic product. There is also a two-level system of substitution between products from different sources - an import-domestic top-level CES function above an import-import sub-level CES function. Trade flows generate supply and demand for international transport services, and this is accounted for in the model. There is also no international trade in primary factors in the standard version of GTAP.\nWe use the GTAP Database Version 10 for year 2014 (Aguiar et al., 2019). It represents globally consistent data on consumption, production, and international trade (including transportation and protection data), energy data and CO2 emissions for 140 regions and 57 commodities. These regions and commodities are aggregated into 37 regions and 17 commodity groups. The GTAP Data Base is composed of Input Output Tables statistics, which are mainly contributed by members of the GTAP Network. The GTAP 10 Database includes separate IO tables for 121 individual countries representing 98 percent of global gross domestic product and 92 percent of the world’s population. Key value flows in the database include both input-output flows within each region, bilateral international trade flows, capital stock and savings information, international transports costs, domestic input and output subsidies, export subsidies and import tariffs, as well as revenue flows from taxes and tariffs. Most flows are measured at both tax-free and tax-paid prices (i.e., taxes are implicitly accounted for). Key behavioral parameters provided with the GTAP Data Base include the source-substitution or Armington elasticities (used to differentiate goods by country or origin), the factor substitution elasticities, the factor transformation elasticities affecting the sluggish factors, the investment parameters, and the parameters governing the consumer demand elasticities. The first three sets of parameters are taken from external sources, while the rest are calibrated from the database.\nThe standard GTAP model is implemented using the GEMPACK (General Equilibrium Modelling PACKage) suite of economic modeling software (Harrison & Pearson, 1996). GEMPACK is distributed by The Centre of Policy Studies Knowledgebase at Victoria University, Melbourne, Australia (https://www.copsmodels.com/gempack.htm). Following the standard for the GEMPACK program, all equations of the GTAP model are recorded not in levels (e.g., million USD), but in percentage change form. Due to non-linearities in formulae and update equations– which result in changes in the underlying shares and price elasticities– the solution requires non-linear methods. The GTAP model can be run via command line as well as the Windows-based RunGTAP tool. RunGTAP is a visual interface to various GEMPACK programs and allows the user to run simulations interactively in a Windows environment using the GTAP general equilibrium model. No previous knowledge of the GEMPACK language or programming skills is necessary to use the program. Results and complementary information for further analysis are also provided in a Windows environment and can be accessed interactively. RunGTAP also has several add-on tools that can be helpful to users. The welfare decomposition tool permits the user to break down the regional equivalent variation metric into its component parts, including changes due to allocative efficiency, terms of trade, improved technology, and endowments. The systematic sensitivity analysis tool allows uncertainty analysis in the model shocks and parameters, thereby generating both mean and standard deviations of model output. Finally, the subtotals tool utilizes numerical integration techniques in order to exactly decompose changes in the model outputs as sums of the contributions made by the change in each exogenous variable (Harrison, Horridge & Pearson, 2000). The subtotals are particularly useful in understanding the key drivers of model outcomes. All the input files are binary header array (HAR) files, to keep the size of the files small. The HAR files are designed to work with the GEMPACK program. There is also a GAMS version of the standard GTAP model and software exist for readily converting these HAR files to the General Algebraic Modeling System (GAMS) data exchange file (GDX) format, as well as to CSV files.\nCapital, skilled labor, and unskilled labor are perfectly mobile in the GTAP model. This report assumes perfect labor mobility, which means that labor can move across sectors but not across skill types. In general, perfect mobility implies that returns to each factor will be equated across all sectors. Therefore, there is a single economy-wide price for each mobile factor (capital, skilled and unskilled labor), with market equilibrium determined by setting aggregate demand equal to (exogenous) supply. Land is partially mobile (or sluggish in GTAP terminology). The supply of aggregate land to individual activities is less than perfectly elastic, as there is a transformation frontier (Constant Elasticity of Transformation) that moderates the movement of the land across activities. This results in sector-differentiated land prices for each land using sector. The economy-wide price of each land type is then calculated as the CET aggregate price of each land factor.\n\n4.1. Productivity growth from agricultural R&D\nIncreasing public agricultural R&D investments is one of the key levers policy makers can use to alter the trajectory of agricultural productivity. Following Baldos et al. (2020), GTAP models implements R&D policy by modelling the linkages between the flow of R&D spending, the stock of accumulated knowledge capital, and subsequent total factor (TFP) productivity growth (Alston et al., 2011; Griliches, 1979; P. Heisey et al., 2011; Huffman, 2009). TFP captures the rise in total output given all inputs used in agricultural production, unlike crop yields, which ignores the role of other farm inputs (i.e., total output per area of land input used). The historical national R&D spending data are based on Fuglie (2018), who complied data on public agricultural R&D expenditures from the literature[2] starting from the 1960s worldwide and from the 1930s for some developed countries, measured in 2005 Purchasing Power Parity (PPP) dollars. Equations 1 to 3 summarize the key linkages under this framework.\n\nStarting with Eq.1, R&D expenditure at time t (XD_t ) contributes to R&D stock in years t+1 (RD_t+1 ) through t + L, where \\beta_RD,i is the R&D lag weight at period i and total lag length L is the number of years R&D contributes to productivity until it fully depreciates (Appendix Figure S.4.1). Following the structure of the R&D lag weights, initially R&D spending at time  contributes little to knowledge capital stock, but its effect builds over time as technology arising from that research is developed and is disseminated to farmers. Eventually, the effects peak when technology is fully disseminated, but then wanes due to technology obsolescence. Following Alston et al. (2011), this process is modeled by imposing a gamma distribution for the R&D lag weights (Equation 2). We utilize separate R&D lag distributions for developing and developed regions, calibrated according to lag structures suggested in the literature (Alston et al. 2010). Specifically, we impose a lag length that spans 50 years for developed countries, which are assumed to be on the productivity frontier.  Peak impacts of R&D spending on knowledge stocks (and productivity) occur after 26 years (\\sigma, \\lambda = (0.90, 0.70)). For developing countries, we impose a total lag length of 35 years, with peak effects at year 10 (\\sigma, \\lambda = (0.80, 0.75)).\n\nFigure S.4.1: R&D Lag Weights used to convert R&D spending to R&D knowledge stocks across different years Note: figure shows the R&D lag weights used to convert R&D spending to R&D knowledge stocks across different years. The value of the weights is zero at Year 0 (investment year), increases as time progress, and eventually goes to zero. The shorter lag length for developing countries reflects their ability to focus more on adaptive R&D, borrowing from global knowledge capital to close existing yield gaps.  The longer lag structure for developed countries reflects a greater focus on discovery R&D to push out the global science and technology frontier. We define the total lag length as 50 years following Alston et al (2010).\nThe growth in knowledge capital stocks is linked to growth in agricultural total factor productivity (TFP) via elasticities that describe the percent rise in TFP given a 1 percent rise in knowledge capital stock (see Equation 3). Using the empirical estimates in the literature as a guide (Fuglie 2018), we assign the R&D stock–to–TFP elasticities for each world region (Appendix Table 1). The values of these parameters reflect generally lower capacity of R&D systems in developing regions (where the value of the R&D elasticity ranges from 0.18 to 0.23) compared to developed countries (where ranges from 0.23 to 0.30). Lower elasticity values imply larger increases in R&D stocks – achieved via greater R&D spending growth – are required in order to raise TFP sufficiently to offset the adverse effects of climate change. It is consistent with the lower research intensities (less R&D spending relative to the value of agricultural output) in developing countries (Pardey et al. 2016). It is important to note that we only consider climate adaptation driven by public R&D investments. We exclude private and international R&D spending for which the contribution to global spending pool has grown steadily in recent years (Beintema et al., 2012). We also abstract from the potential for additional technological spillovers across regions in the context of climate adaptation.\nTable S.4.1: Annual growth rates of key economic variables over 2014-2030. Note: Compounded annual growth rates are calculated from total changes over 2014-30 in Capital Stock, Real GDP, Population, Unskilled Labor and Skilled Labor from Econmap (v2.4) (Fouré et al 2013) based on SSP2 scenario. Regional productivity growth is used to target changes in regional Real GDP. Sector specific productivity growth for Crops, Ruminants and Non-Ruminants are based on per annum growth rates from Ludena et al (2007) over 2001-40. Due to lack of estimates, global agricultural productivity growth from Ludena et al (2007) is imposed on the managed forestry sector. A 2% productivity growth is imposed on Manufactures sector to reflect the productivity gap between Manufactures and Service sectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegions\nAnnual growth rates of key economic drivers over 2014-2030 (in %)\n\n\n\n\n\n\n\n\n\nCapital Stock\nReal GDP\nPopulation\nUnskilled Labor\nSkilled Labor\nTotal Factor Productivity\n\n\n\n\n\n\n\n\n\n\n\nCrops\nRuminant\nNon-ruminants\n\n\nArgentina\n3.38\n2.79\n0.61\n0.94\n1.41\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nBangladesh\n3.66\n3.91\n0.80\n1.27\n0.94\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrazil\n3.26\n2.62\n0.57\n0.81\n1.94\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Amer\n3.94\n3.53\n0.86\n1.47\n1.21\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanada\n2.62\n2.82\n0.96\n0.72\n0.92\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\n6.82\n6.07\n-0.07\n-0.30\n1.62\n0.81\n1.68\n3.66\n\n\n\n\n\n\n\n\n\n\n\n\n\nColombia\n2.61\n1.96\n0.97\n1.39\n2.16\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nRest of E Asia\n6.69\n5.70\n1.04\n1.02\n0.29\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nEgypt\n5.58\n5.21\n1.42\n2.13\n1.76\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthiopia\n4.34\n3.73\n1.60\n2.40\n-0.04\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nEU\n1.96\n1.81\n0.25\n-0.12\n1.17\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndonesia\n3.45\n3.05\n0.57\n1.51\n4.53\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndia\n5.08\n5.43\n1.08\n1.69\n1.64\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nJapan\n1.54\n1.41\n-0.32\n-0.28\n0.84\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nKorea\n4.24\n3.58\n0.02\n0.17\n0.85\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nMorroco\n3.13\n3.31\n0.41\n0.89\n2.35\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nMadagascar\n0.59\n2.50\n2.18\n3.03\n-2.39\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nM East N Africa\n3.47\n2.30\n1.35\n1.47\n1.54\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nMexico\n3.52\n2.82\n0.64\n0.98\n0.60\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nMalaysia\n4.68\n3.73\n1.28\n1.49\n1.23\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nNigeria\n7.00\n6.47\n2.20\n3.02\n4.68\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nOceania\n3.01\n3.14\n1.36\n1.47\n-0.46\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nC Asia\n4.14\n4.63\n0.07\n0.02\n0.64\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Europe\n2.62\n1.90\n0.62\n0.22\n1.22\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nPakistan\n4.70\n3.59\n1.56\n2.01\n1.24\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhilippines\n5.11\n4.94\n1.42\n2.22\n0.86\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoland\n4.04\n3.89\n-0.06\n-0.44\n1.29\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nRest of S Asia\n4.11\n3.80\n2.10\n2.59\n0.28\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nRest of SE Asia\n4.56\n3.45\n0.61\n0.78\n2.38\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nRussia\n3.40\n3.42\n-0.12\n-0.67\n-1.21\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nS America\n3.54\n2.65\n0.78\n1.34\n1.13\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nSS Africa\n4.38\n4.77\n2.08\n2.83\n1.98\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurkey\n4.76\n4.41\n0.88\n1.26\n1.31\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nUSA\n2.12\n1.77\n0.71\n0.40\n-0.53\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nVietnam\n5.37\n4.59\n0.61\n0.54\n2.77\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nAngola+DRC\n6.89\n5.93\n2.60\n3.46\n3.05\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth Africa\n3.67\n3.37\n0.62\n1.26\n1.57\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable S.4.2: Historical and Projected Average Annual Growth Rate in R&D Spending. Note: Subsidy Repurposing includes R&D spending under Baseline 2021-2030 in addition to savings from subsidy removal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nAverage annual growth rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Data\nFuture Projections\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline\nSubsidy Repurposing 100 percent\nSubsidy Repurposing 50 percent\nSubsidy Repurposing 20 percent\nSubsidy Repurposing 10 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1991-2000\n2001-2010\n2011-2020\n2021-2030\n2021-2030\n2021-2030\n2021-2030\n2021-2030\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral America\n-3.7 percent\n4.0 percent\n3.2 percent\n3.6 percent\n62.1 percent\n22.1 percent\n15.3 percent\n12.4 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth America\n-0.3 percent\n2.7 percent\n3.5 percent\n3.6 percent\n7.8 percent\n5.5 percent\n4.4 percent\n4.1 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth Asia\n6.9 percent\n5.4 percent\n4.0 percent\n3.6 percent\n20.3 percent\n9.8 percent\n5.9 percent\n4.3 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth East Asia\n3.4 percent\n1.6 percent\n3.4 percent\n3.6 percent\n35.6 percent\n17.3 percent\n13.3 percent\n11.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorth East Asia\n6.9 percent\n11.5 percent\n4.7 percent\n3.6 percent\n27.2 percent\n13.0 percent\n8.5 percent\n6.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nWest Asia\n3.9 percent\n2.6 percent\n3.7 percent\n3.6 percent\n17.0 percent\n11.3 percent\n9.4 percent\n8.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorth Africa\n2.5 percent\n3.0 percent\n4.7 percent\n3.6 percent\n17.0 percent\n11.3 percent\n9.4 percent\n8.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSub-Saharan Africa\n0.7 percent\n2.8 percent\n3.4 percent\n3.6 percent\n23.5 percent\n14.9 percent\n12.6 percent\n11.8 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth Africa\n1.4 percent\n-1.4 percent\n1.1 percent\n1.5 percent\n4.4 percent\n2.8 percent\n2.0 percent\n1.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nOceania\n1.2 percent\n-1.6 percent\n1.2 percent\n1.5 percent\n4.6 percent\n3.1 percent\n2.3 percent\n2.0 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanada/USA\n1.4 percent\n-0.2 percent\n0.6 percent\n1.5 percent\n10.0 percent\n5.2 percent\n2.9 percent\n2.0 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nJapan/Korea\n2.2 percent\n1.3 percent\n1.3 percent\n1.5 percent\n10.5 percent\n5.7 percent\n3.4 percent\n2.5 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nWestern Europe\n0.6 percent\n1.3 percent\n1.4 percent\n1.5 percent\n16.4 percent\n7.8 percent\n4.2 percent\n2.8 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransition Regions\n10.9 percent\n5.8 percent\n3.6 percent\n3.6 percent\n381.6 percent\n12.6 percent\n7.9 percent\n5.9 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nWORLD\n1.9 percent\n2.8 percent\n2.7 percent\n2.8 percent\n32.3 percent\n10.0 percent\n6.7 percent\n5.4 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. Extending GTAP to include a physical representation of land via GTAP-AEZ\nThe base GTAP CGE excludes several components necessary to link to environmental models. As discussed above, we chose to augment the base model with specific agro-ecological zones (AEZs) in order to specify biophysical production parameters, make explicit the use of environmental resources (e.g., water), and endogenize use-differentiated land resources (for forestry, agriculture and other land-intensive sectors). Inclusion of these extra factors exposes the key variables in the economy to which our fully spatialized GTAP-InVEST model can connect. The underlying GTAP-AEZ model is based on the formulation in Lee (2005), updated by Tahierpour et al. (2013) and Baldos (2016). We further updated the model for this paper to the newest GTAP database (Version 10) with greater country-level and AEZ-level disaggregation. The underlying equations in GTAP and GTAP-AEZ are well documented throughout the literature, so we do not present them in full.\n\n\n4.3. Example GTAP configuration file\nThe configuration below is for the BAU Ecosystem Service adjustment run. See the code repository at github.com/jandrewjohnson/gtap_investfor all configuration files. These configuration files are called iteratively by the python code using a batch file that replaces the &lt;cmf&gt; tag with the name of the scenario being run.\nstart with mmnz = 200000000;  ! Assign largest starting memory allocation (max is 2100000000)\nMA48 increase_MMNZ = veryfast; ! If largest memory is not enough, allow\nCPU = yes;\nNDS = yes;\nExtrapolation accuracy file = NO;\naux files = \"GTAPAEZ\";\n! Input files\nfile GTAPSETS = ..\\gtp1414\\sets.har;\nfile GTAPDATA = ..\\work\\2021_30_BAU_noES.upd;\nfile GTAPPARM = ..\\gtp1414\\default.prm;\nfile GTAPSUPP = ..\\gtp1414\\&lt;cmf&gt;_SUPP.har;\n! Output files\nfile GTAPSUM          = ..\\work\\&lt;cmf&gt;_sum.har;\nUpdated file GTAPDATA = ..\\work\\&lt;cmf&gt;.upd;\nSolution file         = ..\\work\\&lt;cmf&gt;.sl4;\nVerbal Description =&lt;cmf&gt; ;\nlog file  =  ..\\work\\&lt;cmf&gt;.log;\nMethod = Euler;\nSteps = 2 4 6;\nautomatic accuracy = yes;\naccuracy figures = 4;\naccuracy percent = 90;\nminimum subinterval length =  0.0001;\nminimum subinterval fails = stop;\naccuracy criterion = Both;\nsubintervals =5;\nexogenous\npop\npsaveslack pfactwld\nprofitslack incomeslack endwslack\ncgdslack tradslack\nams atm atf ats atd\naosec aoreg\navasec avareg\nafcom afsec afreg afecom afesec afereg\naoall afall afeall aoall2 aoall3 aoall4\nau dppriv dpgov dpsave\nto_1 to_2 !to\n!EC change for revenue neutral scenario\ntfijr\ntfreg\n!End: EC change for revenue neutral scenario\ntp tm tms tx txs\nqo(\"UnSkLab\",REG)\nqo(\"SkLab\",REG)\nqo(\"Capital\",REG)\nqo(\"NatRes\",REG)\ntfm tfd;\nExogenous p_slacklandr;\nExogenous p_ECONLAND  = zero value on    file ..\\gtp1414\\basedata.har header \"MAXL\" ;\nExogenous p_slackland = nonzero value on file ..\\gtp1414\\basedata.har header \"MAXL\" ;\nExogenous p_LANDCOVER_L(AEZ_COMM,\"UNMNGLAND\",REG);\nExogenous c_MAX_LAND;\nRest Endogenous ;\n!===========\n! Shocks\n!===========\n! Ecosystem services shocks\n! (1) Fishery and Forestry shocks\nShock aoall2(\"fishery\",REG)    = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"fishery\";\nShock aoall2(\"forestsec\",REG)  = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"forestsec\";\n! (2) Pollination shocks from InVEST : Pollination Collapse\nShock aoall3(\"cotton\",REG)     = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"cotton\";\nShock aoall3(\"crsgrns\",REG)    = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"crsgrns\";\nShock aoall3(\"fruitveg\",REG)   = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"fruitveg\";\nShock aoall3(\"oilsds\",REG)     = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"oilsds\";\nShock aoall3(\"othercrps\",REG)  = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"othercrps\";\n!===========\n! Subtotal\n!===========\nSubtotal aoall2(\"fishery\",REG)  = fish;\nSubtotal aoall2(\"forestsec\",REG)= forest;\nSubtotal aoall3(\"crsgrns\",REG) aoall3(\"fruitveg\",REG) aoall3(\"oilsds\",REG) aoall3(\"cotton\",REG) aoall3(\"othercrps\",REG)  = polli;",
    "crumbs": [
      "4. GTAP model details"
    ]
  },
  {
    "objectID": "gtap_invest/results/index.html",
    "href": "gtap_invest/results/index.html",
    "title": "GTAP-InVEST Results",
    "section": "",
    "text": "Download the full results from Johnson et al. 2023 (PNAS). This google drive directory has the results in raw Header Array format (.har) files used by GEMPACK, along with the results in CSV format for easier viewing. See the data description and readme file for explanations of the data organization."
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#getting-set-up",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#getting-set-up",
    "title": "Justin Andrew Johnson",
    "section": "Getting set up",
    "text": "Getting set up\n\n\nMake sure you have followed all of the steps in the installation page.\n\nIn particular, Clone the SEALS and Hazelbean repositories in the correct location\n\nDescribed here and here\n\nYou will know you’ve got them installed correctly if your VS Code Explorer tab shows the repositories without an error message\n\nAs pictured"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#explore-the-seals-code",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#explore-the-seals-code",
    "title": "Justin Andrew Johnson",
    "section": "Explore the SEALS code",
    "text": "Explore the SEALS code\n\n\nIn the VS Code Explorer tab, navigate to your seals_dev directory\n\nQuick note about file organization\n\nThe rool of seals_dev contains more than just the seals library, such as directories for scripts, images, etc.\nThe library itself is in the seals subdirectory seals_dev/seals which may seem redundant but is necessary for the way Python imports work.\nIf you inspect the seals directory, you will see an __init__.py file. This make Python able to import the directory as a package.\n\nYou will also see a seals_main.py file. This is where most of the actual logic of seals is."
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-1",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-1",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-2",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-2",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-3",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-3",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-4",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-4",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-5",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-5",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-6",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-6",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-7",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-7",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-8",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-8",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-9",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-9",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-10",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files-10",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne does not simply run a main.py (fig1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (fig2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#setting-up-the-run-file",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#setting-up-the-run-file",
    "title": "Justin Andrew Johnson",
    "section": "Setting up the run file",
    "text": "Setting up the run file\n\n\nThe run file begins with standard python imports\nThen in the if __name__ == '__main__': block, we define the project directory and initialize the project flow object\n\nThe reason for putting it in this block is so that you don’t accidentally run the code when you import the file in another script\n\nSEALS (and the EE Devstack) assumes (or softly requires) that you put all code and data somewhere relative to the user’s home directory os.path.expanduser('~')\n\nCan put it in suddirectories with extra_dirs = ['Files', 'seals', 'projects']\n\nIf you followed the EE method, you will have already created the seals directory at &lt;user_dir&gt;/Files/seals\n\nIn the seals directory, your code is in seals_dev\nIn the seals directory, you also will have a projects direcotry\n\nThis is created automatically if its not there\nAll data and outputs will be saved in this directory\n\nAs a best practice, you should not save data in the seals_dev directory\n\n\n\n\nimport os, sys\nimport seals_utils\nimport seals_initialize_project\nimport hazelbean as hb\nimport pandas as pd\nfrom seals_utils import download_google_cloud_blob\n\nmain = ''\nif __name__ == '__main__':\n:::\n\n\n\n\n\n# on the attributes set above and saved to the location in scenarios_definitions_path.\np.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n\n\n# and will be written to a CSV file in your project's input dir.\n\n\n# and will be written to a scenarios csv in your project's input_dir for later use/editing/expansion.\n\n# String that uniquely identifies the scenario. Will be referenced by other scenarios for comparison.\np.scenario_label = 'ssp2_rcp45_luh2-globio_bau'\n\n\n# as what the scenario should be compared against. I.e., Policy minus BAU.\np.scenario_type = 'bau'\n::: r-fit-text\nThis computing stack also uses hazelbean to automatically download needed data at run time. In the code block below, notice the absolute path assigned to p.base_data_dir. Hazelbean will look here for certain files that are necessary and will download them from a cloud bucket if they are not present. This also lets you use the same base data across different projects.\nIn addition to defining a base_data_dir, you will need to For this to work, you need to also point SEALS to the correct data_credentials_path. If you don’t have a credentils file, email jajohns@umn.edu. The data are freely available but are very, very large (and thus expensive to host), so I limit access via credentials.\n\np.base_data_dir = os.path.join('G:/My Drive/Files/base_data')\n\np.data_credentials_path = '..\\\\api_key_credentials.json'\nNOTE THAT the final directory has to be named base_data to match the naming convention on the google cloud bucket."
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#running-the-model",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#running-the-model",
    "title": "Justin Andrew Johnson",
    "section": "Running the model",
    "text": "Running the model\n\nAfter doing the above steps, you should be ready to run run_test_seals.py. Upon starting, SEALS will report the “task tree” of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.\nOnce the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:\n [THIS IS NOT THE CORRECT IMAGE]"
  },
  {
    "objectID": "earth_economy_devstack/seals_quickstart.html",
    "href": "earth_economy_devstack/seals_quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "Quickstart\nThe simplest way to run SEALS is to clone the repository and then open run_test_standard.py in your preferred editor. Then, update the values in ENVIRONMENT SETTINGS near the top of run_test_standard.py for your local computer (ensuring this points to directories you have write-access for and is not a virtual/cloud directory).\nFinally, run seals via the command line with the command python run_test_standard.py in the appropriate directory. If configured correctly, SEALS will download the required data at runtime (assuming you point to a data bucket you have access to). The default bucket is publicly available, though other configurations are provided for alternate uses.",
    "crumbs": [
      "SEALS",
      "Quickstart"
    ]
  },
  {
    "objectID": "earth_economy_devstack/run_file.html",
    "href": "earth_economy_devstack/run_file.html",
    "title": "Run File",
    "section": "",
    "text": "Run File"
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html",
    "href": "earth_economy_devstack/project_flow.html",
    "title": "Project Flow",
    "section": "",
    "text": "Project Flow is intended to allow a user to flow easily past the different stages of software complexity. A common situation for an academic or a research software engineer (RES) to find themselves in is that they wrote a quick script to answer a specific quesiton, but it turned out to be useful in other context. This can lead to a the script grows and grows until complexity hurts its usefulness. A software developer would then think “oops, I should really make this modular.” ProjectFlow provides several modalities useful to researchers ranging from simple drop-in solution to complex scripting framework. To do this, ProjectFlow manages folders and defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks.\nProject Flow is intended to flow easily from the situation where you have coded a script that grows and grows until you think “oops, I should really make this modular.” Thus, it has several modalities useful to researchers ranging from simple drop-in solution to complex scripting framework. Similar to the Stages of Complexity page, we will introduce progressively more complex usages, starting with the simples examples and working towards distributed parallelization on complex task-trees. If you would like to skip the stages and go straight to a full-functioning example, go to Standard ProjectFlow run file.\n\n\nWe will use the third level of complexity from the previous example as the launching point for how to use ProjectFlow. The code below adds a few additional steps we will work with.\nimport os\nimport numpy as np\nimport gdal\n\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# Load the raster\nraster_input_path = 'data/yield.tif' # TODOO[make all these actually point to data in hazelbean]\narray = raster_to_array(raster_input_path)\n\n# Set no data values equal to zero\n\n# Method 1 (creates new array)\narray_ndv_fix = np.where(array == -9999, 0, array)\n\n# Method 2 (inplace)\narray[array == -9999] = 0\n\n# Sum the raster\nsum = np.sum(array_ndv_fix)\n\n# Calculate the average value on &gt;0 cells\n\n## First create a binary map of where there is positive value\nnon_zero = np.where(array_ndv_fix &gt; 0, 1, 0) \n\n## Count those\nn_non_zero = np.sum(non_zero)\n\n## Calculate the average\nmean = sum / n_non_zero\n\n## Write the value to a file\nwith open(output_path, rw) as f: # TODOO[fix my no copilot typo]\n    print('Write here.')\n\nprint('Sums of layers: ' + str(mean))",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#starter-code",
    "href": "earth_economy_devstack/project_flow.html#starter-code",
    "title": "Project Flow",
    "section": "",
    "text": "We will use the third level of complexity from the previous example as the launching point for how to use ProjectFlow. The code below adds a few additional steps we will work with.\nimport os\nimport numpy as np\nimport gdal\n\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# Load the raster\nraster_input_path = 'data/yield.tif' # TODOO[make all these actually point to data in hazelbean]\narray = raster_to_array(raster_input_path)\n\n# Set no data values equal to zero\n\n# Method 1 (creates new array)\narray_ndv_fix = np.where(array == -9999, 0, array)\n\n# Method 2 (inplace)\narray[array == -9999] = 0\n\n# Sum the raster\nsum = np.sum(array_ndv_fix)\n\n# Calculate the average value on &gt;0 cells\n\n## First create a binary map of where there is positive value\nnon_zero = np.where(array_ndv_fix &gt; 0, 1, 0) \n\n## Count those\nn_non_zero = np.sum(non_zero)\n\n## Calculate the average\nmean = sum / n_non_zero\n\n## Write the value to a file\nwith open(output_path, rw) as f: # TODOO[fix my no copilot typo]\n    print('Write here.')\n\nprint('Sums of layers: ' + str(mean))",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#iterating-over-many-model-assumptions",
    "href": "earth_economy_devstack/project_flow.html#iterating-over-many-model-assumptions",
    "title": "Project Flow",
    "section": "Iterating over many model assumptions",
    "text": "Iterating over many model assumptions\nIn many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:\n\nfor aggregation_label in p.aggregation_labels:\n     \n    for experiment_label in p.experiment_labels:\n        \n        for n_years_counter, ending_year in enumerate(p.years):\n            \n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))\nBut sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.\n\nfor index, row in p.scenarios_df.iterrows():\n    seals_utils.assign_df_row_to_object_attributes(p, row)\n    \n    if p.scenario_type != 'baseline':\n                            \n        for n_years_counter, ending_year in enumerate(p.years):\n\n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)\n            output_dir = p.get_path(current_run_dirs, str(ending_year))\n            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')\nIn this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via seals_utils.assign_df_row_to_object_attributes(p, row). This is used instead of the nested for loop.",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#creating-scenarios-spreadsheets",
    "href": "earth_economy_devstack/project_flow.html#creating-scenarios-spreadsheets",
    "title": "Project Flow",
    "section": "Creating scenarios spreadsheets",
    "text": "Creating scenarios spreadsheets\nHere. Explain why it writes the scenarios_csv FROM CODE rather than downloading it (keeps it up to date as code changes quickly). However, this gets convoluted when you also have to initialize the attributes before you write?!?\n\n    # If you want to run SEALS with the run.py file in a different directory (ie in the project dir)\n    # then you need to add the path to the seals directory to the system path.\n    custom_seals_path = None\n    if custom_seals_path is not None: # G:/My Drive/Files/Research/seals/seals_dev/seals\n        sys.path.insert(0, custom_seals_path)\n\n    # SEALS will run based on the scenarios defined in a scenario_definitions.csv\n    # If you have not run SEALS before, SEALS will generate it in your project's input_dir.\n    # A useful way to get started is to to run SEALS on the test data without modification\n    # and then edit the scenario_definitions.csv to your project needs.\n    # Some of the other test files use different scenario definition csvs \n    # to illustrate the technique. If you point to one of these \n    # (or any one CSV that already exists), SEALS will not generate a new one.\n    # The avalable example files in the default_inputs include:\n    # - test_three_scenario_defininitions.csv\n    # - test_scenario_defininitions_multi_coeffs.csvs\n    \n    p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n    # Set defaults and generate the scenario_definitions.csv if it doesn't exist.\n    if not hb.path_exists(p.scenario_definitions_path):\n        # There are several possibilities for what you might want to set as the default.\n        # Choose accordingly by uncommenting your desired one. The set of\n        # supported options are\n        # - set_attributes_to_dynamic_default (primary one)\n        # - set_attributes_to_dynamic_many_year_default\n        # - set_attributes_to_default # Deprecated\n\n        gtap_invest_utils.set_attributes_to_dynamic_gtap_default(p) # Default option\n\n\n        # # Optional overrides for us in intitla scenarios\n        # p.aoi = 'RWA'\n\n        # gtap_invest_utils.set_attributes_to_dynamic_default(p)\n        # Once the attributes are set, generate the scenarios csv and put it in the input_dir.\n        gtap_invest_utils.generate_gtap_invest_scenarios_csv_and_put_in_input_dir(p)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n    else:\n        # Read in the scenarios csv and assign the first row to the attributes of this object (in order to setup additional \n        # project attributes like the resolutions of the fine scale and coarse scale data)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n\n        # Because we've only read the scenarios file, set the attributes\n        # to what is in the first row.\n        for index, row in p.scenarios_df.iterrows():\n            seals_utils.assign_df_row_to_object_attributes(p, row)\n            break # Just get first for initialization.",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#task-format",
    "href": "earth_economy_devstack/project_flow.html#task-format",
    "title": "Project Flow",
    "section": "Task Format",
    "text": "Task Format\nProject flow requires a consistent format for tasks. The following is an example of a task that creates a correspondence file from gtap11 regions to gtapaez11 regions. The task itself defined as a function that takes a p object as an argument. This p object is a ProjectFlow object that contains all the project-level variables, manages folders and files, and manages tasks and parallelization. p also includes documentation, which will be written directly into the task directory.\nAlso note that any project-level attribute defined in between the function start and the if p.run_this: component are the “project level variables” that are fair-game for use in other tasks. These paths are critical for high performance because they enable quick-skipping of completed tasks and determiniation of which parts of the task tree need rerunning.\nTasks should be named as a noun (this breaks Python pep8 style) referencing what will be stored in the tasks output dir. This might feel awkward at first, but it means that the resultant file structure is easier to interpret by a non-EE outsider.\n\ndef gtap_aez_seals_correspondences(p):\n    p.current_task_documentation = \"\"\"\n    Create correspondence CSVs from ISO3 countries to GTAPv11 160\n    regions, and then to gtapaezv11 50ish regions, also put the classification\n    for seals simplification and luh.  \n    \"\"\"\n    p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')\n    p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')\n    p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')    \n\n    if p.run_this:\n        \n        \"logic here\"",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#automatic-directory-organization-via-tasks",
    "href": "earth_economy_devstack/project_flow.html#automatic-directory-organization-via-tasks",
    "title": "Project Flow",
    "section": "Automatic Directory Organization via Tasks",
    "text": "Automatic Directory Organization via Tasks\nHazelbean automatically defines directory organization as a function of the task tree. When the ProjectFlow object is created, it takes a directory as its only required input. This directory defines the root of the project. The other directory that needs to be referenced is the base_data_dir. When you initialize the p object, it notes this:\nCreated ProjectFlow object at C:\\Users\\jajohns\\Files\\gtap_invest\\projects\\cwon     from script C:\\Users\\jajohns\\Files\\gtap_invest\\gtap_invest_dev\\gtap_invest\\run_cwon.py     with base_data set at C:\\Users\\jajohns\\Files/base_data\nIn the run file, the following line generates the task tree:\ngtap_invest_initialize_project.build_extract_and_run_aez_seals_task_tree(p)\nWhich points to a builder function in the initialize file, looking something like this:\n\nThis would generate the following task tree:\n\nTwo notations are especially useful within this task tree.\n\nWithin the function that defines a task, p.cur_dir points to the directory of that task. So for instance, the last task defined in the image above, in its code, you could reference p.cur_dir, and it would point to &lt;project_root&gt;/econ_visualization/econ_lcovercom\nOutside of a given function’s code, you can still refer to paths that were defined from within the functions code, but now (because you are outside the function) it is given a new reference. Using the example above, you could reference the same directory with p.econ_lcovercom_dir where the p attribute is named exactly as &lt;function_name&gt;_dir\n\nAll of this setup enable another useful feature: automatic management of file generation, storage and downloading. This is done via the hazelbean function:\n\nuseful_path = hb.get_path(relative_path)\nThis function will iteratively search multiple locations and return the most “useful” one. By default, the relative_path variable will first joined with the p.cur_dir. If the file exists, it returns it. If not, it checks the next location, which is p.input_dir, and then p.base_data_dir. If it doesn’t find it anywhere, it will attempt to download it from google cloud (NYI) and save it in the p.cur_dir. If it is not available to download on google cloud, then it treats the path as something we will be generating within the task, and thus, get_path returns the first option above, namely joining the relative_path with p.cur_dir.\nOne important use-case that needs explaining is for tasks that generate files that will eventually be placed in the base_data_dir. The goal is to enable easy generation of it to the intermediate directory in the appropriate task_dir, but then have the ability to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches the desired directory relative to the base data dir. So, for example, we include 'gtappy', 'aggregation_mappings' at the beginning of the relative path for in the intermediate directory in the appropriate task_dir, but then we also will want to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches that in the base_data_dir, for example &lt;base_data_dir&gt;/'gtappy/aggregation_mappings/gadm_adm0.gpkg',\n\ntemplate_path = p.get_path(os.path.join('gtappy', 'aggregation_mappings', 'gadm_adm0.gpkg')) \nIt can be hard deciding what counts as a base_data_generating task or not, but generally if it is a file that will not be used by other projects, you should not treat it as a base_data_generating task. Instead, you should just make it relative to the cur_dir (or wahtever makes sense), as below:\n\noutput_path = p.get_path(os.path.join(aggregation_label + '_' + experiment_label + '_' + header + '_stacked_time_series.csv'))\nOne additional exception to the above is if you are calling get_path outside of a task/task_tree. One common example is in the run file before you build the task tree. In this case, the default_dirs will not make sense, and so you need to specify it manually as here:\n\np.countries_iso3_path = p.get_path(os.path.join('cartographic', 'gadm', 'gadm_adm0_10sec.gpkg'), possible_dirs=[p.input_dir, p.base_data_dir])",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#validation-of-files",
    "href": "earth_economy_devstack/project_flow.html#validation-of-files",
    "title": "Project Flow",
    "section": "Validation of files",
    "text": "Validation of files\nProjectFlow is designed to calculate very fast while simultaneously validating that everything is approximately correct. It does this by checking for the existence of files (often combined with hb.get_path()). For example\n\np.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'gadm_r263_gtapv7_r251_r160_r50_regions.gpkg'))     \nif not hb.path_exists(p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path):         \n    hb.log('Creating ' + p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path)         \n    \"computationally expensive thing here.\"\nProjectFlow very carefully defines whether or not you should run something based on the existence of specific paths. Usually this is just checking for each written path and only executing the code if it’s missing, but in some cases where lots of files are created, it’s possible to take the shortcut of just checking for the existence of the last-created path.\n\nEliminating redundant calculation across projects\nIf you have a time consuming task that, or example, writes to\n\nbig_file_path = hb.get_path('lulc', 'esa', 'seals7', 'convolutions', '2017', 'convolution_esa_seals7_2017_cropland_gaussian_5.tif' )\nIn this example, suppose you needed to create this file via your create_convolutions() task or something. When you first do this, it obviously won’t exist yet, so get_path() will join that relative path in the p.cur_dir location. If you run the ProjectFlow again, it will see it’s there and then instantly skip recalcualting it.\nIn addition to the 5 repos plus the EE repo, there is a managed base data set stored in teh same location\n\nA ProjectFlow object must have a base_data_dir set (I think…). This is because the p.get_path() will look in this folder for it, and/or will download to it.\n\n\n\nPython Tips and Conventions\nFor large files that take a long time to load, use a string-&gt;dataframe/dataset substitution as below. Make a LOCAL variable to contain the loaded object, and have that be assigned to the correct project-level path string. In subsequent usages, check type and if it’s still a string, then it hasn’t been loaded yet, so do that. I’m debating making it a project level variable trick\n\ngadm = p.gadm_adm0_vector_input_path    \n\n# Just load it on first pass\nif type(gadm) is str:\n    gadm = hb.read_vector(p.gadm_adm0_vector_input_path)",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html",
    "href": "earth_economy_devstack/organization.html",
    "title": "Organization",
    "section": "",
    "text": "Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github.\n\n\n\nNext, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section).\n\n\n\nNavigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories.\n\n\n\n\nThe earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#getting-the-earth_economy_devstack-repository",
    "href": "earth_economy_devstack/organization.html#getting-the-earth_economy_devstack-repository",
    "title": "Organization",
    "section": "",
    "text": "Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#get-the-other-repositories",
    "href": "earth_economy_devstack/organization.html#get-the-other-repositories",
    "title": "Organization",
    "section": "",
    "text": "Next, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section).",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#launching-the-devstack-in-vs-code",
    "href": "earth_economy_devstack/organization.html#launching-the-devstack-in-vs-code",
    "title": "Organization",
    "section": "",
    "text": "Navigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#launch-and-vs-code-configurations",
    "href": "earth_economy_devstack/organization.html#launch-and-vs-code-configurations",
    "title": "Organization",
    "section": "",
    "text": "The earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/magpie.html",
    "href": "earth_economy_devstack/magpie.html",
    "title": "Magpie",
    "section": "",
    "text": "git clone -b develop https://github.com/magpiemodel/magpie.git magpie_develop\nHas input and output folders in gitignore\nMAgpie has a config folder\ndefault.cfg\nis something huge, so use a start script which takes lines from default and overwrites.\nThen in the non-ignored starts folder, has a test_runs.R"
  },
  {
    "objectID": "earth_economy_devstack/magpie.html#installation",
    "href": "earth_economy_devstack/magpie.html#installation",
    "title": "Magpie",
    "section": "",
    "text": "git clone -b develop https://github.com/magpiemodel/magpie.git magpie_develop\nHas input and output folders in gitignore\nMAgpie has a config folder\ndefault.cfg\nis something huge, so use a start script which takes lines from default and overwrites.\nThen in the non-ignored starts folder, has a test_runs.R"
  },
  {
    "objectID": "earth_economy_devstack/index.html",
    "href": "earth_economy_devstack/index.html",
    "title": "Earth-Economy Devstack",
    "section": "",
    "text": "Earth-Economy Devstack\nThis is the documentation for the Earth-Economy Devstack, which is the set of repositories and code tools used by the Johnson-Polasky Lab and NatCap TEEMs. This documentation starts with overall organization of the Earth-Economy Devstack and discusses common coding practices used among the multiple releated repositories.",
    "crumbs": [
      "Earth-Economy Devstack"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html",
    "href": "earth_economy_devstack/hazelbean_overview.html",
    "title": "Hazelbean",
    "section": "",
    "text": "Hazelbean is a collection of geospatial processing tools based on gdal, numpy, scipy, cython, pygeoprocessing, taskgraph, natcap.invest, geopandas and many others to assist in common spatial analysis tasks in sustainability science, ecosystem service assessment, global integrated modelling assessment, natural capital accounting, and/or calculable general equilibrium modelling.\nNote that for all of the features of hazelbean to work, your computer will need to be configured to compile Cython files to C code. This workflow is tested in a Python 3.10, 64 bit Windows environment. It should work on other system environments, but this is not yet tested.\n\n\nFollow the instructions in the Earth-Economy Devstack repository.\n\n\n\nTest that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.\nimport hazelbean as hb\nFrom here, explore examples of the useful spatial, statistical and economic functions in the examples section of this documentation. A good starting example would be the zonal_statistics function. These functions are documented in their code declarations and in the Hazelbean Spatial Algorithms section. #TODOO[Make Link]. Anothr good starting point would be to understand the ProjectFlow() object, which is described in the project_flow page #TODOO[make link]\n\n\n\nHere is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))\n\n\n\nThis is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad.\n\n\n\nThe coding approach in level 2 becomes intractable when there are lots of layers to consider. It’s also a pain to have to repeat code to do some common tasks, like loading the raster to a dataset and then to an array. This complexity level starts to apply good coding practices, such as defining helper functions like raster_to_array() below. Code is also made much shorter and more elegant by using loops. It minimizes the number of code statements, reduces bugs and scales better to long lists of input files.\nimport os\nimport numpy as np\nimport gdal\n\n# NOTE 1: Helper function defined\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# NOTE 2: Inputs put into an iterable\ninput_paths = [\n    'yield_per_cell_1.tif',\n    'yield_per_cell_2.tif',\n    'yield_per_cell_3.tif',\n    'yield_per_cell_4.tif',\n]\n\n# NOTE 3: Calculation happens in loops, recording results to an output object\nsummations = []\nfor raster_path in input_paths:\n    array = raster_to_array(raster_path)\n    summations.append(np.sum(array))\n\nprint('Sums of layers: ' + str(summations))\n\n\n\nBelow is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Unlike the other levels, do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment! Instead, I provide a better example of code below that does things better, in the Earth-Economy Devstack way,\n\n\nimport logging\nimport os\nimport csv\nimport math, time, random\nfrom osgeo import gdal, gdalconst\nimport numpy as np\n\n# NOTE 1: I started to pull in Cython (Python code compiled to C for speed) because my code was getting slow\nimport pyximport\npyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n\n# NOTE 2: I wrote my own Python Library (geoecon_utils), which went through several more \n# iterations (Numdal, Lol!), until it got finalized as hazelbean\nimport geoecon_utils.geoecon_utils as gu\nimport geoecon_utils.geoecon_cython_utils as gcu\n\n# NOTE 3: Logging becomes important to manage information input-output used by the developer\nlog_id = gu.pretty_time()\nLOGGER = logging.getLogger('ag_tradeoffs')\nLOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\nfile_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\nLOGGER.addHandler(file_handler)\n\n# NOTE 4: Defining inputs and outputs is now based on a workspace, from which everything\n# else is defined with relative paths. Scales better with inputs and to other users.\nworkspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\nc_1km_file = 'c_1km.tif'\nc_1km_uri = workspace + c_1km_file\nha_per_cell_5m_file = 'ha_per_cell_5m.tif'\nha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n\n# NOTE 5: Here's an example of using custom libraries to \nha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n# NOTE 6: Here we start to deal with conditional running of code that skips outputs if they have already been\n# created. This is often the first (and often most eficatious) optimization of code to run fast.\ndo_30s_resample = False\nif do_30s_resample:\n    # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n    desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n    c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n    gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n\n# NOTE 7: An here, we see an incredibly slow approach that seems intuitive but is wrong\n# because it is 1000x slower than correct vectorized calculations (which are provided by numpy)\narray = raster_to_array(c_30s_unscaled_uri)\nfor row in range(array.shape[0]):\n    for col in range(array.shape[1]):\n        if array[row, col] &lt; 0:\n            array[row, col] = -9999 # Set negative values to the no-data-value\n\n\n\n# For reference, the correct way would have been as below. We will introduce hazelbean utils (like hb.as_array() soon.\narray = hb.as_array(input_path)\narray = np.where(array &lt; 0, -9999, array)\n\n\n\n\nDepending on the size of the array, the numpy where command used above would fail with a MemoryError or something similar. Below you will the first way that I dealt with this (BAD CODE) and then the correct way. In either case, it is almost always true that the most likely solution to larger-than-memory situations is to apply your algorithm in chunks. We’ll do this in both cases.\n\n\nIn this code, I created a thing I made up, a “Tile Reference” which I implemented with geoecon_utils.Tr(). This returned a set of tiles, defined by their row, column, x_width and y_width. We used these to load just subsets of the array and do operations on the smaller thing.\n\nliteral_aggregation_to_5m_cell = False\nif literal_aggregation_to_5m_cell:\n    factor =  10\n    shape = (2160, 4320)\n    c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n    cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n    cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n    for tile in c_30s_tr.tr_frame:\n        tile_array = c_30s_tr.tile_to_array(tile)\n        print 'Aggregating tile', tile\n        for row in range(c_30s_tr.chunkshape[0] / factor):\n            for col in range(c_30s_tr.chunkshape[1] / factor):\n                cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n                cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n    cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n    print gu.desc(cell_sum_5m)\n    gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n\n\n\nThe better way is to use a function that builds in the tiling functionality. Eventually this will expand to multi-computer approaches, but for now, we’ll just use the local but parallelized hb.raster_calculator()\nhb.raster_calculator(this)\n\n\n\n\nThe final level of complexity we will discuss (before just using the Earth Economy Devstack approach) arises when the number of files that must be managed becomes a challenge both for performance reasons and the challenges of managing complexity.\nOne example where this comes up is when a computation requires writing tiles of output. In many big-data applications and in most of the very large datasets that are available online, the data are themselves stored in tiles. On the one hand, this is nice because it automatically suggests a chunk-by-chunk parallelization strategy. On the other hand, it quickly becomes challenging when, for instance, you want to look at an area of interest (AOI) that spans multiple tiles. There are a plethora of software solutions to deal with this, such as GDAL’s Virtual Raster (VRT) file type, but many of these have limitations.\nWhen the computation in question requires many complex steps which might be contingent on other intermediate products from adjacent tiles, even some of the most cutting-edge solutions that implement complex tiling architecture (like DASK with rioxarray) will not be sufficient. This was the challenge that arose when doing downscaling with the SEALS model, especially when the algorithm had to be trained on such tiles millions of times. The optimized algorithm in this complex data and computation dependency-tree situation required a new tool, which would also have to address all of the above challenges in project complexity.\nThis led to ProjectFlow, one of the key tools within the Earth Economy Devstack and a part of Hazelbean.",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#installation",
    "href": "earth_economy_devstack/hazelbean_overview.html#installation",
    "title": "Hazelbean",
    "section": "",
    "text": "Follow the instructions in the Earth-Economy Devstack repository.",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#quickstart",
    "href": "earth_economy_devstack/hazelbean_overview.html#quickstart",
    "title": "Hazelbean",
    "section": "",
    "text": "Test that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.\nimport hazelbean as hb\nFrom here, explore examples of the useful spatial, statistical and economic functions in the examples section of this documentation. A good starting example would be the zonal_statistics function. These functions are documented in their code declarations and in the Hazelbean Spatial Algorithms section. #TODOO[Make Link]. Anothr good starting point would be to understand the ProjectFlow() object, which is described in the project_flow page #TODOO[make link]",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-1-simple-question-answered-well",
    "href": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-1-simple-question-answered-well",
    "title": "Hazelbean",
    "section": "",
    "text": "Here is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "href": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "title": "Hazelbean",
    "section": "",
    "text": "This is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad.",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-3-starting-to-deal-with-generalization-reusing-code-and-shortening-scripts.",
    "href": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-3-starting-to-deal-with-generalization-reusing-code-and-shortening-scripts.",
    "title": "Hazelbean",
    "section": "",
    "text": "The coding approach in level 2 becomes intractable when there are lots of layers to consider. It’s also a pain to have to repeat code to do some common tasks, like loading the raster to a dataset and then to an array. This complexity level starts to apply good coding practices, such as defining helper functions like raster_to_array() below. Code is also made much shorter and more elegant by using loops. It minimizes the number of code statements, reduces bugs and scales better to long lists of input files.\nimport os\nimport numpy as np\nimport gdal\n\n# NOTE 1: Helper function defined\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# NOTE 2: Inputs put into an iterable\ninput_paths = [\n    'yield_per_cell_1.tif',\n    'yield_per_cell_2.tif',\n    'yield_per_cell_3.tif',\n    'yield_per_cell_4.tif',\n]\n\n# NOTE 3: Calculation happens in loops, recording results to an output object\nsummations = []\nfor raster_path in input_paths:\n    array = raster_to_array(raster_path)\n    summations.append(np.sum(array))\n\nprint('Sums of layers: ' + str(summations))",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-4-starting-to-deal-with-performance-and-generalization.",
    "href": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-4-starting-to-deal-with-performance-and-generalization.",
    "title": "Hazelbean",
    "section": "",
    "text": "Below is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Unlike the other levels, do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment! Instead, I provide a better example of code below that does things better, in the Earth-Economy Devstack way,\n\n\nimport logging\nimport os\nimport csv\nimport math, time, random\nfrom osgeo import gdal, gdalconst\nimport numpy as np\n\n# NOTE 1: I started to pull in Cython (Python code compiled to C for speed) because my code was getting slow\nimport pyximport\npyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n\n# NOTE 2: I wrote my own Python Library (geoecon_utils), which went through several more \n# iterations (Numdal, Lol!), until it got finalized as hazelbean\nimport geoecon_utils.geoecon_utils as gu\nimport geoecon_utils.geoecon_cython_utils as gcu\n\n# NOTE 3: Logging becomes important to manage information input-output used by the developer\nlog_id = gu.pretty_time()\nLOGGER = logging.getLogger('ag_tradeoffs')\nLOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\nfile_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\nLOGGER.addHandler(file_handler)\n\n# NOTE 4: Defining inputs and outputs is now based on a workspace, from which everything\n# else is defined with relative paths. Scales better with inputs and to other users.\nworkspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\nc_1km_file = 'c_1km.tif'\nc_1km_uri = workspace + c_1km_file\nha_per_cell_5m_file = 'ha_per_cell_5m.tif'\nha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n\n# NOTE 5: Here's an example of using custom libraries to \nha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n# NOTE 6: Here we start to deal with conditional running of code that skips outputs if they have already been\n# created. This is often the first (and often most eficatious) optimization of code to run fast.\ndo_30s_resample = False\nif do_30s_resample:\n    # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n    desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n    c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n    gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n\n# NOTE 7: An here, we see an incredibly slow approach that seems intuitive but is wrong\n# because it is 1000x slower than correct vectorized calculations (which are provided by numpy)\narray = raster_to_array(c_30s_unscaled_uri)\nfor row in range(array.shape[0]):\n    for col in range(array.shape[1]):\n        if array[row, col] &lt; 0:\n            array[row, col] = -9999 # Set negative values to the no-data-value\n\n\n\n# For reference, the correct way would have been as below. We will introduce hazelbean utils (like hb.as_array() soon.\narray = hb.as_array(input_path)\narray = np.where(array &lt; 0, -9999, array)",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-5-dealing-with-larger-than-memory-data",
    "href": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-5-dealing-with-larger-than-memory-data",
    "title": "Hazelbean",
    "section": "",
    "text": "Depending on the size of the array, the numpy where command used above would fail with a MemoryError or something similar. Below you will the first way that I dealt with this (BAD CODE) and then the correct way. In either case, it is almost always true that the most likely solution to larger-than-memory situations is to apply your algorithm in chunks. We’ll do this in both cases.\n\n\nIn this code, I created a thing I made up, a “Tile Reference” which I implemented with geoecon_utils.Tr(). This returned a set of tiles, defined by their row, column, x_width and y_width. We used these to load just subsets of the array and do operations on the smaller thing.\n\nliteral_aggregation_to_5m_cell = False\nif literal_aggregation_to_5m_cell:\n    factor =  10\n    shape = (2160, 4320)\n    c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n    cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n    cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n    for tile in c_30s_tr.tr_frame:\n        tile_array = c_30s_tr.tile_to_array(tile)\n        print 'Aggregating tile', tile\n        for row in range(c_30s_tr.chunkshape[0] / factor):\n            for col in range(c_30s_tr.chunkshape[1] / factor):\n                cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n                cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n    cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n    print gu.desc(cell_sum_5m)\n    gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n\n\n\nThe better way is to use a function that builds in the tiling functionality. Eventually this will expand to multi-computer approaches, but for now, we’ll just use the local but parallelized hb.raster_calculator()\nhb.raster_calculator(this)",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-6-systematic-file-management-file-management.",
    "href": "earth_economy_devstack/hazelbean_overview.html#project-complexity-level-6-systematic-file-management-file-management.",
    "title": "Hazelbean",
    "section": "",
    "text": "The final level of complexity we will discuss (before just using the Earth Economy Devstack approach) arises when the number of files that must be managed becomes a challenge both for performance reasons and the challenges of managing complexity.\nOne example where this comes up is when a computation requires writing tiles of output. In many big-data applications and in most of the very large datasets that are available online, the data are themselves stored in tiles. On the one hand, this is nice because it automatically suggests a chunk-by-chunk parallelization strategy. On the other hand, it quickly becomes challenging when, for instance, you want to look at an area of interest (AOI) that spans multiple tiles. There are a plethora of software solutions to deal with this, such as GDAL’s Virtual Raster (VRT) file type, but many of these have limitations.\nWhen the computation in question requires many complex steps which might be contingent on other intermediate products from adjacent tiles, even some of the most cutting-edge solutions that implement complex tiling architecture (like DASK with rioxarray) will not be sufficient. This was the challenge that arose when doing downscaling with the SEALS model, especially when the algorithm had to be trained on such tiles millions of times. The optimized algorithm in this complex data and computation dependency-tree situation required a new tool, which would also have to address all of the above challenges in project complexity.\nThis led to ProjectFlow, one of the key tools within the Earth Economy Devstack and a part of Hazelbean.",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html",
    "href": "earth_economy_devstack/gtappy.html",
    "title": "GTAPPy",
    "section": "",
    "text": "Step 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want.\n\n\n\n\nGetting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database.\n\n\n\nGTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression.\n\n\n\n\n\nWe need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\n\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\n\n\n\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\n\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\n\n\n\n\n\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\n\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#installation",
    "href": "earth_economy_devstack/gtappy.html#installation",
    "title": "GTAPPy",
    "section": "",
    "text": "Step 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "href": "earth_economy_devstack/gtappy.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "title": "GTAPPy",
    "section": "",
    "text": "Getting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#iterating-over-multiple-aggregations",
    "href": "earth_economy_devstack/gtappy.html#iterating-over-multiple-aggregations",
    "title": "GTAPPy",
    "section": "",
    "text": "GTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#release-notes",
    "href": "earth_economy_devstack/gtappy.html#release-notes",
    "title": "GTAPPy",
    "section": "",
    "text": "We need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\n\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\n\n\n\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\n\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\n\n\n\n\n\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\n\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html",
    "href": "earth_economy_devstack/conventions.html",
    "title": "Conventions",
    "section": "",
    "text": "In programming, id and index are two different concepts that are used in different contexts.\nid refers to the unique identifier of an object. In this specification, it is an integer that is sorted by the ONE in the many-to-one correspondence, sorted alphabetically at generation time (though not necessarily to remain sorted given downstream correspondences).\nindex refers to the position of an element in a sequence (e.g. a list or a string). In the context of a correspondence file, this is the position of the row within the sorted spreadsheet, but is not assumed to be stable and shouldn’t generally be used.\nlabelheader refers to an (exactly) 4 character string that is lowercase-alphanumeric with no special symbols besides underscore. Useful for the Header label in har files. Technically is case insensitive but we assume lowercase.\nlabelshort refers to an 8-character or less string that is lowercase-alphanumeric with no special symbols besides underscore. Useful for .har files.\nlabel refers to a string that is lowercase-alphanumeric with no special symbols besides underscore\nname refers to a string of any ascii characters with python-style escaping of special characters: 'She\\'s a star!' . It’s assumed to be short enough to view as a column header or plot label\nDescriptionrefers to a name of any length with detailed description, possibly even formatted MD text.\nIf there is a domain, described below, id, index, etc all should prepended with it to be eg gadm_id.\n\n\n\nNote that geopandas assumes the vector data are indexed with an FID. This is the order in which the geometries are added to the file and can get wonky when dealing with legacy file types (like ESRI Shapefiles). Additionally, when you write a GPKG to a CSV, it will not include the fid, so you might lose data. To fix this, EE spec requires that any GPKG when saved as a CSV have a new column, id added as the first col, which is generated starting at 1 and incrementing up by 1 after having sorted the data on the simplest non-fid label (e.g., iso3_label). See gtap_invest_generate_base_data.py.\n\n\n\nBased on how the GTAP database is structured, EE spec defines several file types of files to systemetize how dimensions/sets are defined (and then used in e.g. figure plotting). A single dimension is first defined by a labels file. The labels file has at least 3 columns of domain_id, domain_label, domain_name and optionally a domain_description. If present, a column needs to be fully filled (no missing values). Label files are used in other contexts to, e.g., go from id to name for labeling an axis on a plot, as well as building the correspondence files below.\n\n\n\nModel linkages often require mapping many-to-one relationship in a consistent way. Correspondence files define this via a src-to-dst (source and destination). They are named according to a relatively complex pattern. Specifically, using the file path gadm_r263_gtapv7_r251_r160_r50_correspondence, we have a domain label gadm followed by a src dimension-size pair r263 (where r is a label, short for region in this case, and 263 is the number of unique entries in that dimension). To be a correspondence,there needs to be at least one other dst dimension-size pair, where in this case there are 3 additional dimension-size pairs (r251, r160, and r50). However, the later three pairs are from a different domain, namely that of gtapv7. Each pair is identified with the domain most identified most closely prior. The dst dimension-size pairs are sorted in order of decreasing size. The dst dimension-size pairs are then followed by the word correspondence. This example creates a correspondence file that maps from the GADM 263 regions to the GTAPv7 251 regions, which are then mapped to the GTAPv7 160 regions, which are then mapped to the GTAPv7 50 regions.\nAn example of a 2-type correspondence is below. However, in this file, src and dst would have to be replaced with the specific domain names used.\n\n\n\n\n\n\n\n\n\n\n\nsrc_id\ndst_id\nsrc_label\ndst_label\nsrc_description\ndst_description\n\n\n\n\n1\n1\naus\noceania\nAustralia\nOceania (including NZ and AUS)\n\n\n2\n1\nnzl\noceania\nNew Zealand\nOceania (including NZ and AUS)\n\n\n\nHere are the specific labels and corespondence files generated for gtapv7-aez-rd:\n\nIf defined exactly right, 2 dimensional correspondence files will work with Hazelbean via\nseals_utils.set_derived_attributes(p)\nand\n\np.lulc_correspondence_dict = hb.utils.get_reclassification_dict_from_df(p.lulc_correspondence_path, 'src_id', 'dst_id', 'src_label', 'dst_label')}`\n\n# return a very useful dictionary for various reclassification tasks:\n\nreturn_dict = {} return_dict['dst_to_src_reclassification_dict'] = dst_to_src_reclassification_dict  # Dict of one-to-many keys to lists of what each dst_key should be mapped to from each src_key. Useful when aggrigating multiple layers to a aggregated dest type \nreturn_dict['src_to_dst_reclassification_dict'] = src_to_dst_reclassification_dict # Useful when going to a specific value. \nreturn_dict['dst_to_src_labels_dict'] = dst_to_src_labels_dict # Dictionary of lists of labels that map to each dst label \nreturn_dict['src_ids'] = remove_duplicates_in_order(src_ids) # Unique set of src_ids r\nreturn_dict['dst_ids'] = remove_duplicates_in_order(dst_ids) # Unique set of dst_ids \nreturn_dict['src_labels'] = remove_duplicates_in_order(src_labels) # Unique set of src_labels \nreturn_dict['dst_labels'] = remove_duplicates_in_order(dst_labels) # Unique set of dst_labels \nreturn_dict['src_ids_to_labels'] = {k: v for k, v in zip(return_dict['src_ids'], return_dict['src_labels'])} # one-to-one dictionary of src ids to labels \nreturn_dict['dst_ids_to_labels'] = {k: v for k, v in zip(return_dict['dst_ids'], return_dict['dst_labels'])} # one-to-one dictionary of dst ids to labels \nreturn_dict['src_labels_to_ids'] = {k: v for k, v in zip(return_dict['src_labels'], return_dict['src_ids'])} # one-to-one dictionary of src labels to ids \nreturn_dict['dst_labels_to_ids'] = {k: v for k, v in zip(return_dict['dst_labels'], return_dict['dst_ids'])} # one-to-one dictionary of dst labels to ids}\nAmong other possibilities, this could be used for reclassifying LULC geotiffs via\n\nrules = p.lulc_correspondence_dict['src_to_dst_reclassification_dict'] hb.reclassify_raster_hb(raster_path, rules, output_path)\n\n\n\nOne special case of ids is when two different ids are combined together leveraging their decimal position to compress data. For example, if you want a Region-AEZ-specific id stored in single column, you can do that by saying the joined id is 5 digits long, the first three correspond to ee_r264 and the final two correspond to aez18, as in this example:\n\nIn the event of a combined id, not that the _id column above has two region-specifications combined (which otherwise violates the ee spec defined above for non combined ids).\n\n\n\n\n# The rules for naming correspondences are a little different when adding geometry to the data. Because}     \n# # only 1 geometry can be assigned per file, and becasue membership of aggregated regions and their     \n# # members can get confusing, each correspondence file keeps all the labels but then is also saved along     \n# # with a geometry file that drops the other labels (and the word correspondence in the filename).     \np.ee_r264_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'ee_r264_correspondence.gpkg'))     \np.ee_r264_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'ee_r264.gpkg'))\n\n\n\nThe names of project-level variables are carefully defined. For example, in gtapv7_r251_r160_correspondence_input_path,\n\nThe input when put right before the word path implies it is a raw asset we obtained from an external source but haven’t processed it yet. This means that it can be a non-compliant XLSX file. Aslo, path implies that it is a string type that points to a location in a storage device.\nIn this name, we see two other structures. First, the gtapv7 label indicates the “domain” of the correspondence defined in all of the following dimensions (until another domain label). Above, this means that it is the 251 regions, as defined by the gtapv7 domain, mapped to the 160 regions in the same domain.\n\nYou can have multiple mappings in a single correspondence file. For example, gtapv7_r251_s65_r50_s26_correspondence_input_path, we are mapping r251 to r50 and s (sectors) 65 to s26, all in the gtapv7 domain.\nAppart from correspondence files, we also have labels files, such as gtapv7_r251_labels_path. Labels files are for a single set/variable/dimension but map together the synonymous categorizers. Specifically, it must have an id, label, and name, all filled out for every entry. It can optionally have others like description.\nIn the filename gtapv7_r251_labels_path we extract from the input_path and write a EE-compliant labels table for the gtapv7 r251 set. In the filename gtap11_gtapaez11_region_correspondence_path, we use these regions and then connect it to the gtapaez11 labels. We can infer also that it is from a mapping labeled gtap11 to gtapaez11 and that the variable in question is the regions while the word correspondence then indicates this is a many-to-one mapping file.\nNote that the file path says “regions” while the column label in the CSV says “region”.\n\np.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')     \np.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')     \np.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')\n\n\n\nTo keep track of the MANY different filetypes, data processes, variables, scenarios, policies etc, please follow exactly the specifications below.\n\nThe word label refers to a relatively short string (preferably 8 characters long or less) with no spaces, underscores or punctuation (but may have hyphens). This is case-sensitive, but try to avoid capitalization.\nThe word short_label refers to a label that is strictly less or equal to 8 characters to ensure compatibility with HAR files.\nThe word name refers to a longer string that describes a specific label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary.\nThe words index, indices or id refers to numerical data that describes a label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary. If both are used, index/indices refer to a unique, ordered list of ints while an id/ids refer are unique but not necessarily ordered.\nThe word class refers to LULC class. Consider renaming this to lc-class?\nScenarios are defined in the following nested structure:\n\nLabel (with no hyphens) for Exogenous Assumptions (e.g., which SSP, which GDP, which population). Typically this will be fully defined by the SSP.\nLabel (with no hyphens) for Climate Assumption (which RCP)\nLabel (can have hyphens) for which model is used (e.g., magpie, luh2-message). Only model is allowed to have hyphens (because they are used for multistep scenario processing of counterfactuals)\nLabel for Counterfactual. This often represent policy Assumptions/Definition and can include BAU, which is a special counterfactual against which other policies are compared. Different counterfactuals correspond to different shockfiles in the econ model or different LUC projection priorities, etc.\n\nCounterfactuals may have multiple processing steps, which will be denoted by appending a hyphen and exactly 4 chars to the end of the base counterfactual label.\n\nIFor example, a run excludes consideration of ES, insert “-noes”, at the end of the policy_name if it does include ES, postpend nothing (as this will be the one that is referenced by default)\n\n\nYear\n\nWhen the variable is singular, it must be an int. If it is plural, as is ints in a list. However, when either is stored in a dataframe, always type always type check as follows:\n\nIf singular, do str(value), int(value) or float(value) as appropriate when reading from the df into a python variable.\nIf plural, assume the df value is a space-delimited string that needs to be split, e.g. as [int(i) for i in value.split(’ ‘)], or’ ’.join(values) if going into the DF\n\nThree types of years exist, including\n\np.base_years, (which recall will always be a list even if there is a single entry because the variable name is plural)\n\n\n\nTogether, the labels above mean that the scenarios can be represented by directories as follows:\n\nssp2/rcp45/luh2-message/bau/filename_2050.tif\n\nNote, the last layer of the hierarchy will be included as a the suffix of the filename start rather than as a directory (also see below for filename conventions)\n\n\nFor filenames, there are two possible conventions:\n\nImplied: This means that the directory structure above defines all the labels with the exception of year (which is postpended to the filename label) and the current variable name (such as lulc) which appears at the front of the filename.\n\ne.g., project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif\n\nExplicit: Even if a file is in a directory which implies its labels, explicit file naming will always include each label (and the variable label stays in front of the filename), so the above example is:\n\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\nAnd if there are no ES considered, it would be project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\n\n\nBecause labels have no spaces or underscores, it is possible to convert the nested structure above to a single string, as in filename_ssp2_rcp45_policyname_year.tif.\nFiletypes that supported in this computation environment include\n\nNetcdf with the above listed dimensions in the same order\nA set of geotiffs embedded in directories. Each label gets a directory level expect for year, which by convention, will ALWAYS be the last 4 characters of a filename before the extension (with an underscore before it).\nA spreadsheet linkable to a geographic representation (e.g., a shapefile or a geopackage) in vertical format\n\nAlso we will create a set of tables to analyze results\n\nThese will define Regions (shp) for quick result plotting\nSpecifically, we will have a full vertically stacked CSV of results, then for each Report Archetype we would output 1 minimal info CSV and the corresponding Figure.\n\nMiscellaneous:\n\nbase_years is correct, never baseline_years (due to confusion between baseline and bau)\n\nScenario types\n\nThree scenario_types are supported: baseline, bau and policy\n\nBaseline assumes the year has data existing from observations (rather than modelled) and that these years are defined in p.years (and identically defined in p.base_years).\n\nOne exception is when eg GTAP is used to update the base year from 2017 to 2023, and then policies are applied on 2023.\n\nBAU and policy scenarios assume the results are modelled and that their years are defined in p.years (but not p.base_years)\n\nClarify what is the naming difference between src dst versus input output. Is the former only for file paths or can it also be e.g. array. OR does this have to do with if it is a function return.\n\nProposed answer: src/dst is a pointer/reference to a thing and Input/Output is the thing itself. Esp useful for paths.\nSimilarly, _path and _dir imply the string is a reference, so src_path and src_dir are common.\nYou might often see e.g. input_array = hb.as_array(src_path), illustrating this difference.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#variable-and-scenario-naming-conventions",
    "href": "earth_economy_devstack/conventions.html#variable-and-scenario-naming-conventions",
    "title": "Conventions",
    "section": "",
    "text": "To keep track of the MANY different filetypes, data processes, variables, scenarios, policies etc, please follow exactly the specifications below.\n\nThe word label refers to a relatively short string (preferably 8 characters long or less) with no spaces, underscores or punctuation (but may have hyphens). This is case-sensitive, but try to avoid capitalization.\nThe word short_label refers to a label that is strictly less or equal to 8 characters to ensure compatibility with HAR files.\nThe word name refers to a longer string that describes a specific label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary.\nThe words index, indices or id refers to numerical data that describes a label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary. If both are used, index/indices refer to a unique, ordered list of ints while an id/ids refer are unique but not necessarily ordered.\nThe word class refers to LULC class. Consider renaming this to lc-class?\nScenarios are defined in the following nested structure:\n\nLabel (with no hyphens) for Exogenous Assumptions (e.g., which SSP, which GDP, which population). Typically this will be fully defined by the SSP.\nLabel (with no hyphens) for Climate Assumption (which RCP)\nLabel (can have hyphens) for which model is used (e.g., magpie, luh2-message). Only model is allowed to have hyphens (because they are used for multistep scenario processing of counterfactuals)\nLabel for Counterfactual. This often represent policy Assumptions/Definition and can include BAU, which is a special counterfactual against which other policies are compared. Different counterfactuals correspond to different shockfiles in the econ model or different LUC projection priorities, etc.\n\nCounterfactuals may have multiple processing steps, which will be denoted by appending a hyphen and exactly 4 chars to the end of the base counterfactual label.\n\nIFor example, a run excludes consideration of ES, insert “-noes”, at the end of the policy_name if it does include ES, postpend nothing (as this will be the one that is referenced by default)\n\n\nYear\n\nWhen the variable is singular, it must be an int. If it is plural, as is ints in a list. However, when either is stored in a dataframe, always type always type check as follows:\n\nIf singular, do str(value), int(value) or float(value) as appropriate when reading from the df into a python variable.\nIf plural, assume the df value is a space-delimited string that needs to be split, e.g. as [int(i) for i in value.split(’ ‘)], or’ ’.join(values) if going into the DF\n\nThree types of years exist, including\n\np.base_years, (which recall will always be a list even if there is a single entry because the variable name is plural)\n\n\n\nTogether, the labels above mean that the scenarios can be represented by directories as follows:\n\nssp2/rcp45/luh2-message/bau/filename_2050.tif\n\nNote, the last layer of the hierarchy will be included as a the suffix of the filename start rather than as a directory (also see below for filename conventions)\n\n\nFor filenames, there are two possible conventions:\n\nImplied: This means that the directory structure above defines all the labels with the exception of year (which is postpended to the filename label) and the current variable name (such as lulc) which appears at the front of the filename.\n\ne.g., project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif\n\nExplicit: Even if a file is in a directory which implies its labels, explicit file naming will always include each label (and the variable label stays in front of the filename), so the above example is:\n\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\nAnd if there are no ES considered, it would be project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\n\n\nBecause labels have no spaces or underscores, it is possible to convert the nested structure above to a single string, as in filename_ssp2_rcp45_policyname_year.tif.\nFiletypes that supported in this computation environment include\n\nNetcdf with the above listed dimensions in the same order\nA set of geotiffs embedded in directories. Each label gets a directory level expect for year, which by convention, will ALWAYS be the last 4 characters of a filename before the extension (with an underscore before it).\nA spreadsheet linkable to a geographic representation (e.g., a shapefile or a geopackage) in vertical format\n\nAlso we will create a set of tables to analyze results\n\nThese will define Regions (shp) for quick result plotting\nSpecifically, we will have a full vertically stacked CSV of results, then for each Report Archetype we would output 1 minimal info CSV and the corresponding Figure.\n\nMiscellaneous:\n\nbase_years is correct, never baseline_years (due to confusion between baseline and bau)\n\nScenario types\n\nThree scenario_types are supported: baseline, bau and policy\n\nBaseline assumes the year has data existing from observations (rather than modelled) and that these years are defined in p.years (and identically defined in p.base_years).\n\nOne exception is when eg GTAP is used to update the base year from 2017 to 2023, and then policies are applied on 2023.\n\nBAU and policy scenarios assume the results are modelled and that their years are defined in p.years (but not p.base_years)\n\nClarify what is the naming difference between src dst versus input output. Is the former only for file paths or can it also be e.g. array. OR does this have to do with if it is a function return.\n\nProposed answer: src/dst is a pointer/reference to a thing and Input/Output is the thing itself. Esp useful for paths.\nSimilarly, _path and _dir imply the string is a reference, so src_path and src_dir are common.\nYou might often see e.g. input_array = hb.as_array(src_path), illustrating this difference.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#setting-the-environment",
    "href": "earth_economy_devstack/conventions.html#setting-the-environment",
    "title": "Conventions",
    "section": "Setting the environment",
    "text": "Setting the environment\n\nWhat does this mean?\n\nLet’s find the executable we installed with Mamba\n\n\nWhen we set the interpretter/environment in VS Code, we are essentially telling it to look for this python.exe file in this exact folder.\n\nTo get the path, we can right click on the file and select “Copy Path”\n\n\n\n\nWhy might you need the actual path?\n\nOn the command line, when we call python this_script.py sometimes it will fail to find (or find the right) python executable to call. We could have been explicit by calling C:\\Users\\jajohns\\mambaforge\\envs\\env2023a\\python.exe this_script.py instead.\nAdditionally, when you are publishing Quarto Books or Jupyter Notebooks, you might want to set the path explicitly\n\nFor example, to publish the EE Devstack, I have a line in the quarto.yml file like this:\nyaml     execute:         engine: python         python: C:\\Users\\jajohns\\mambaforge\\envs\\env2023a\\python.exe",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "An abbreviated version of my CV can be downloaded here. For the full and most recent CV, please contact me.\nCV – Justin Andrew Johnson"
  },
  {
    "objectID": "bio/index.html",
    "href": "bio/index.html",
    "title": "Bio",
    "section": "",
    "text": "Justin Andrew Johnson is an Assistant Professor of Applied Economics at the University of Minnesota. He received his Ph.D. from the department in 2014. Justin works closely with the Natural Capital Project at the University of Minnesota and Stanford University. Justin’s research focuses on how the economy affects the environment, and vice versa, on global to local scales. Currently, Justin leads a project that links the Global Trade Analysis Project (GTAP) out of Purdue University with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model from the Natural Capital Project, aiming to build strong quantitative evidence on how changes in ecosystem services affect economic performance at the macroeconomic level and how global policies can be designed to sustainably manage our natural capital."
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "I have two books currently in production. Both are open-access and open-source. The first is a principles of microeconomics book and the second is a book on the economics of the environment. Both are written in markdown and converted to html, pdf, epubs, etc. using quarto. The html files are then hosted on github pages.\n\n\nCan be accessed at Open Principles of Microeconomics\n\n\n\nNot yet public, but soon will be hosted here. This is a book about the GTAP-InVEST model and the related set of similar models and tools."
  },
  {
    "objectID": "books/index.html#open-principles-of-microeconomics",
    "href": "books/index.html#open-principles-of-microeconomics",
    "title": "Books",
    "section": "",
    "text": "Can be accessed at Open Principles of Microeconomics"
  },
  {
    "objectID": "books/index.html#earth-economy-modeling",
    "href": "books/index.html#earth-economy-modeling",
    "title": "Books",
    "section": "",
    "text": "Not yet public, but soon will be hosted here. This is a book about the GTAP-InVEST model and the related set of similar models and tools."
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Data",
    "section": "",
    "text": "Data used in my work is free to use (though of course I appreciate citation and/or collaboration!). The following are links to several frequently frequently requested data sets. Feel free to contact me for other supporting documents from publications."
  },
  {
    "objectID": "data/index.html#globally-harmonized-carbon-storage-data-using-a-simple-decision-tree-approach",
    "href": "data/index.html#globally-harmonized-carbon-storage-data-using-a-simple-decision-tree-approach",
    "title": "Data",
    "section": "Globally Harmonized Carbon Storage Data using a Simple Decision Tree Approach",
    "text": "Globally Harmonized Carbon Storage Data using a Simple Decision Tree Approach\nResults can be downloaded here: data download."
  },
  {
    "objectID": "data/index.html#global-food-demand-and-carbon-preserving-cropland-expansion-under-varying-levels-of-intensification",
    "href": "data/index.html#global-food-demand-and-carbon-preserving-cropland-expansion-under-varying-levels-of-intensification",
    "title": "Data",
    "section": "Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification",
    "text": "Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification\nAll new data from this paper can be downloaded at https://drive.google.com/drive/folders/0BxVzEBY6ApU5Yk9WX0p0LUdNM28 If a map from the paper is not available it is publicly available on the data creator's site."
  },
  {
    "objectID": "data/index.html#global-agriculture-and-carbon-trade-offs",
    "href": "data/index.html#global-agriculture-and-carbon-trade-offs",
    "title": "Data",
    "section": "Global agriculture and carbon trade-offs",
    "text": "Global agriculture and carbon trade-offs\nAll new data from this paper can be downloaded at https://drive.google.com/folderview?id=0BxVzEBY6ApU5dkNLcUhMbEVkdU0  If a map from the paper is not available it is publicly available on the data creator's site."
  },
  {
    "objectID": "data/index.html#data-for-mesh-mapping-ecosystem-services-to-human-well-being",
    "href": "data/index.html#data-for-mesh-mapping-ecosystem-services-to-human-well-being",
    "title": "Data",
    "section": "Data for MESH (Mapping Ecosystem Services to Human well-being)",
    "text": "Data for MESH (Mapping Ecosystem Services to Human well-being)\nSee the MESH page for more details. Data is now hosted by The Natural Capital Project at https://naturalcapitalproject.stanford.edu/software/mesh and is distributed as a part of the software installation."
  },
  {
    "objectID": "earth_economy_devstack/global_invest.html",
    "href": "earth_economy_devstack/global_invest.html",
    "title": "Global InVEST",
    "section": "",
    "text": "Global InVEST\nThe standard version of InVEST, provided by the Natural Capital Project, is a set of tools for quantifying the values of natural capital in clear, credible, and practical ways. InVEST enables decision-makers to assess the trade-offs associated with alternative management choices and to identify the policies and practices that can best balance human needs with environmental sustainability. InVEST is designed to be accessible to a broad audience, including conservation organizations, governments, development banks, academics, and the private sector.\nThe InVEST software comprises an easy-to-use graphical interface version as well as direct access to the Python library for advanced users. Typical InVEST applications looked at individual watersheds or small administrative regions. Starting withChaplin-Kramer et al. (2019), however, a series of global applications of InVEST have developed. These applications have sometimes used the standard InVEST Python library, but in most cases, custom versions were created to enable calculation of the models globally (which was challenging for both computation and data reasons). The term Global InVEST refers informally to the multiple code repositories behind these applications. Although the code-base for Global InVEST is still fragmented, we are working to organize and standardize these models.\nThe three main sources for Global InVEST is\n\nNature’s Contributions to People: Chaplin-Kramer et al. (2019)\nGTAP-InVEST: Johnson et al. (2023)\nNature’s Frontiers: Damania et al. (2023)",
    "crumbs": [
      "Global InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtap_invest.html",
    "href": "earth_economy_devstack/gtap_invest.html",
    "title": "GTAP-InVEST",
    "section": "",
    "text": "GTAP-InVEST\nAlthough GTAP-InVEST is a part of the Earth-Economy Devstack, the GTAP-InVEST User Guide is hosted under the GTAP-InVEST home page.",
    "crumbs": [
      "GTAP-InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/how_to_contribute.html",
    "href": "earth_economy_devstack/how_to_contribute.html",
    "title": "How to contribute",
    "section": "",
    "text": "Adding your own code to the Earth Economy Devstack is Easy! After you have cloned\n\n\n\nThe Earth Economy Devstack and all of its published article code is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system git (https://git-scm.com/). git allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system git can vary and there generally is no ‘right’ or ‘wrong’ in collaborative software development.\nThere are two different ways to interact with a github repository: 1. making a fork; 2; working on a branch. This section describes both options, however internal members of TEEMs will usually use branches.\n\n\n\n\n\nHowever, some basic preferred workflows are outlined in the following.\n\n\n\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for - code development or fixes - submitting a pull request to the original SEALS repository.\n\n\n\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. To clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;.\n\n\n\n\n\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.\n\n\n\nWheter you are working on a Fork or on the seals repository itself, we will organize contributions with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\n\nSuppose you want to make a change to the hazelbean_dev repository. We will use this example to explain how we use Git Graph to make this contribution. First, let’s take a look at the Git Graph interface.\n\nFirst note the Repo dropdown box indicating we are looking at the hazelbean_dev repo. Here you could switch to other Repos that have been added to your workspace. The next thing to note are the three different branch tages (Blue, Magenta and Green boxes on the Git Graph tree view). These are the three branches that are currently in the hazelbean_dev repository. The blue branch is the main branch, the magenta branch is the develop branch and the green branch is a branch specific to the user and/or the feature. In this case, it is develop_justin branch. The lines and dots indicates the history of how files were edited and then how the different branches were merged back together after an edit. By default, when you clone a repository, you will be on the main branch. To switch to another branch, right-click the tag and select checkout branch. When you do this, the files on your harddrive will be changed by Git to match the status of the branch you just checked out. In the image below, we can see which is the current branch because it has an open-circle dot in the graph and the name is bold. Other things to notice is that after the name of the branch there is the word origin. This indicates that the branch is synced with the remote repository (named origin by convention).\nTo make a change, you will first want to create your own branch. First, make absolutely certain you currently have checked out the develop branch (unless you know why to do otherwise). We will use main branch only to contain working “releases”. Once develop is checked out, use the Command Pallate (ctrl-shift-p) and search for create branch.\n\nChoose the right repository among those loaded in your workspace, hazelbean_dev in this example.\n\nThen select it and give the new branch a name, either develop_&lt;your_name&gt; or feature_&lt;your_feature_name&gt;. Below we have created a new feature branch called feature_test branched off of the develop branch. It should look like this\n\nYou’ll notice that the tag is bold, indicating you have checked it out and it is your current branch. Also notice though that it does not have the tag origin after it, indicating that it is not synced with the remote repository. To sync it, you will need to push it to the remote repository. To do this, right-click the tag and select push branch. This will push the branch to the remote repository and it will be available to other users.\nAnother way to push the branch is using the Version Control tab in VS Code. Click the Version Control tab on the Lefthand bar. There you will see all of the repos you have loaded into our workspace. For hazelbean_dev, you will see it has a publish branch button. Clicking this will have the same effect as the push branch command in Git Graph.\n\nRegardless of how you pushed it to the remote repository, you will now see the branch has the ‘origin’ tag after it, indicating it is synced with the remote repository. It is now also the checked-out branch and so all changes you make will be made to this branch.\n\nNow, we’re going to make a small change to the code. In the VS Code file explorer, I will open up file_io.py in the hazelbean module and scroll to the function I want to edit.\n\nOn lines 778-779, you’ll see VS Code has greyed the variables out indicating they are not used, so I will remove them. Once gone, you’ll see a blue bar on the left side of the editor. This is Git and VS Code indicating to you you made a change. Blue indicates a modification where as green indicates a new addition and red indicates a deletion.\n\nTo illustrate a new addition, I will add a line at to space out our for loop. Once I do this, you’ll see the green bar on the left side of the editor. In the image you can also see that the file_io.py is now a different color and has an M next to it. This indicates that the file has been modified.\n\nAnother useful thing you can do is click on the Blue or Green edit bar to see more details, as below.\n\nHere, you can see the exact changes you made. Additionally, there are buttons at the top of this new box that let you revert the change to go back to how it was before.\nBefore we commit our changes, look at the Git Graph view. You’ll see that we now have a new grey line coming from the feature_test branch, showing that we have uncommitted changes. You could click on the uncommitted changes link to see the edits we made.\n\nWe are now going to commit our changes. To do this, go to the Version Control Tab. You will now see that there is a change listed under the hazelbean_dev repository. Click on the change to see the details. You will see a more detailed “diff editor” that lets you understand (or change) what was edited. To accept these changes, we will click the commit button. But first write a short commit message. Click Commit (but don’t yet click the next button to Sync changes). After committing, look back at the Git Graph view. You’ll see that the grey line is now gone and the blue feature_test tag is at the top of our commit tree along with our comitt message.\n\nNotice though that the tag for origin/feature_test is not yet up at the new location. This is because we have not yet pushed our changes to the remote repository. To do this, click the Sync button in the bottom right of the VS Code window. This will push your changes to the remote repository and update the tag to the new location, like this.\n\nYour code is now on GitHub and other contributors with access could check it out and try it. But, this code will be different than their code and if you made non-trivial changes, it could be hard to keep straight what is going on. To clarify this, we are going to merge our feature_test branch back into develop.\nTo do this, first you must make sure you have the most recent version of the develop branch. To do this, first we will use the command palette and search for git pull. This will pull the most recent changes from the remote repository and update your local develop branch. Next, we want to make sure that any changes in develop are merged with what we just did to our feature_test branch. If there were no changes, this is not strictly necessary, but it’s a good precation to take to avoid future merge conflicts. To do this, right click on the develop tag and select merge into current branch. A popup will ask you to confirm this, which you do want to (with the default options).\n\nNow that we know for sure we have the most up-to-date develop branch details, we can merge our new feature into the develop branch. However, we will protect the develop branch so that only features that pass unit tests can be merged in. Thus, you will make a pull request, as described above, to get me to merge your work in the develop branch. For completeness, here we discuss how one would to that. To do this, first right-click on the develop tag and select Checkout branch. Our git tree will now show we have develop checked out:\n\nWith develop checked out, now right click on feature_test and select merge into current branch. Select confirm in the popup box. Click Sync Changes in the Version Control Tab.\n\nthe feature_test and develop branches are now identical. You could now delete feature_test and nothing would be lost. Now, the develop branch is ready for a user to use and/or make new branches off of. # End",
    "crumbs": [
      "Methods",
      "How to Contribute"
    ]
  },
  {
    "objectID": "earth_economy_devstack/how_to_contribute.html#adding-your-own-repository",
    "href": "earth_economy_devstack/how_to_contribute.html#adding-your-own-repository",
    "title": "How to contribute",
    "section": "",
    "text": "Adding your own code to the Earth Economy Devstack is Easy! After you have cloned",
    "crumbs": [
      "Methods",
      "How to Contribute"
    ]
  },
  {
    "objectID": "earth_economy_devstack/how_to_contribute.html#version-management-with-git-github-repository",
    "href": "earth_economy_devstack/how_to_contribute.html#version-management-with-git-github-repository",
    "title": "How to contribute",
    "section": "",
    "text": "The Earth Economy Devstack and all of its published article code is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system git (https://git-scm.com/). git allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system git can vary and there generally is no ‘right’ or ‘wrong’ in collaborative software development.\nThere are two different ways to interact with a github repository: 1. making a fork; 2; working on a branch. This section describes both options, however internal members of TEEMs will usually use branches.",
    "crumbs": [
      "Methods",
      "How to Contribute"
    ]
  },
  {
    "objectID": "earth_economy_devstack/how_to_contribute.html#section",
    "href": "earth_economy_devstack/how_to_contribute.html#section",
    "title": "How to contribute",
    "section": "",
    "text": "However, some basic preferred workflows are outlined in the following.\n\n\n\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for - code development or fixes - submitting a pull request to the original SEALS repository.\n\n\n\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. To clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;.\n\n\n\n\n\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.\n\n\n\nWheter you are working on a Fork or on the seals repository itself, we will organize contributions with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\n\nSuppose you want to make a change to the hazelbean_dev repository. We will use this example to explain how we use Git Graph to make this contribution. First, let’s take a look at the Git Graph interface.\n\nFirst note the Repo dropdown box indicating we are looking at the hazelbean_dev repo. Here you could switch to other Repos that have been added to your workspace. The next thing to note are the three different branch tages (Blue, Magenta and Green boxes on the Git Graph tree view). These are the three branches that are currently in the hazelbean_dev repository. The blue branch is the main branch, the magenta branch is the develop branch and the green branch is a branch specific to the user and/or the feature. In this case, it is develop_justin branch. The lines and dots indicates the history of how files were edited and then how the different branches were merged back together after an edit. By default, when you clone a repository, you will be on the main branch. To switch to another branch, right-click the tag and select checkout branch. When you do this, the files on your harddrive will be changed by Git to match the status of the branch you just checked out. In the image below, we can see which is the current branch because it has an open-circle dot in the graph and the name is bold. Other things to notice is that after the name of the branch there is the word origin. This indicates that the branch is synced with the remote repository (named origin by convention).\nTo make a change, you will first want to create your own branch. First, make absolutely certain you currently have checked out the develop branch (unless you know why to do otherwise). We will use main branch only to contain working “releases”. Once develop is checked out, use the Command Pallate (ctrl-shift-p) and search for create branch.\n\nChoose the right repository among those loaded in your workspace, hazelbean_dev in this example.\n\nThen select it and give the new branch a name, either develop_&lt;your_name&gt; or feature_&lt;your_feature_name&gt;. Below we have created a new feature branch called feature_test branched off of the develop branch. It should look like this\n\nYou’ll notice that the tag is bold, indicating you have checked it out and it is your current branch. Also notice though that it does not have the tag origin after it, indicating that it is not synced with the remote repository. To sync it, you will need to push it to the remote repository. To do this, right-click the tag and select push branch. This will push the branch to the remote repository and it will be available to other users.\nAnother way to push the branch is using the Version Control tab in VS Code. Click the Version Control tab on the Lefthand bar. There you will see all of the repos you have loaded into our workspace. For hazelbean_dev, you will see it has a publish branch button. Clicking this will have the same effect as the push branch command in Git Graph.\n\nRegardless of how you pushed it to the remote repository, you will now see the branch has the ‘origin’ tag after it, indicating it is synced with the remote repository. It is now also the checked-out branch and so all changes you make will be made to this branch.\n\nNow, we’re going to make a small change to the code. In the VS Code file explorer, I will open up file_io.py in the hazelbean module and scroll to the function I want to edit.\n\nOn lines 778-779, you’ll see VS Code has greyed the variables out indicating they are not used, so I will remove them. Once gone, you’ll see a blue bar on the left side of the editor. This is Git and VS Code indicating to you you made a change. Blue indicates a modification where as green indicates a new addition and red indicates a deletion.\n\nTo illustrate a new addition, I will add a line at to space out our for loop. Once I do this, you’ll see the green bar on the left side of the editor. In the image you can also see that the file_io.py is now a different color and has an M next to it. This indicates that the file has been modified.\n\nAnother useful thing you can do is click on the Blue or Green edit bar to see more details, as below.\n\nHere, you can see the exact changes you made. Additionally, there are buttons at the top of this new box that let you revert the change to go back to how it was before.\nBefore we commit our changes, look at the Git Graph view. You’ll see that we now have a new grey line coming from the feature_test branch, showing that we have uncommitted changes. You could click on the uncommitted changes link to see the edits we made.\n\nWe are now going to commit our changes. To do this, go to the Version Control Tab. You will now see that there is a change listed under the hazelbean_dev repository. Click on the change to see the details. You will see a more detailed “diff editor” that lets you understand (or change) what was edited. To accept these changes, we will click the commit button. But first write a short commit message. Click Commit (but don’t yet click the next button to Sync changes). After committing, look back at the Git Graph view. You’ll see that the grey line is now gone and the blue feature_test tag is at the top of our commit tree along with our comitt message.\n\nNotice though that the tag for origin/feature_test is not yet up at the new location. This is because we have not yet pushed our changes to the remote repository. To do this, click the Sync button in the bottom right of the VS Code window. This will push your changes to the remote repository and update the tag to the new location, like this.\n\nYour code is now on GitHub and other contributors with access could check it out and try it. But, this code will be different than their code and if you made non-trivial changes, it could be hard to keep straight what is going on. To clarify this, we are going to merge our feature_test branch back into develop.\nTo do this, first you must make sure you have the most recent version of the develop branch. To do this, first we will use the command palette and search for git pull. This will pull the most recent changes from the remote repository and update your local develop branch. Next, we want to make sure that any changes in develop are merged with what we just did to our feature_test branch. If there were no changes, this is not strictly necessary, but it’s a good precation to take to avoid future merge conflicts. To do this, right click on the develop tag and select merge into current branch. A popup will ask you to confirm this, which you do want to (with the default options).\n\nNow that we know for sure we have the most up-to-date develop branch details, we can merge our new feature into the develop branch. However, we will protect the develop branch so that only features that pass unit tests can be merged in. Thus, you will make a pull request, as described above, to get me to merge your work in the develop branch. For completeness, here we discuss how one would to that. To do this, first right-click on the develop tag and select Checkout branch. Our git tree will now show we have develop checked out:\n\nWith develop checked out, now right click on feature_test and select merge into current branch. Select confirm in the popup box. Click Sync Changes in the Version Control Tab.\n\nthe feature_test and develop branches are now identical. You could now delete feature_test and nothing would be lost. Now, the develop branch is ready for a user to use and/or make new branches off of. # End",
    "crumbs": [
      "Methods",
      "How to Contribute"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html",
    "href": "earth_economy_devstack/installation.html",
    "title": "Installation",
    "section": "",
    "text": "The Earth Economy Devstack is a specific way of organizing files, VS Code workspaces, local development repositories and preconfigured launch configurations that let a user contribute to NatCap TEEMs projects. This document contains:\n\nA quickstark installation guide for installing the Earth Economy Devstack which assumes a basic understanding of the command line and Python.\nA detailed step-by-step installation guide of Python, Git, QGIS and other related software.\n\nBoth sections will discuss putting files in specific locations relative to your user directory. If followed exactly, this will allow the multiple repositories to work together as intended. Other file organization schemes are possible, but will require manual configuration of the launch configurations and other settings.\n\n\n\n\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option). - Install in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac) - During installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable” - If you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm sympy gekko\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\n\n\n\nInstall required extensions\n\nInstall the Python extension in VS Code\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\nInstall Quarto extension in VS Code\nInstall GitGraph extension in VS Code\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\n\n\n\n\n\n\n\n\nGet the git software\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\nGit vs. GitHub\n\nGithub is a website that hosts code and connects a community of coders.\nGit is a “version control” software tool that records the history of files in a Git repository.\nNearly every coder uses Git to push their repository of code to GitHub.\n\nUse the default options for everything\n\nUnless you REALLY know what you’re doing.\n\n\n\n\n\nOpen up the command prompt and type git to test that it’s installed\n\n(PC) Search for cmd in the start menu\n\nThis is the OG way of working on computers\n\nAll version control tasks can be done via git commands here, but we will be using VS Code instead\n\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\n\n\n\nYou’ll know it worked if you see (base) in front of your path\n\nBase is the default “environment” that you will use.\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n- When you have lots of projects, most people create multiple environments specific to each project. \n- For now, we’re going to install everything to the base environment\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n- You’ll know it worked if you see (env_name) in front of your path\n- You can deactivate the environment with `conda deactivate`\n- ![](images/2024-02-02-10-39-21.png)\n- You can list all environments with `conda env list`\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm sympy gekko\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\nThis step may take a long time because you are downloading and installing hundreds of libraries\n\n\n\nWhen you’re done, it should look like the image here.\n\n\n\nSuccess! You now have a modern scientific computing environment (sometimes called a scientific computing stack) on your computer!\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\n\nBy default, your terminal will open in your user directory, so C:\\Users\\&lt;YOUR_USERNAME&gt;\nYou can navigate to the directory you created with the command cd Files\n\nSee image below.\n\n\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\n\n\n\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\nYou could just open VS Code now, but we’re going to open it up with a specific Worspace Configuration file below\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\n\n\n\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\nWe’ll describe debugging more in future sections\n\nFor now, just know that these launch configurations make sure you’re using the repositories that we’ve added to your workspace\n\n\nAlso note that the other repositories shown in the VS Code file explorer will be empty until you git clone them (described below)\n\n\n\n\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\nInstall required extensions\n\nInstall the Python extension\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\n\nInstall Quarto extension in VS Code\n\nWe use Quarto to create reports and documents on .qmd, .md and .ipynb files\n\nYou can edit in source mode:\n\n\n\nOr you can press ctrl+shift+f4 to use the visual editor\n\n\n\n\n\nInstall Git Graph extension in VS Code\n\nOnce installed, click the Git Graph button on the bottom-left status bar to see a visual representation of your git history\n\n\n\n\n\n\n\n\nThe Earth Economy Devstack organizes repositories in a specific way, described here.\n\nRecall that our workspace links to e.g. the hazelbean_dev repository, but it currently points to a directory that doesn’t yet exist.\n\nIn our Files directory, create a directory named hazelbean (not hazelbean_dev as that will be the repository’s name)\n\n\n\nBelow, we will use git to clone the hazelbean_dev into this directory we just created\n\n\n\n\n\n\n\n\n\n\nInstead of using the command line, we will use Git via VS Code’s “Command Pallate”\n\nThe Command Pallate is accessed via \nIt is a search bar that you can use to run commands in VS Code\n\nOnce you’ve opened the Command Pallate, type “git clone” and it will search for the command\n\n\n\nOnce you select the command, it will prompt you if you want to write in your Repo’s GitHub URL manually, or you can use VS Code to search the different repositories you have access to\n\n\n\nSearching via GitHub found, for instance, NatCapTEEMs/gep repo\n\n\n\nOnce you select that, it will ask you what local directory you want to clone the repository to.\n\nBy default it assumes you want to clone it to your user directory as below\n\n\n\nWe instead want to clone it to the hazelbean directory we created earlier, which will put a new folder hazelbean_dev in that directory\nTo do this, navigate to the hazelbean directory and select it\n\n\n\n\nAfter you’ve cloned a repository, you can now access it in the file explorer\n\n\n\n\n\n\n\n\nIf you are a member of NatCap TEEMs or the JohnsonPolaskyLab, you should have access to these repositories via our GitHub organization\nIf you’re not a member, you will still be able to clone all of the public repositories (which are all documented in various journal articles)",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html#quickstart-installation",
    "href": "earth_economy_devstack/installation.html#quickstart-installation",
    "title": "Installation",
    "section": "",
    "text": "Install and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option). - Install in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac) - During installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable” - If you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm sympy gekko\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\n\n\n\nInstall required extensions\n\nInstall the Python extension in VS Code\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\nInstall Quarto extension in VS Code\nInstall GitGraph extension in VS Code\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html#step-by-step-installation",
    "href": "earth_economy_devstack/installation.html#step-by-step-installation",
    "title": "Installation",
    "section": "",
    "text": "Get the git software\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\nGit vs. GitHub\n\nGithub is a website that hosts code and connects a community of coders.\nGit is a “version control” software tool that records the history of files in a Git repository.\nNearly every coder uses Git to push their repository of code to GitHub.\n\nUse the default options for everything\n\nUnless you REALLY know what you’re doing.\n\n\n\n\n\nOpen up the command prompt and type git to test that it’s installed\n\n(PC) Search for cmd in the start menu\n\nThis is the OG way of working on computers\n\nAll version control tasks can be done via git commands here, but we will be using VS Code instead\n\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\n\n\n\nYou’ll know it worked if you see (base) in front of your path\n\nBase is the default “environment” that you will use.\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n- When you have lots of projects, most people create multiple environments specific to each project. \n- For now, we’re going to install everything to the base environment\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n- You’ll know it worked if you see (env_name) in front of your path\n- You can deactivate the environment with `conda deactivate`\n- ![](images/2024-02-02-10-39-21.png)\n- You can list all environments with `conda env list`\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm sympy gekko\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\nThis step may take a long time because you are downloading and installing hundreds of libraries\n\n\n\nWhen you’re done, it should look like the image here.\n\n\n\nSuccess! You now have a modern scientific computing environment (sometimes called a scientific computing stack) on your computer!\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\n\nBy default, your terminal will open in your user directory, so C:\\Users\\&lt;YOUR_USERNAME&gt;\nYou can navigate to the directory you created with the command cd Files\n\nSee image below.\n\n\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\n\n\n\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\nYou could just open VS Code now, but we’re going to open it up with a specific Worspace Configuration file below\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\n\n\n\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\nWe’ll describe debugging more in future sections\n\nFor now, just know that these launch configurations make sure you’re using the repositories that we’ve added to your workspace\n\n\nAlso note that the other repositories shown in the VS Code file explorer will be empty until you git clone them (described below)\n\n\n\n\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\nInstall required extensions\n\nInstall the Python extension\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\n\nInstall Quarto extension in VS Code\n\nWe use Quarto to create reports and documents on .qmd, .md and .ipynb files\n\nYou can edit in source mode:\n\n\n\nOr you can press ctrl+shift+f4 to use the visual editor\n\n\n\n\n\nInstall Git Graph extension in VS Code\n\nOnce installed, click the Git Graph button on the bottom-left status bar to see a visual representation of your git history\n\n\n\n\n\n\n\n\nThe Earth Economy Devstack organizes repositories in a specific way, described here.\n\nRecall that our workspace links to e.g. the hazelbean_dev repository, but it currently points to a directory that doesn’t yet exist.\n\nIn our Files directory, create a directory named hazelbean (not hazelbean_dev as that will be the repository’s name)\n\n\n\nBelow, we will use git to clone the hazelbean_dev into this directory we just created\n\n\n\n\n\n\n\n\n\n\nInstead of using the command line, we will use Git via VS Code’s “Command Pallate”\n\nThe Command Pallate is accessed via \nIt is a search bar that you can use to run commands in VS Code\n\nOnce you’ve opened the Command Pallate, type “git clone” and it will search for the command\n\n\n\nOnce you select the command, it will prompt you if you want to write in your Repo’s GitHub URL manually, or you can use VS Code to search the different repositories you have access to\n\n\n\nSearching via GitHub found, for instance, NatCapTEEMs/gep repo\n\n\n\nOnce you select that, it will ask you what local directory you want to clone the repository to.\n\nBy default it assumes you want to clone it to your user directory as below\n\n\n\nWe instead want to clone it to the hazelbean directory we created earlier, which will put a new folder hazelbean_dev in that directory\nTo do this, navigate to the hazelbean directory and select it\n\n\n\n\nAfter you’ve cloned a repository, you can now access it in the file explorer\n\n\n\n\n\n\n\n\nIf you are a member of NatCap TEEMs or the JohnsonPolaskyLab, you should have access to these repositories via our GitHub organization\nIf you’re not a member, you will still be able to clone all of the public repositories (which are all documented in various journal articles)",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/methods.html",
    "href": "earth_economy_devstack/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nThis section describes the methods for contributing to and using the overall devstack.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html",
    "href": "earth_economy_devstack/project_complexity.html",
    "title": "The stages of project complexity",
    "section": "",
    "text": "As an introduction and/or motivation for using the software in the EE Devstack, I would like to talk through the process that I went through as a PhD student, postdoc and staff researcher to answer progressively harder questions. I break these out into 6 stages below. Solving these challenges is what led to the creation of Hazelbean and many other software solutions.\n\n\nHere is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))\n\n\n\nThis is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad.\n\n\n\nThe coding approach in level 2 becomes intractable when there are lots of layers to consider. It’s also a pain to have to repeat code to do some common tasks, like loading the raster to a dataset and then to an array. This complexity level starts to apply good coding practices, such as defining helper functions like raster_to_array() below. Code is also made much shorter and more elegant by using loops. It minimizes the number of code statements, reduces bugs and scales better to long lists of input files.\nimport os\nimport numpy as np\nimport gdal\n\n# NOTE 1: Helper function defined\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# NOTE 2: Inputs put into an iterable\ninput_paths = [\n    'yield_per_cell_1.tif',\n    'yield_per_cell_2.tif',\n    'yield_per_cell_3.tif',\n    'yield_per_cell_4.tif',\n]\n\n# NOTE 3: Calculation happens in loops, recording results to an output object\nsummations = []\nfor raster_path in input_paths:\n    array = raster_to_array(raster_path)\n    summations.append(np.sum(array))\n\nprint('Sums of layers: ' + str(summations))\n\n\n\nBelow is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Unlike the other levels, do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment! Instead, I provide a better example of code below that does things better, in the Earth-Economy Devstack way,\n\n\nimport logging\nimport os\nimport csv\nimport math, time, random\nfrom osgeo import gdal, gdalconst\nimport numpy as np\n\n# NOTE 1: I started to pull in Cython (Python code compiled to C for speed) because my code was getting slow\nimport pyximport\npyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n\n# NOTE 2: I wrote my own Python Library (geoecon_utils), which went through several more \n# iterations (Numdal, Lol!), until it got finalized as hazelbean\nimport geoecon_utils.geoecon_utils as gu\nimport geoecon_utils.geoecon_cython_utils as gcu\n\n# NOTE 3: Logging becomes important to manage information input-output used by the developer\nlog_id = gu.pretty_time()\nLOGGER = logging.getLogger('ag_tradeoffs')\nLOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\nfile_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\nLOGGER.addHandler(file_handler)\n\n# NOTE 4: Defining inputs and outputs is now based on a workspace, from which everything\n# else is defined with relative paths. Scales better with inputs and to other users.\nworkspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\nc_1km_file = 'c_1km.tif'\nc_1km_uri = workspace + c_1km_file\nha_per_cell_5m_file = 'ha_per_cell_5m.tif'\nha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n\n# NOTE 5: Here's an example of using custom libraries to \nha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n# NOTE 6: Here we start to deal with conditional running of code that skips outputs if they have already been\n# created. This is often the first (and often most eficatious) optimization of code to run fast.\ndo_30s_resample = False\nif do_30s_resample:\n    # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n    desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n    c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n    gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n\n# NOTE 7: An here, we see an incredibly slow approach that seems intuitive but is wrong\n# because it is 1000x slower than correct vectorized calculations (which are provided by numpy)\narray = raster_to_array(c_30s_unscaled_uri)\nfor row in range(array.shape[0]):\n    for col in range(array.shape[1]):\n        if array[row, col] &lt; 0:\n            array[row, col] = -9999 # Set negative values to the no-data-value\n\n\n\n# For reference, the correct way would have been as below. We will introduce hazelbean utils (like hb.as_array() soon.\narray = hb.as_array(input_path)\narray = np.where(array &lt; 0, -9999, array)\n\n\n\n\nDepending on the size of the array, the numpy where command used above would fail with a MemoryError or something similar. Below you will the first way that I dealt with this (BAD CODE) and then the correct way. In either case, it is almost always true that the most likely solution to larger-than-memory situations is to apply your algorithm in chunks. We’ll do this in both cases.\n\n\nIn this code, I created a thing I made up, a “Tile Reference” which I implemented with geoecon_utils.Tr(). This returned a set of tiles, defined by their row, column, x_width and y_width. We used these to load just subsets of the array and do operations on the smaller thing.\n\nliteral_aggregation_to_5m_cell = False\nif literal_aggregation_to_5m_cell:\n    factor =  10\n    shape = (2160, 4320)\n    c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n    cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n    cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n    for tile in c_30s_tr.tr_frame:\n        tile_array = c_30s_tr.tile_to_array(tile)\n        print 'Aggregating tile', tile\n        for row in range(c_30s_tr.chunkshape[0] / factor):\n            for col in range(c_30s_tr.chunkshape[1] / factor):\n                cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n                cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n    cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n    print gu.desc(cell_sum_5m)\n    gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n\n\n\nThe better way is to use a function that builds in the tiling functionality. Eventually this will expand to multi-computer approaches, but for now, we’ll just use the local but parallelized hb.raster_calculator()\nhb.raster_calculator(this)\n\n\n\n\nThe final level of complexity we will discuss (before just using the Earth Economy Devstack approach) arises when the number of files that must be managed becomes a challenge both for performance reasons and the challenges of managing complexity.\nOne example where this comes up is when a computation requires writing tiles of output. In many big-data applications and in most of the very large datasets that are available online, the data are themselves stored in tiles. On the one hand, this is nice because it automatically suggests a chunk-by-chunk parallelization strategy. On the other hand, it quickly becomes challenging when, for instance, you want to look at an area of interest (AOI) that spans multiple tiles. There are a plethora of software solutions to deal with this, such as GDAL’s Virtual Raster (VRT) file type, but many of these have limitations.\nWhen the computation in question requires many complex steps which might be contingent on other intermediate products from adjacent tiles, even some of the most cutting-edge solutions that implement complex tiling architecture (like DASK with rioxarray) will not be sufficient. This was the challenge that arose when doing downscaling with the SEALS model, especially when the algorithm had to be trained on such tiles millions of times. The optimized algorithm in this complex data and computation dependency-tree situation required a new tool, which would also have to address all of the above challenges in project complexity.\nThis led to ProjectFlow, one of the key tools within the Earth Economy Devstack and a part of Hazelbean.",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-1-simple-question-answered-well",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-1-simple-question-answered-well",
    "title": "The stages of project complexity",
    "section": "",
    "text": "Here is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "This is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad.",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-3-starting-to-deal-with-generalization-reusing-code-and-shortening-scripts.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-3-starting-to-deal-with-generalization-reusing-code-and-shortening-scripts.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "The coding approach in level 2 becomes intractable when there are lots of layers to consider. It’s also a pain to have to repeat code to do some common tasks, like loading the raster to a dataset and then to an array. This complexity level starts to apply good coding practices, such as defining helper functions like raster_to_array() below. Code is also made much shorter and more elegant by using loops. It minimizes the number of code statements, reduces bugs and scales better to long lists of input files.\nimport os\nimport numpy as np\nimport gdal\n\n# NOTE 1: Helper function defined\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# NOTE 2: Inputs put into an iterable\ninput_paths = [\n    'yield_per_cell_1.tif',\n    'yield_per_cell_2.tif',\n    'yield_per_cell_3.tif',\n    'yield_per_cell_4.tif',\n]\n\n# NOTE 3: Calculation happens in loops, recording results to an output object\nsummations = []\nfor raster_path in input_paths:\n    array = raster_to_array(raster_path)\n    summations.append(np.sum(array))\n\nprint('Sums of layers: ' + str(summations))",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-4-starting-to-deal-with-performance-and-generalization.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-4-starting-to-deal-with-performance-and-generalization.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "Below is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Unlike the other levels, do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment! Instead, I provide a better example of code below that does things better, in the Earth-Economy Devstack way,\n\n\nimport logging\nimport os\nimport csv\nimport math, time, random\nfrom osgeo import gdal, gdalconst\nimport numpy as np\n\n# NOTE 1: I started to pull in Cython (Python code compiled to C for speed) because my code was getting slow\nimport pyximport\npyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n\n# NOTE 2: I wrote my own Python Library (geoecon_utils), which went through several more \n# iterations (Numdal, Lol!), until it got finalized as hazelbean\nimport geoecon_utils.geoecon_utils as gu\nimport geoecon_utils.geoecon_cython_utils as gcu\n\n# NOTE 3: Logging becomes important to manage information input-output used by the developer\nlog_id = gu.pretty_time()\nLOGGER = logging.getLogger('ag_tradeoffs')\nLOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\nfile_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\nLOGGER.addHandler(file_handler)\n\n# NOTE 4: Defining inputs and outputs is now based on a workspace, from which everything\n# else is defined with relative paths. Scales better with inputs and to other users.\nworkspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\nc_1km_file = 'c_1km.tif'\nc_1km_uri = workspace + c_1km_file\nha_per_cell_5m_file = 'ha_per_cell_5m.tif'\nha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n\n# NOTE 5: Here's an example of using custom libraries to \nha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n# NOTE 6: Here we start to deal with conditional running of code that skips outputs if they have already been\n# created. This is often the first (and often most eficatious) optimization of code to run fast.\ndo_30s_resample = False\nif do_30s_resample:\n    # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n    desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n    c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n    gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n\n# NOTE 7: An here, we see an incredibly slow approach that seems intuitive but is wrong\n# because it is 1000x slower than correct vectorized calculations (which are provided by numpy)\narray = raster_to_array(c_30s_unscaled_uri)\nfor row in range(array.shape[0]):\n    for col in range(array.shape[1]):\n        if array[row, col] &lt; 0:\n            array[row, col] = -9999 # Set negative values to the no-data-value\n\n\n\n# For reference, the correct way would have been as below. We will introduce hazelbean utils (like hb.as_array() soon.\narray = hb.as_array(input_path)\narray = np.where(array &lt; 0, -9999, array)",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-5-dealing-with-larger-than-memory-data",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-5-dealing-with-larger-than-memory-data",
    "title": "The stages of project complexity",
    "section": "",
    "text": "Depending on the size of the array, the numpy where command used above would fail with a MemoryError or something similar. Below you will the first way that I dealt with this (BAD CODE) and then the correct way. In either case, it is almost always true that the most likely solution to larger-than-memory situations is to apply your algorithm in chunks. We’ll do this in both cases.\n\n\nIn this code, I created a thing I made up, a “Tile Reference” which I implemented with geoecon_utils.Tr(). This returned a set of tiles, defined by their row, column, x_width and y_width. We used these to load just subsets of the array and do operations on the smaller thing.\n\nliteral_aggregation_to_5m_cell = False\nif literal_aggregation_to_5m_cell:\n    factor =  10\n    shape = (2160, 4320)\n    c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n    cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n    cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n    for tile in c_30s_tr.tr_frame:\n        tile_array = c_30s_tr.tile_to_array(tile)\n        print 'Aggregating tile', tile\n        for row in range(c_30s_tr.chunkshape[0] / factor):\n            for col in range(c_30s_tr.chunkshape[1] / factor):\n                cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n                cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n    cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n    print gu.desc(cell_sum_5m)\n    gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n\n\n\nThe better way is to use a function that builds in the tiling functionality. Eventually this will expand to multi-computer approaches, but for now, we’ll just use the local but parallelized hb.raster_calculator()\nhb.raster_calculator(this)",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-6-systematic-file-management-file-management.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-6-systematic-file-management-file-management.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "The final level of complexity we will discuss (before just using the Earth Economy Devstack approach) arises when the number of files that must be managed becomes a challenge both for performance reasons and the challenges of managing complexity.\nOne example where this comes up is when a computation requires writing tiles of output. In many big-data applications and in most of the very large datasets that are available online, the data are themselves stored in tiles. On the one hand, this is nice because it automatically suggests a chunk-by-chunk parallelization strategy. On the other hand, it quickly becomes challenging when, for instance, you want to look at an area of interest (AOI) that spans multiple tiles. There are a plethora of software solutions to deal with this, such as GDAL’s Virtual Raster (VRT) file type, but many of these have limitations.\nWhen the computation in question requires many complex steps which might be contingent on other intermediate products from adjacent tiles, even some of the most cutting-edge solutions that implement complex tiling architecture (like DASK with rioxarray) will not be sufficient. This was the challenge that arose when doing downscaling with the SEALS model, especially when the algorithm had to be trained on such tiles millions of times. The optimized algorithm in this complex data and computation dependency-tree situation required a new tool, which would also have to address all of the above challenges in project complexity.\nThis led to ProjectFlow, one of the key tools within the Earth Economy Devstack and a part of Hazelbean.",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/release_notes.html",
    "href": "earth_economy_devstack/release_notes.html",
    "title": "Release Notes",
    "section": "",
    "text": "Downloading of base data now works.\n\n\n\nNow all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.\nIf no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file.",
    "crumbs": [
      "SEALS",
      "Release Notes"
    ]
  },
  {
    "objectID": "earth_economy_devstack/release_notes.html#update-v0.5.0",
    "href": "earth_economy_devstack/release_notes.html#update-v0.5.0",
    "title": "Release Notes",
    "section": "",
    "text": "Downloading of base data now works.",
    "crumbs": [
      "SEALS",
      "Release Notes"
    ]
  },
  {
    "objectID": "earth_economy_devstack/release_notes.html#update-v0.4.0",
    "href": "earth_economy_devstack/release_notes.html#update-v0.4.0",
    "title": "Release Notes",
    "section": "",
    "text": "Now all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.\nIf no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file.",
    "crumbs": [
      "SEALS",
      "Release Notes"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_overview.html",
    "href": "earth_economy_devstack/seals_overview.html",
    "title": "SEALS",
    "section": "",
    "text": "SEALS, the Spatial Economic Allocation Landscape Simulator, is a land-use change model that downscales predictions of land-use change from aggregate (regional or coarse-gridded) inputs to a finer resolution (typically 10-300 meters). The primary comparative advantage of SEALS is fast computation. It is able to downscale to 300 meters globally (~8.4 billion grid-cells) in about an hour on a laptop. It is designed with parallelization in mind, and so scales well to high-performance, cloud or distributed computing. It is written in C++ for all performance-critical functions and uses Python for everything else. SEALS is a part of the Earth Economy Devstack, a software platform supported by NatCap TEEMs (The Earth-Economy Modellers).\n\n\nFull instructions for installing SEALS can be found in the Installation Steps. SEALS is under active development and is changing rapidly. We have submitted this code for publication but are waiting on reviews.",
    "crumbs": [
      "SEALS"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_overview.html#installation",
    "href": "earth_economy_devstack/seals_overview.html#installation",
    "title": "SEALS",
    "section": "",
    "text": "Full instructions for installing SEALS can be found in the Installation Steps. SEALS is under active development and is changing rapidly. We have submitted this code for publication but are waiting on reviews.",
    "crumbs": [
      "SEALS"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html",
    "href": "earth_economy_devstack/seals_walkthrough.html",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "Make sure you have followed all of the steps in the installation page.\n\nIn particular, Clone the SEALS and Hazelbean repositories in the correct location, as described here\nYou will know you’ve got them installed correctly if your VS Code Explorer tab shows the repositories without an error message (Figure 1)\n\n\n\n\n\n\n\nIn the VS Code Explorer tab, navigate to your seals_dev directory (Figure 1)\n\nQuick note about file organization\n\nThe root directory of seals_dev contains more than just the seals library, such as directories for scripts, images, etc.\nThe library itself is in the seals subdirectory seals_dev/seals which may seem redundant but is necessary for the way Python imports work.\nIf you inspect the seals directory, you will see an __init__.py file. This make Python able to import the directory as a package.\n\nYou will also see a seals_main.py file. This is where most of the actual logic of seals is.\n\n\n\n\n\n\n\nOne does not simply run a main.py (Figure 1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (Figure 2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)\n\n\n\n\n\n\n\nThe run file begins with standard python imports\nThen in the if __name__ == '__main__': block, we define the project directory and initialize the project flow object\n\nThe reason for putting it in this block is so that you don’t accidentally run the code when you import the file in another script\n\n\nimport os, sys\nimport seals_utils\nimport seals_initialize_project\nimport hazelbean as hb\nimport pandas as pd\nfrom seals_utils import download_google_cloud_blob\n\nmain = ''\nif __name__ == '__main__':\n    content = \"here\"\n\n\n\n\nSEALS (and the EE Devstack) assumes (or softly requires) that you put all code and data somewhere relative to the user’s home directory os.path.expanduser('~')\n\nCan put it in suddirectories with extra_dirs = ['Files', 'seals', 'projects']\n\nIf you followed the EE method, you will have already created the seals directory at &lt;user_dir&gt;/Files/seals\n\nIn the seals directory, your code is in seals_dev\nIn the seals directory, you also will have a projects direcotry\n\nThis is created automatically if its not there\nAll data and outputs will be saved in this directory\n\nAs a best practice, you should not save data in the seals_dev directory\n\n\n\n\n    ### ------- ENVIRONMENT SETTINGS -------------------------------\n    # Users should only need to edit lines in this ENVIRONMENT SETTINGS section\n\n    # A ProjectFlow object is created from the Hazelbean library to organize directories and enable parallel processing.\n    # project-level variables are assigned as attributes to the p object (such as in p.base_data_dir = ... below)\n    # The only agrument for a project flow object is where the project directory is relative to the current_working_directory.\n    # This organization, defined with extra dirs relative to the user_dir is the EE-spec.\n    user_dir = os.path.expanduser('~')        \n    extra_dirs = ['Files', 'seals', 'projects']\n\n\n\n\nSet the project name\n\nThis is where your new files will be written\n\nThe next line defines the project dir with the above information\n\n    # The project_name is used to name the project directory below. Also note that\n    # ProjectFlow only calculates tasks that haven't been done yet, so adding \n    # a new project_name will give a fresh directory and ensure all parts\n    # are run.\n    project_name = 'test_standard'\n\n    # The project-dir is where everything will be stored, in particular in an input, intermediate, or output dir\n    # IMPORTANT NOTE: This should not be in a cloud-synced directory (e.g. dropbox, google drive, etc.), which\n    # will either make the run fail or cause it to be very slow. The recommended place is (as coded above)\n    # somewhere in the users's home directory.\n    project_dir = os.path.join(user_dir, os.sep.join(extra_dirs), project_name)\n\n\n\n\nCreate a ProjectFlow object with the project_dir as the only argument\n\nPython is an object-oriented programming langage\n\nThe hb.ProjectFlow() defines a class, which is like a recipe for an object\nWhen we call it, it generates on object of that class, which we assign to the variable p\n\n\n\n    # Create the ProjectFlow Object\n    p = hb.ProjectFlow(project_dir)\n\n\n\n\nThe p object we created will organize input variables (called attributes when assigned to an object)\n\nLike this: p.attribute_name = 'ItsName\n\nThe p object also has functions tied specificially to it (called methods when assigned to an object)\n\nSuch as: p.validate_name()\nMethods operate on the object that defined it\n\nSo validate_name() is specifically looking at the p object, often doing something to the p object, like fixing value of p.attribute_name",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#getting-set-up",
    "href": "earth_economy_devstack/seals_walkthrough.html#getting-set-up",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "Make sure you have followed all of the steps in the installation page.\n\nIn particular, Clone the SEALS and Hazelbean repositories in the correct location, as described here\nYou will know you’ve got them installed correctly if your VS Code Explorer tab shows the repositories without an error message (Figure 1)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#explore-the-seals-code",
    "href": "earth_economy_devstack/seals_walkthrough.html#explore-the-seals-code",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "In the VS Code Explorer tab, navigate to your seals_dev directory (Figure 1)\n\nQuick note about file organization\n\nThe root directory of seals_dev contains more than just the seals library, such as directories for scripts, images, etc.\nThe library itself is in the seals subdirectory seals_dev/seals which may seem redundant but is necessary for the way Python imports work.\nIf you inspect the seals directory, you will see an __init__.py file. This make Python able to import the directory as a package.\n\nYou will also see a seals_main.py file. This is where most of the actual logic of seals is.",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#run-files",
    "href": "earth_economy_devstack/seals_walkthrough.html#run-files",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "One does not simply run a main.py (Figure 1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (Figure 2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#setting-up-the-run-file",
    "href": "earth_economy_devstack/seals_walkthrough.html#setting-up-the-run-file",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "The run file begins with standard python imports\nThen in the if __name__ == '__main__': block, we define the project directory and initialize the project flow object\n\nThe reason for putting it in this block is so that you don’t accidentally run the code when you import the file in another script\n\n\nimport os, sys\nimport seals_utils\nimport seals_initialize_project\nimport hazelbean as hb\nimport pandas as pd\nfrom seals_utils import download_google_cloud_blob\n\nmain = ''\nif __name__ == '__main__':\n    content = \"here\"",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#project-directory-structure",
    "href": "earth_economy_devstack/seals_walkthrough.html#project-directory-structure",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "SEALS (and the EE Devstack) assumes (or softly requires) that you put all code and data somewhere relative to the user’s home directory os.path.expanduser('~')\n\nCan put it in suddirectories with extra_dirs = ['Files', 'seals', 'projects']\n\nIf you followed the EE method, you will have already created the seals directory at &lt;user_dir&gt;/Files/seals\n\nIn the seals directory, your code is in seals_dev\nIn the seals directory, you also will have a projects direcotry\n\nThis is created automatically if its not there\nAll data and outputs will be saved in this directory\n\nAs a best practice, you should not save data in the seals_dev directory\n\n\n\n\n    ### ------- ENVIRONMENT SETTINGS -------------------------------\n    # Users should only need to edit lines in this ENVIRONMENT SETTINGS section\n\n    # A ProjectFlow object is created from the Hazelbean library to organize directories and enable parallel processing.\n    # project-level variables are assigned as attributes to the p object (such as in p.base_data_dir = ... below)\n    # The only agrument for a project flow object is where the project directory is relative to the current_working_directory.\n    # This organization, defined with extra dirs relative to the user_dir is the EE-spec.\n    user_dir = os.path.expanduser('~')        \n    extra_dirs = ['Files', 'seals', 'projects']",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#project-name",
    "href": "earth_economy_devstack/seals_walkthrough.html#project-name",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "Set the project name\n\nThis is where your new files will be written\n\nThe next line defines the project dir with the above information\n\n    # The project_name is used to name the project directory below. Also note that\n    # ProjectFlow only calculates tasks that haven't been done yet, so adding \n    # a new project_name will give a fresh directory and ensure all parts\n    # are run.\n    project_name = 'test_standard'\n\n    # The project-dir is where everything will be stored, in particular in an input, intermediate, or output dir\n    # IMPORTANT NOTE: This should not be in a cloud-synced directory (e.g. dropbox, google drive, etc.), which\n    # will either make the run fail or cause it to be very slow. The recommended place is (as coded above)\n    # somewhere in the users's home directory.\n    project_dir = os.path.join(user_dir, os.sep.join(extra_dirs), project_name)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#create-projectflow-object",
    "href": "earth_economy_devstack/seals_walkthrough.html#create-projectflow-object",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "Create a ProjectFlow object with the project_dir as the only argument\n\nPython is an object-oriented programming langage\n\nThe hb.ProjectFlow() defines a class, which is like a recipe for an object\nWhen we call it, it generates on object of that class, which we assign to the variable p\n\n\n\n    # Create the ProjectFlow Object\n    p = hb.ProjectFlow(project_dir)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#using-objects",
    "href": "earth_economy_devstack/seals_walkthrough.html#using-objects",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "The p object we created will organize input variables (called attributes when assigned to an object)\n\nLike this: p.attribute_name = 'ItsName\n\nThe p object also has functions tied specificially to it (called methods when assigned to an object)\n\nSuch as: p.validate_name()\nMethods operate on the object that defined it\n\nSo validate_name() is specifically looking at the p object, often doing something to the p object, like fixing value of p.attribute_name",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#inspect-default-variables",
    "href": "earth_economy_devstack/seals_walkthrough.html#inspect-default-variables",
    "title": "First SEALS run walkthrough",
    "section": "Inspect default variables",
    "text": "Inspect default variables\n\nThe default variables might not need to be modified from their default values\n\nBut we need to understand them to get what SEALS is doing\n\nBelow, we set the attribute for where the scenario_definitions csv should be stored\n\nNote that we never defined p.input_dir. It was based on the project_dir when we created p\n\nThe scenario_definitions file specifies what defines the many different scenarios you want to run\n\nEach row will be one scenario\nEach time the model runs a new scenario, it will update its attributes based on this row\n\nIf you haven’t run SEALS yet, you won’t have a scenario_defintions file, so it will create a default one on the first run\n\np.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#running-the-model",
    "href": "earth_economy_devstack/seals_walkthrough.html#running-the-model",
    "title": "First SEALS run walkthrough",
    "section": "Running the model",
    "text": "Running the model\nAfter doing the above steps, you should be ready to run run_test_seals.py. Upon starting, SEALS will report the “task tree” of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.\nOnce the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:\n [THIS IS NOT THE CORRECT IMAGE]\nOpen up the projected one (e.g., lulc_ssp2_rcp45_luh2-message_bau_2045.tif) in QGIS and enjoy your new, high-resolution land-use change projection!",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "gtap_invest/index.html",
    "href": "gtap_invest/index.html",
    "title": "GTAP-InVEST Home",
    "section": "",
    "text": "GTAP-InVEST is an Earth-Economy model that calculates how economic activity affects the environment, and vice versa, on global to local scales. GTAP-InVEST can assess:\n\nWhat “Nature Smart” policies are good for both the economy and the environment?\nWhat impacts and dependencies link the environment and the economy?\nWhat happens to the economy when ecosystems become degraded?\nHow can we produce enough food while not undermining our critical natural capital?\nWhere is land-use change (e.g., deforestation) most likely to happen and what drives this?\nHow does climate change affect natural capital and the value it provides (via ecosystem services).\n\nThe figure below illustrates how GTAP-InVEST connects economic models with environmental models.\n\nAlternatively, see a recent presentation I gave at the Chilean Central Bank’s Annual Conference below or this Youtube playlist: Earth-Economy Modeling Playlist.\n\n\nOn a more technical level, GTAP-InVEST is a global, earth-economy model that integrates a computable general equilibrium (CGE) model from the Global Trade Analysis Project (GTAP), a land-use change (LUC) model, and a ecosystem services model, InVEST (Integrated Valuation of Ecosystem Services and Tradeoffs), provided by the Natural Capital Project. It has most recently been documented in The Proceedings of the National Academy of Sciences in Johnson et al. (2023), https://www.pnas.org/doi/10.1073/pnas.2220401120. This page is the GTAP-InVEST project site, which describes the model in general terms. See also the GTAP-InVEST User Guide or the download page of results from the PNAS article. Additionally, we are hosting a live, visualization tool at https://mygeohub.org/tools/gtapinvest. \nGTAP-InVEST is open-source and can be accessed at github.com/jandrewjohnson/gtap_invest. However, this version of GTAP-InVEST relies on two proprietary elements: one must have a GTAP database license for version 10+ and one must have a GEMPACK license to access the CGE solver we used. See https://www.gtap.agecon.purdue.edu/ for access to these resources.\nGTAP-InVEST originated from several academic papers, including:\n\n“The Economic Case for Nature” (Johnson et al. 2021)\n“Projected landscape-scale repercussions of global action for climate and biodiversity protection” (von Jeetze et al. 2023, Nature Communications, https://www.nature.com/articles/s41467-023-38043-1)\n“Closing yield gap is crucial to avoid potential surge in global carbon emissions” (Suh et al. 2020, Global Environmental Change, https://doi.org/10.1016/j.gloenvcha.2020.102100)\n“Global Future: Assessing The Global Economic Impacts of Environmental Change to Support Policy-Making” (Roxburg et al. 2020, World Wildlife Fund, https://www.gtap.agecon.purdue.edu/resources/res_display.asp?RecordID=5987).\n\nThe GTAP-InVEST model is written in Python, C, R, and GEMPACK. Running the model requires a considerable amount of technical skill and involves multiple different programming languages and paradigms. Future versions of GTAP-InVEST will aim to simplify this software complexity, but for now, because this is the first “hard-linked” version of a global ecosystem service and general equilibrium model, there still exist many suboptimal design choices that were necessary in “gluing together” all of these models. Python is used as the main glue of the model while each of the preexisting models are coded in their default language. This section describes how to get the combined model software running.\nThe three building blocks of the model, documented in more detail below in the User Guide, are:\n\nThe GTAP-AEZ (Agroecological Zones) model, which calculates equilibrium prices, quantities, GDP, trade and many other economic variables, and most notably for this exercise, endogenously calculates how land is used in different sectors and how natural land is brought into economic use through agricultural, pasture and managed forest expansion.\nThe Spatial Economic Allocation Landscape Simulator (SEALS) model, which downscales regional estimates of land-use change from GTAP-AEZ to a high enough resolution (300m) to run ecosystem service models.\nInVEST, which takes the downscaled land-use, land-cover maps from SEALS to calculate changes in ecosystem service provision. These ecosystem service changes are then further expressed as shocks to the economy and used as inputs into a second run of GTAP-AEZ, which calculates how losses of ecosystem services affect economic performance.\n\n\nFor the most up-to-date documentation, please see the dynamically-updated User Guide. If you would like to analyze the raw results of the PNAS paper, please see full results from PNAS paper."
  },
  {
    "objectID": "gtap_invest/user_guide/es_impact.html",
    "href": "gtap_invest/user_guide/es_impact.html",
    "title": "6. Connecting InVEST outputs to GTAP inputs",
    "section": "",
    "text": "6. Connecting InVEST outputs to GTAP inputs\nIn the previous section, we described how we calculated the biophysical outputs of the InVEST model. These results on their own provide potentially useful results on the provision of ecosystem services. However, these biophysical changes alone are not enough to identify the impact on the economy. This section walks through the calculations for converting the raw InVEST biophysical outputs to Region-AEZ specific, factor-neutral productivity shocks.\n\n6.1. How Region-AEZ aggregated shocks are inputted into GTAP\nIn GTAP-InVEST, there are three primary ways in which changing environmental conditions enter the CGE model. These include shifting the land supply curve directly to reflect a change in production, implementing a land-augmenting or land-reducing technological changes, or shifting the production function via a factor-neutral productivity shock.\nIn this section, we describe the third type of shock, which was used for the pollination linkage. We implement this as a factor-neutral (also referred to as Hicks’-neutral) productivity shock, which changes the efficiency coefficient  in each impacted production function, scaled uniquely for each region  and production activity in the following production function:\n\nWe solve the system of equations in the GTAP model (documented extensively in the GTAP user resources) for equilibrium values of prices and quantities where supply equals demand in all markets and all other regions and sectors can be affected by the change. This interconnection of markets shows one of the important advances captured by using a general equilibrium approach: changes in one component of the model will change equilibrium production and input usage, leading to different overall levels of change in the economy depending on exactly which sector in which region is affected.\nFor the pollination shock, the key parameter then is . To identify this parameter, we applied the following algorithm to process outputs from the InVEST Pollinator Sufficiency model\n\nCalculate the total value in 2014 USD of crop production on each 10 arcsecond grid-cell.\n\nFor each grid-cell and for each agricultural production activity , multiply the production tonnage (Monfreda et al. 2008) by the price per ton of that crop, specific to the country the in which the grid-cell is located. These prices are produced from the FAOSTAT database, on which we applied the following missing-data procedure:\n\nIf a price was not available for a crop in a given country for the year 2000, use the 10-year moving average.\nIf 1.b.i cannot be calculated because of missing data, use the continental average price (or its moving average if needed).\nIf 1.b.ii cannot be calculated due to missing data, use the global average price.\n\nResample each activity-specific crop value map from 5 arcminutes to 10 arcseconds (to match the LULC map) using bilinear interpolation.\nAggregate all crop-specific production values from 1.b to get the total value of crop production in each 10 arcsecond grid-cell.\nCalculate the maximum loss of value for each crop in each grid-cell that would occur if zero pollination habitat existed. To get this, we multiplied the production value from 1.b by 1 minus the pollinator dependence of each crop, as identified in Klein et al. 2013.\nAggregate all crop-specific maximum production loss figures from 3 to get the total maximum loss of crop production value.\nCalculate the proportion of maximum value lost for a specific scenario by using the pollination sufficiency map outputted by the InVEST Pollination model for that scenario (see Sharp et al. 2020), categorizing all values greater than 0.3 as having sufficient pollination (threshold chosen based on the approach used in Chaplin-Kramer et al, 2019). In these grid-cells, assign them the value 1, indicating no loss from degraded pollination habitat. For all grid-cells in which pollination dependence is below 0.3, assign them the value 1-(1/.3)*pollionation_sufficiency, which scales the pollination dependence variable so that it is 1.0 right at the threshold but falls linearly to 0 when pollination sufficiency approaches zero.\nCalculate the crop production value lost for each scenario by multiplying the aggregate maximum cropland value lost (4) with the proportion of max value lost specific to that scenario (5).\nFor each AEZ-Region  (n=341), calculate the percent change in cropland value lost in the scenario minus the baseline value lost in 2014. Note that this means that we are only considering newly-lost crop production value and how it compares to the loss of value from pollinators already included in the baseline.\nScale the value in 7 so that it can be aggregated from the AEZ-Region to the Region (n=36) level. Assign this scaled value to the five pollination-dependent production activities (variables gro, ocr, osd, pfb, v_f). This identifies the factor-neutral productivity shock  for each activity and region.\nRerun the full GTAP-model but with the values for  as defined above.\n\n\nThis approach improves upon existing economic models that incorporate pollination. Specifically, past models quantified the contribution of pollination by multiplying the pollination yield-dependency ratio by the value of output for each crop (Gallai, Salles, Settele, & Vaissière, 2009; Lautenbach, Seppelt, Liebscher, & Dormann, 2012). This would provide a proxy estimate of value lost in a static case, but it does not incorporate how changes in production methods or factor usage would lead to different production choices. Other approaches employ CGE methods, such as in Bauer and Wing (2016), but these consider only the complete loss of pollination (and thereby do not consider any degree of spatial dependence between pollinators and pollination-dependent crops). Our approach improves on the literature by incorporating spatially explicit information on which land areas will experience loss in pollinator habitat and calculates the losses from changes in crop pollination, specifying how these losses arise from relevant scenarios of economic growth.\nOne important difference with our model from the CGE analyses of pollination impact discussed above is that our model goes beyond just calculating some arbitrarily large shock on pollination services, but instead calculates how a specific land-use change, calculated by GTAP-AEZ and represented with a high-resolution LULC map, leads to production effects. Calculating the spatially heterogenous impacts of a changed landscape generate informative results. Figure S.6.1.1 presents one example of this, showing how the precise configuration of where the land-use change happens can have a large effect on the productivity shock.\n\nFigure S.6.1: Complexity of landscape affects the degree of pollination service:\n\n\n6.2. Carbon Storage and Timber provision\nWe used the InVEST carbon storage model for both estimating carbon sequestration (this was subsequently valued using the social cost of carbon) and to estimate changes in productivity in the forestry sector. The InVEST carbon storage and sequestration model works by specifying carbon storage levels present in each of four carbon pools (above-ground, below-ground, soil, and dead matter) specific to each LULC class (see the land-use change downscaling section in this SI for specific LULC classes used, along with their parameters). We use parameters for the carbon storage model taken directly from the IPCC Tier 1 method (Ruesch and Gibbs 2008). The base InVEST model is intended to run for a single ecofloristic region, using carbon pool parameters specific to that region (Sharp et al. 2020). To run this globally, we developed separate carbon-pool tables for each of the approximately 125 carbon zones, where each carbon zone is defined as the unique combination of ecofloristic region, continent, and whether the forest is a frontier forest as specified by the IPCC (as in Ruesch and Gibbs 2008). To develop these tables, we built on work from Suh et al. (in submission), which recategorized ESA LULC data into seven functional types. We extended the classes considered to include carbon storage values for agriculture.\nFor the forestry sector, we used the outputs of the carbon storage model to calculate the productivity impacts in each region. Specifically, we calculated the percentage change in carbon storage on land identified by the GTAP-AEZ model as used for forestry. We averaged these estimates for each AEZ-Region to find the average change of biomass on potentially-harvestable land. We implemented this as a factor-neutral productivity shock for the forestry sector of each region according to the average percentage change for in that region. Using carbon storage as a proxy for biomass means that we are including more detail than approaches that are based strictly on forest area (e.g., Baldos et al. 2020), accounting for variation in ecofloristic region, frontier forest status, and other factors, based on field-estimates of aboveground biomass as summarized in Ruesch and Gibbs (2008). However, this proxy approach is very simple compared to detailed modeling of forest harvest. We were not able to compute such forestry models at the global scale, but forthcoming work from several research teams may make this possible for future applications.\n\n\n6.3. Marine Fisheries\nThe InVEST marine fisheries model has been deprecated, and moreover, it would not have been able to produce global estimates. To model the ecosystem changes in marine fisheries, we use outputs from the FISH-MIP program within Intersector Impact Model Intercomparison Project (ISIMIP, isimip.org). We use results from the EcoOcean and BOATS models, based on the GFDL-ESM2M and IPSL climate reanalysis (following the methods documented in Tittensor et al. 2018). The models we used are global food-web models that incorporate both climate change and human pressures on a global, 0.5-degree grid and outputs results for many (51 in the EcoOcean model) trophic and taxonomic groups with the age structure of the populations included. This model run assumed no ocean acidification and excluded diazotrophic fish species (per the FISH-MIP guidelines). These results are reported in figure S.6.3, reproduced with permission from Johnson et al. 2020, showing the biomass density of commercial species under the baseline 2011 condition and under the 2050 BAU.\n We chose to pair three of the FISH-MIP scenarios with our report scenarios as follows: first, our BAU scenario uses the FISH-MIP RCP8.5 and SSP5 scenario with BAU levels of fishing. To calculate the specific shocks given to GTAP, we extracted the total catch biomass (TCB) variable from the FISH-MIP database (hosted under the ISI-MIP data portal at www.isimip.org), which provide monthly and yearly observations of gridded total biomass of catchable, commercially valued species. For the BAU and SP scenarios, we defined the shock as the percentage change in TCB in each of the GTAP zones (augmented to include their 200-mile nautical claims). These shocks, defined in the supplemental data for this article, were applied as a factor-neutral productivity shock, scaling the TFP term up or down each regions’ marine fisheries production function. Note that results for marine fisheries were calculated by joining Marine Exclusive Economic regions with their associated country. We chose to report these results aggregated onto land regions for visual clarity, though the results themselves are calculated on the marine zones.\n[1] https://www.regimeshifts.org/about\n[2] R&D spending data from high-income countries is taken from Heisey and Fuglie (2018). For developing countries, we rely on data from two sources. Data after 1981 is available from the Agricultural Science and Technology Indicators database (https://www.asti.cgiar.org/), and we obtain data before 1981 from Pardey and Roseboom (1989) and Pardey, Roseboom, and Anderson (1991). Our R&D spending data for Eastern Europe and the Soviet Union come from Judd, Boyce, and Evenson (1991).",
    "crumbs": [
      "6. Connecting InVEST outputs to GTAP inputs"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/index.html",
    "href": "gtap_invest/user_guide/index.html",
    "title": "GTAP-InVEST User Guide",
    "section": "",
    "text": "This is the User Guide for GTAP-InVEST, a new earth-economy model introduced in “Investing in nature can improve equity and economic returns” (Johnson et al. 2023, PNAS). The content here originally derived from the supplemental information document included with PNAS article. However, development on the model is progressing rapidly, so please refer to this document, which will remain up-to-date. For additional details, please see the GTAP-InVEST Github repository (github.com/jandrewjohnson/gtap_invest). All code and results of the model are available and licensed under a permissive open-source license. Full results may be downloaded at justinandrewjohnson.com/gtap_invest/results.",
    "crumbs": [
      "GTAP-InVEST User Guide"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/index.html#introduction",
    "href": "gtap_invest/user_guide/index.html#introduction",
    "title": "GTAP-InVEST User Guide",
    "section": "",
    "text": "This is the User Guide for GTAP-InVEST, a new earth-economy model introduced in “Investing in nature can improve equity and economic returns” (Johnson et al. 2023, PNAS). The content here originally derived from the supplemental information document included with PNAS article. However, development on the model is progressing rapidly, so please refer to this document, which will remain up-to-date. For additional details, please see the GTAP-InVEST Github repository (github.com/jandrewjohnson/gtap_invest). All code and results of the model are available and licensed under a permissive open-source license. Full results may be downloaded at justinandrewjohnson.com/gtap_invest/results.",
    "crumbs": [
      "GTAP-InVEST User Guide"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/model_summary.html",
    "href": "gtap_invest/user_guide/model_summary.html",
    "title": "2. GTAP-InVEST Model Summary",
    "section": "",
    "text": "2. GTAP-InVEST Model Summary\nIn this section, we describe how we link together the models that underlie GTAP-InVEST and discuss the new elements added to each model to make this possible. This section assumes the reader is familiar with the underlying GTAP and InVEST models. If this is not the case, see Sections 4 and 5, which document these two models in more depth.\n\n2.1. Overall model structure\nGTAP-InVEST is based on several existing models, though a number of new advances were necessary to make the model linkage. These necessary advances fall into two categories: 1. modifications of the underlying models so that they compute what is needed by the other models; and 2., creation of “linkage” code that expresses the outputs of one model as inputs to another model. Figure S.2.1 summarizes the overall structure of the model.\n\nFigure S.2.1: Overall model linkages within GTAP-InVEST\nThe first step in the model is to project how the economy will evolve between the base year and the terminal year while ignoring impacts from ecosystem services. We refer to this as GTAP Run 1 or the “economic projections run.” It is summarized in Column 1 of Figure S.2.1 (see SI Section 3 for details on the exact input assumptions). The economic projections run calculates a business-as-usual scenario to 2030 that does not yet account for ecosystem services (we label these results as BAU-noES throughout). Because many of the policies rely on detailed consideration of how changing market forces will endogenously drive land-use change and conversion of natural land into economically-utilized land, the GTAP model has expanded representation of heterogeneous land and can endogenously calculate land-use change at the GTAP-AEZ scale. See Section 2.2.3 for a description of how we created this new version of GTAP-AEZ. The results of GTAP Run 1 provide projections of regional land-use change for cropland, pastureland, managed forests and natural land. The next step in GTAP-InVEST is to downscale these regional results (Column 2) using the Spatial Economic Allocation Landscape Simulator (SEALS) from 341 regions to 8.4 billion grid-cells (10 arc-second resolution, or roughly 300 meters at the equator). This is necessary because the models of biodiversity and ecosystem services we used, specifically the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model, require very high-resolution data to capture local dynamics (Column 3). Finally, the outputs of InVEST are fed back into a second run of the economic model (Column 4), referred to as the GTAP Run 2, or the “ecosystem services impact run.” Run 2 assesses how changes in ecosystem services will have feedback effects on the 2030 economy.  In our results, outputs from this second run are labeled with “allES” (or “aES” for short-name variables) to denote that the impacts on the economy from changed ecosystem services are now included. The difference between GTAP Run 1 and Run 2 identifies how much ecosystem services matter (see Figure 1 in the manuscript). The outputs of GTAP Run 2 provide detailed macroeconomic results described in figures 1-3 of the manuscript and in section 7 of this supplemental information.\n\n\n2.2. Input Assumptions\nThe primary source of inputs for assumptions on exogenous drivers come from the Shared Socioeconomic Pathway (SSP) framework from Riahi et al. (2017) and the associated Representative Concentration Pathways (RCPs), such as for population and global surface temperature change (Figure S.2.2). We take population change, technology change (represented as total-factor productivity projections) and other exogenous factors from the literature, following the approach documented in a special issue of the Journal of Global Economic Analysis (Dellink et al., 2020).\nSpecifically, these economic drivers include future growth rates for key economic factors, namely Real GDP, Capital Stock, Population, Unskilled Labor and Skilled Labor from Econmap (v2.4) (Fouré et al., 2013) based on the SSP2 scenario. Sector specific productivity growth for Crops, Ruminants and Non-Ruminants are based on per annum growth rates from Ludena et al (2007) over 2001-2040. Because Ludena et al (2007) does not estimate the growth of the managed forestry sector, we infer growth in this sector based on global agricultural productivity growth as defined in Ludena et al (2007). We impose a 2% productivity growth rate for the Manufactures sector to reflect the productivity gap between the Manufactures and Service sectors (Chateau et al., 2020).\n\nFigure S.2.2: Input Assumptions from the Shared Socioecononomic Pathways. Source: IIASA (2015). SSP Database 2012-2015. Preliminary scenarios. Note:Available from https://tntcat.iiasa.ac.at/SspDb\n\n\n2.3. GTAP-AEZ economic methods\nThe variant of the GTAP model that we use as the starting point for our model extensions is GTAP-AEZ, which builds on the standard model and database by introducing competition for land resources across crops, pasture, and forestry, as well as heterogeneous land use and land endowments within each region and within each Agro-Ecological Zone (AEZ) suitable for a sector’s use. The AEZs are defined in terms of 60 day-long length-of-growing periods, of which there are six, each differentiated by climatic zone (tropical, temperate and boreal). We derive AEZ-level crop production information from Monfreda et al. (2008), while managed forest sector production is based on Sohngen et al. (2009). We draw on multiple sources for land cover information. We use cropland and pasture cover data from Ramankutty et al (2008), urban land cover data from Schneider et al (2009, 2010) and potential vegetation information from Ramankutty (1999). The GTAP-AEZ database is updated to the latest version of the standard GTAP database using national level data from FAOSTAT following the methods described in Baldos (2017). One purpose for defining AEZs is that it lets us specific region-specific endowments of land, which are used by producers as an input to production.\nFigure S.2.3.1 shows the modified production structure in the GTAP AEZ. Note that these changes are limited to managed forestry, ruminant livestock and crops sectors - sectors that use land endowments. Following the standard GTAP model, we estimate sectoral output using a constant elasticities of scale (CES) production structure which minimizes cost of production by combining intermediate and value-added inputs. The former are raw commodity inputs used in the production process while the latter includes factors such as land, labor, capital and natural resources. Our model combines skilled and unskilled labor, land, capital and natural resources under the value-added input CES sub-nest, with land endowments in across AEZs pooled under the land input CES sub-nest. Supply of land across each crop sectors and across land cover types are illustrated in Figure S.2.3.2.\n\nFigure S.2.3.1. Production structure in GTAP-AEZ\n\nFigure S.2.3.2. Land supply in GTAP-AEZ\nIn the model, the regional household owns land endowments and maximizes total returns to land by allocating their endowment across different uses. Starting at the bottom of the constant elasticity of transformation (CET) function land supply nest (Figure S.2.3.2), the regional household allocates the land endowment within its AEZ to managed forestland, cropland and pastureland based on maximizing returns to land. Managed forestland represents land endowments for the forestry sector, while the ruminant livestock sector uses pastureland. Within the crops sector, the household can allocate available cropland for use in the production of each 8 GTAP crop aggregates, depending on changes in land rents for each use. The model computes returns to land endowments based on the cost shares in the database.\n2.3.1. Modifications of GTAP-AEZ\nIn the original GTAP-AEZ model, we assume the supply of land in each AEZ is fixed, though different sectors are able to compete over how land will be used. This means that additional land demanded by one sector needs to come from other sectors. Although this method represented a great advance in the literature on making computable general equilibrium (CGE) models accurately represent land conversion, it is limited in its ability to assess new land being brought into economic use, converted from natural land cover. Our primary modification of GTAP-AEZ in this paper was to add land supply curves, uniquely identified and parameterized for each AEZ-Region, in order to endogenize land supply in the GTAP-AEZ model. Following the approach of Eickhout et al (2009), which the MAGNET model also uses (Woltjer and Kuiper, 2014), land supply in each AEZ is defined as a function of real land rental rates as well as an asymptote which captures maximum available land for use:\n\nThe alpha and beta parameters are taken directly from Woltjer and Kuiper (2014) and the maximum land available is calculated as described below. Using this specification, land supply increases when there are positive increases in land rents. Likewise, if land rents fall, then the supply of land also declines, and we assume that any land not being used is allocated back to natural cover. With this specification, it is possible to set aside land for natural use by reducing the maximum area of available land, as long as these reductions are relatively small (see Dixon et al. 2012).\nThe determination of maximum arable land is very important in this structure. Many existing approaches exist in the literature for defining arable land (e.g., from the Food and Agriculture Organization, but also from the authors of the MAGNET model (Woltjer et al. 2014)). In this paper, we updated the land-supply curves using more recent and higher resolution data on available land. We calculated this using 300 meter resolution data with global availability. In this approach, we also improved the consistency in land-use availability between the competing uses of cropland, pastureland and managed forest land.\nSpecifically, GTAP-InVEST calculates the land-supply curve for each region using the following approach. We combined data on soil suitability and soil-based growth constraints (Fisher et al., 2008), existing crop caloric yield on nearby areas (Johnson et al. 2016), topographic roughness (authors’ calculations, based on data from Hijmans et al. 2008), and existing LULC (based on the European Space Agency’s Climate Change Initiative data, henceforth the ESACCI). For the soils data, which are based on the Harmonized World Soils Database, the method excluded any land that had constraints worse than class 4 (where 1 is unconstrained and 7 is completely constrained).  After eliminating land based on soil constraints, the methodology then further excluded the following areas: land that had less than 20 billion kilocalories produced within the 5 km of the target cell, land that had a topographic roughness index greater than 20, and land with an overall crop suitability lower than 30 percent. Finally, the methodology excluded urban, water, barren, ice and rocky land-use types. See the GTAP-InVEST GitHub repository for more details. This process resulted in AEZ-Region specific values as presented in the supplemental file land_supply_parameters.csv.\nTo download the full defintitions of the 18 AEZs, 37 aggregated GTAP regions and the corresponding ISO3 codes as a vector file (.gpkg), see here GTAP37_AEZ18_ISO3\n\n\n2.4. SEALS downscaling methods\nIn order to analyze ecosystem services, we need to know where within the country the specific changes happen with sufficiently high-resolution data. To enable this, we downscale the regional projections of change produced by the GTAP Run 1 model to a fine (10 arc-second, ~300m) resolution using the Spatial Econometric Allocation Landscape Simulator (SEALS, published in Suh et al. 2020 and extended in Johnson et al. 2020, 2021).\nTo generate useful results, we also ensured that our downscaled results were consistent with medium-resolution land-use change products currently being used by the global sustainability community. Specifically, we used results reported by the Land-Use Harmonization 2 (LUH2), which provides yearly measures of land-use change for 13 classes under each of 5 different Shared Socioeconomic Pathway (SSP) scenarios used by IPBES, reported at a medium (30km) resolution. We combined the LUH2 data with the AEZ-Region results from GTAP Run 1 by scaling the medium resolution LUH2 data up/down in each AEZ-Region so that (1) the total exactly matched that projected by GTAP Run 1 and (2) within the AEZ-Region, the spatial distribution at the medium resolution matched that of the LUH2 data. See Johnson et al. (2023) for more details along with the code documenting this calculation in the GTAP-InVEST repository.\nThese results that combine the GTAP-AEZ projections with the LUH2 data, however, are still not at a fine enough resolution to be used in the InVEST ecosystem service tools. Thus, we used SEALS to downscale from the combined 30km LULC data described above to the necessary 300m resolution. Figure S.2.4.1 illustrates how SEALS downscales the medium resolution data to the finer scale. Specifically, the top panel of the figure shows the spatial distribution of five LUC transitions in the LUH2 data, focusing on grassland conversion, and then zooms in to show which 300m grid-cells change in the downscaled data. The bottom panel shows the projections for agricultural land-use change but illustrates both the contraction (brown) and expansion (green) that happens on the landscape. Note that while initially SEALS modeled the expansion of single land-use types, such as maize expansion (Suh et al. 2020), we expanded from the algorithm defined in Suh et al. so that the SEALS model can consider all land-use changes simultaneously.\n\n\nFigure 2.4.1: Downscaling using the SEALS model\n\n2.4.1.  Allocation Algorithm\nWe report a simplified explanation of the SEALS algorithm for convenience, drawn from previous publications. See the specific publications for more details (e.g., Suh et al. 2020, Johnson et al. 2020, 2021). SEALS uses a simplified LULC classification scheme that is a hierarchically-defined subset of the ESACCI classes (ESA, 2017).The simplification was used because many relationships were not statistically different among similar class specifications (e.g., between deciduous broadleaf and deciduous needle-leaf forests).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSEALS LULC Types\nid\nCombined ESA LULC Types\n\n\nUrban\n1\n190\n\n\nCropland\n2\n10, 11, 12, 20, 30\n\n\nPasture/Grassland\n3\n130\n\n\nForest\n4\n40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100\n\n\nNon-forest vegetation\n5\n110, 120, 121, 122, 140\n\n\nWater\n6\n210\n\n\nBarren or Other\n7\n150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220\n\n\nNo-data\n255\n\n\n\n\nTable S.2.4.1: ESA LULC simplification scheme\nSEALS allocates land-use change by identifying the net change of each LULC class required in each coarse region, then identifying a net change vector , where each entry represents the net change for the i-th land-use type in the coarse cell. The allocation algorithm then takes an n by i matrix of coefficients for how each n-th spatial input affects the probability of i-th expansion in each grid-cell. We provide an example of the table and specification of the functional forms in Table A-4.2. The coefficients actually used are obtained by iteratively solving the allocation algorithm to search for the parameters that minimize the difference between observed change and projected change.\nSEALS Allocation Algorithm\n\n\n\n\n2.4.2 Coefficients used\nTable S.2.4.2. defines the specific coefficients used. These were established using the calibration algorithm described in the next section, averaging calibrated coefficients from 10 different sampled regions.\n\n\n\n\n\n\n\n\n\n\n\n\nspatial_regressor_name\ntype\nclass_1\nclass_2\nclass_3\nclass_4\nclass_5\n\n\nclass_1_constraint\nmultiplicative\n0\n0\n0\n0\n0\n\n\nclass_2_constraint\nmultiplicative\n1\n0\n1\n1\n1\n\n\nclass_3_constraint\nmultiplicative\n1\n1\n0\n1\n1\n\n\nclass_4_constraint\nmultiplicative\n1\n1\n1\n0\n1\n\n\nclass_5_constraint\nmultiplicative\n1\n1\n1\n1\n0\n\n\nclass_6_constraint\nmultiplicative\n0\n0\n0\n0\n0\n\n\nclass_7_constraint\nmultiplicative\n1\n1\n1\n1\n1\n\n\nclass_1_binary\nadditive\n0\n-0.032222222\n0.013888889\n-0.013888889\n-0.016666667\n\n\nclass_2_binary\nadditive\n-0.027777778\n0\n0.016666667\n0.011111111\n0.004333333\n\n\nclass_3_binary\nadditive\n0.005555556\n0.018888889\n0\n0.041666667\n-0.026111111\n\n\nclass_4_binary\nadditive\n-0.019444444\n-0.016666667\n-0.002666667\n0\n0.033444444\n\n\nclass_5_binary\nadditive\n0.01\n0.144444444\n0.060111111\n0.02\n0\n\n\nclass_6_binary\nadditive\n0\n0\n0\n0\n0\n\n\nclass_7_binary\nadditive\n-1.119444444\n0.001666667\n0.126666667\n0.061111111\n-0.023333333\n\n\nclass_1_gaussian_1\ngaussian_parametric_1\n1.713888889\n-1122.233444\n-11.13055556\n0.041666667\n-1122.241667\n\n\nclass_2_gaussian_1\ngaussian_parametric_1\n0.105555556\n0.333444444\n0.022222222\n0\n-11.24444444\n\n\nclass_3_gaussian_1\ngaussian_parametric_1\n0.054444444\n0.005444444\n0.38\n0.018222222\n0.085555556\n\n\nclass_4_gaussian_1\ngaussian_parametric_1\n-0.122222222\n1111.065556\n-0.011\n0.276666667\n-0.022233333\n\n\nclass_5_gaussian_1\ngaussian_parametric_1\n0.010888889\n0\n0.019444444\n-0.122222222\n0.466666667\n\n\nclass_6_gaussian_1\ngaussian_parametric_1\n0.036555556\n-112.2027778\n-112.24\n-1.105555556\n-1133.322333\n\n\nclass_7_gaussian_1\ngaussian_parametric_1\n-0.127777778\n-112.2555556\n-1112.144444\n-1111.133333\n0.005555556\n\n\nclass_1_gaussian_5\ngaussian_parametric_1\n-0.072222222\n-11.52222222\n-111.2638889\n-0.093333333\n-0.087777778\n\n\nclass_2_gaussian_5\ngaussian_parametric_1\n0.068888889\n0.162333333\n-0.016666667\n0.122222222\n0.065555556\n\n\nclass_3_gaussian_5\ngaussian_parametric_1\n0.100222222\n-0.025\n0.431111111\n-0.026677778\n-0.041666667\n\n\nclass_4_gaussian_5\ngaussian_parametric_1\n0.133222222\n0.367777778\n0.076333333\n0.281777778\n0.113333333\n\n\nclass_5_gaussian_5\ngaussian_parametric_1\n0\n-0.073111111\n0.024444444\n-0.005555556\n0.152777778\n\n\nclass_6_gaussian_5\ngaussian_parametric_1\n0.091666667\n0.005\n-1111.105444\n-1.092777778\n-0.002222222\n\n\nclass_7_gaussian_5\ngaussian_parametric_1\n0.045555556\n0.15\n-1111.077778\n-0.008333333\n-110.89\n\n\nclass_1_gaussian_30\ngaussian_parametric_1\n-0.066666667\n-0.073333333\n0.077777778\n-0.026111111\n0\n\n\nclass_2_gaussian_30\ngaussian_parametric_1\n0.011111111\n0.034888889\n-0.081666667\n-0.016666667\n-0.037777778\n\n\nclass_3_gaussian_30\ngaussian_parametric_1\n-0.017222222\n-0.006\n0.308333333\n0.009444444\n0.024333333\n\n\nclass_4_gaussian_30\ngaussian_parametric_1\n-0.016111111\n0.155555556\n0.108888889\n0.056777778\n0.153444444\n\n\nclass_5_gaussian_30\ngaussian_parametric_1\n0.005555556\n-0.021111111\n0.137222222\n0.143444444\n0.105555556\n\n\nclass_6_gaussian_30\ngaussian_parametric_1\n-0.021111111\n0.036555556\n0.152444444\n0\n0.055555556\n\n\nclass_7_gaussian_30\ngaussian_parametric_1\n0.025\n1109.978889\n0.204444444\n-1.080555556\n-0.034555556\n\n\nsoil_organic_content_1m_30s\nadditive\n0.027777778\n-0.15\n110.9777778\n-111.14\n-0.027777778\n\n\nbio_12\nadditive\n11.11944444\n-0.994444444\n1.14\n-1.075\n11.00444444\n\n\nalt\nadditive\n-0.104444444\n0.085\n-0.024888889\n-0.037788889\n0.01\n\n\nbio_1\nadditive\n-0.022111111\n0.044444444\n-0.011111111\n-0.01\n-0.001111111\n\n\nminutes_to_market_30s\nadditive\n0.016122222\n0.21\n0.005555556\n1111.077778\n-0.034333333\n\n\npop_30s\nadditive\n0\n0\n0\n0\n0\n\n\nbulk_density_1m_30s\nadditive\n1.15\n1.122222222\n-0.016666667\n22.18444444\n-11.1\n\n\nCEC_1m_30s\nadditive\n0\n-0.016666667\n0\n111.0722222\n0\n\n\nclay_percent_1m_30s\nadditive\n-0.051111111\n0.02\n-0.186111111\n-0.046222222\n1.084333333\n\n\nph_1m_30s\nadditive\n0\n0.1\n0\n0\n0\n\n\nsand_percent_1m_30s\nadditive\n0.034444444\n0.018333333\n-0.037777778\n-0.048888889\n-0.001111111\n\n\nsilt_percent_1m_30s\nadditive\n-0.012777778\n-0.165\n0\n-0.059011111\n-0.146111111\n\n\n\nTable S.2.4.2. Coefficients used in the SEALS algorithm\n\n\n2.4.3. Calibration\nA key component in SEALS is that it downscales according to observed relationships present in time-series input data. SEALS uses a spatial allocation approach that has been calibrated on the ESACCI 1992-2015 time series using an iterative Gaussian L1-loss function minimization approach. The approach is documented in Figure S.2.4.3.1. as per the following algorithm:\n\nDefine a baseline condition (Panel A, using the year 2000 for this example).\nDefine a projection year in the set of observed years after the baseline year (2010), shown in Panel B, and calculate the net-change between the two years for each coarse resolution (30km) grid-cell. This defines the amount of change in each LULC class that our allocation algorithm will predict.\nAllocate the net change of each LULC class using only the baseline map and a spatial allocation algorithm, S(p1), where p1 is the parameter set used in the allocation and is initially set to an arbitrary value.\nCalculate how accurate the projected LULC map for 2010 (Panel C) compares to the observed 2010 LULC map. Specifically, calculate the difference score, which is the summation of 5 L1-difference functions, one for each LULC transition, that calculates how different (in terms of Gaussian-blurred distance) each class is in the projected map compared to the observed map. This generates a score for the quality of fit for the current set of parameters (Panel D).\nIteratively for each parameter in p1_i, increase the parameter by X percent (initially 10), rerun step 4 with the new parameter, observe the new similarity score, then decrease it by 10 percent and rerun.\nAfter calculating the change in fit from each parameter increase and decrease in Step 5, identify which change had the greatest improvement in the similarity score. Update the parameter set to include the single best change, and then repeat Steps 3-6 until no additional improvements can be made.\n\nFigure 2.4.3.2. shows more detail on the calibration process, highlighting where specific transitions are projected versus where they actually happen, along with the difference score implied.\n\nFigure 2.4.3.1. SEALS calibration process\n\nFigure 2.4.3.2. Assessment of prediction quality-of-fit for 1 LULC class\n\n\n2.4.4. Current Limitations within SEALS\nDue to the computationally heavy nature of calibration, the calibration was only done on a subset of the input data. Subsequent work can improve this by running on more (or potentially all) regions and applying unique values for each region downscaled, instead of averaging the coefficients.\nAdditionally, in our downscaling approach, we chose to match exactly the results from the LUH2 project. This had some downsides, such as locating massive agricultural expansion in the northern Sahara. In locations where the change projected by LUH2 is well outside any observed changes, the calibration is not effective, and visible artifacting is present. In these locations, no allocation method based on the sparse observed data is likely to produce realistic outputs unless the underlying input LUH2 data is modified. We chose not to modify the input LUH2 data in these locations in order to stay consistent with existing approaches, though other applications of this data may benefit from versions that modify the input data. Future research directions should include dynamic updating between the coarse and fine resolutions to resolve the underlying problem.\nIt is also important to note that the modelling outputs (including LULC change maps, InVEST outputs, and GTAP outputs) are not meant to be accurate predictions of future change. Instead, they are illustrations of possible future outcomes given the assumptions used. Furthermore, the modelling approaches used in this project are a first step in exploring how the integration of ecosystem service models (InVEST) and economic models (GTAP) can be connected to help explore the implications of large-scale implementation of global conservation goals, and these methods will be further refined over time.",
    "crumbs": [
      "2. GTAP-InVEST Model Summary"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am an Assistant Professor in Applied Economics at the University of Minnesota. I also work closely with the Natural Capital Project at the University of Minnesota and Stanford University. My research focuses on how the economy affects the environment, and vice versa, on global to local scales. Specifically, I connect models of ecosystem services (how natural capital provides valuable services to humans) with general equilibrium economic models. To do this, I write open-source software (Python, R and C/C++) that uses big data (mostly from satellites) with economic modeling and machine-learning/AI techniques. See my research page for details.\nI now focus mostly on the GTAP-InVEST model, which links the Global Trade Analysis Project (GTAP) out of Purdue University with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model from the Natural Capital Project, based at Stanford University and the University of Minnesota. The project team, which includes the founder of GTAP, Thomas Hertel, and the co-founder of the Natural Capital Project, Stephen Polasky, aims to build strong quantitative evidence on how changes in ecosystem services affect economic performance at the macroeconomic level.\nIn addition to ecosystem services, I also research food security, climate change, agent-based modeling, AI/machine-learning and agricultural management in developing countries. In my spare time, I am a mountain biker, rock climber and board game designer."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "*Authorship notes: Student/advisee names in underline. Shared lead-author is noted with an asterisk.\nJohnson, J. A., Baldos, U. L., Corong, E., Hertel, T., Polasky, P., Cervigni, R., Roxburgh, T., Ruta, G., Salemi, C., and Thakrar, S. “Investing in Nature Can Improve Equity and Economic Returns.” Proceedings of the National Academy of Sciences 120, no. 27 (July 4, 2023): e2220401120. https://doi.org/10.1073/pnas.2220401120.\nKim, H., Peterson, G. D., Cheung, W. W., Ferrier, S., Alkemade, R., Arneth, A., … Johnson, J. A., ...  & Pereira, H. M. (2023). Towards a better future for biodiversity and people: Modelling Nature Futures. Global Environmental Change, 82, 102681.\nvon Jeetze, P. J., Weindl, I., Johnson, J. A., Borrelli, P., Panagos, P., Molina Bacca, E. J., ... & Popp, A. (2023). Projected landscape-scale repercussions of global action for climate and biodiversity protection. Nature Communications, 14(1), 2515.\nCisneros-Pineda, A., Dukes, J. S., Johnson, J. A., Brouder, S., Ramankutty, N., Corong, E., & Chaudhary, A. (2023). The missing markets link in global-to-local-to-global analyses of biodiversity and ecosystem services. Environmental Research Letters, 18(4), 041003.\nKamarajugedda, S. A., Johnson, J. A., McDonald, R., & Hamel, P. (2023). Carbon storage and sequestration in Southeast Asian urban clusters under future land cover change scenarios (2015–2050). Frontiers in Environmental Science, 11, 1105759.\nIsbell, F., Balvanera, P., Mori, A. S., He, J. S., Bullock, J. M., Regmi, G. R., ... Johnson, J. A., ... & Palmer, M. S. (2023). Expert perspectives on global biodiversity loss and its drivers and impacts on people. Frontiers in Ecology and the Environment, 21(2), 94-103.\nJohnson, J. A., Brown, M. E., Corong, E., Dietrich, J. P., Henry, R., Jeetze, P. J. V., ... & Williams, D. R. (2023). The meso scale as a frontier in interdisciplinary modeling of sustainability from local to global scales. Environmental Research Letters.\nChaplin-Kramer, R., Neugarten, R. A., Sharp, R. P., Collins, P. M., Polasky, S., Hole, D., ... Johnson, J. A., ...  & Watson, R. A. (2023). Mapping the planet’s critical natural assets. Nature Ecology & Evolution, 7(1), 51-61.\nJohnson, J. A., & Salemi, C. (2022). Agents on a Landscape: Simulating Spatial and Temporal Interactions in Economic and Ecological Systems. Frontiers in Ecology and Evolution, 524.\nRunge, C. F., Johnson, J. A., Nelson, E., & Redish, A. D. (2022). A neuroscience-based analysis of impacts of disaster memory on economic valuation. Journal of Neuroscience, Psychology, and Economics.\nWeiskopf, Sarah R., Bonnie JE Myers, Maria Isabel Arce-Plata, Julia L. Blanchard, Simon Ferrier, Elizabeth A. Fulton, Mike Harfoot et al. “A Conceptual Framework to Integrate Biodiversity, Ecosystem Function, and Ecosystem Service Models.” BioScience (2022).\nWeiskopf, S. R., Harmáčková, Z., Johnson, C. G., Londoño-Murcia, M. C., Miller, B. W., Myers, B. J., ... Johnson, J. A., … & Rosa, I. M. (2022). Increasing the uptake of ecological model results in policy decisions to improve biodiversity outcomes. Environmental Modelling & Software, 105318.\nJohnson, J. A., Kennedy, C. M., Oakleaf, J. R., Baruch-Mordo, S., Polasky, S., & Kiesecker, J. (2021). Energy matters: Mitigating the impacts of future land expansion will require managing energy and extractive footprints. Ecological Economics, 187, 107106.\nBagstad, K. J., Ingram, J. C., Shapiro, C. D., La Notte, A., Maes, J., Vallecillo, S., ... Johnson, J.A., … & Voigt, B. (2021). Lessons learned from development of natural capital accounts in the United States and European Union. Ecosystem Services, 52, 101359.\nMandle, L., Shields-Estrada, A., Chaplin-Kramer, R., Mitchell, M. G., Bremer, L. L., Gourevitch, J. D., … Johnson, J. A., ... & Ricketts, T. H. (2021). Increasing decision relevance of ecosystem service science. Nature Sustainability, 4(2), 161-169.\nSuh, S., Johnson, J. A., Tambjerg, L., Sim, S., Broeckx-Smith, S., Reyes, W., & Chaplin-Kramer, R. (2020). Closing yield gap is crucial to avoid potential surge in global carbon emissions. Global Environmental Change, 63, 102100.\nJohnson, J. A., Jones, S., Wood, S., Chaplin-Kramer, R., Hawthorne, P., Mulligan, M., Pennington, D., DeClerck, F. (2019). Mapping Ecosystem Services to Human Well-being: a toolkit to support integrated landscape management for the SDGs. Ecological Applications.\nChaplin-Kramer, R., Sharp, R. P., Weil, C., Bennett, E. M., Pascual, U., Arkema, K. Johnson, J.A., … & Hamann, M. (2019). Global modeling of nature’s contributions to people. Science, 366(6462), 255-258.\nOakleaf, J. R., Kennedy, C. M., Baruch-Mordo, S., Gerber, J. S., West, P. C., Johnson, J. A., & Kiesecker, J. (2019). Mapping global development potential for renewable energy, fossil fuels, mining and agriculture sectors. Scientific Data, 6(1), 101.\nSmith, W. K., Nelson, E., Johnson, J. A., Polasky, S., Milder, J. C., Gerber, J. S., ... & Arbuthnot, M. (2019). Voluntary sustainability standards could significantly reduce detrimental impacts of global agriculture. Proceedings of the National Academy of Sciences, 116(6), 2130-2137.\nKeeler, B. L., Hamel, P., McPhearson, T., Hamann, M. H., Donahue, M. L., Prado, K. A. M., … Johnson, J. A., ... & Guerry, A. D. (2019). Social-ecological and technological factors moderate the value of urban nature. Nature Sustainability, 2(1), 29.\nBoyd, J. W., Bagstad, K. J., Ingram, J. C., Shapiro, C. D., Adkins, J. E., Casey, C. F., … Johnson, J. A., ... & Hass, J. L. (2018). The Natural Capital Accounting Opportunity: Let’s Really Do the Numbers. BioScience, 68(12), 940-943.\nKim, H., Rosa, I., Alkemade, R., Leadley, P., Hurtt, G., Popp, A., … Johnson, J. A., ... & Caton, E. (2018). A protocol for an intercomparison of biodiversity and ecosystem services models using harmonized land-use and climate scenarios. Geoscientific Model Development, 11(11), 4537-4562.\nWood, S. L., Jones, S. K., Johnson, J. A. (shared lead-author), Brauman, K. A., Chaplin-Kramer, R., Fremier, A., ... & Mulligan, M. (2018). Distilling the role of ecosystem services in the Sustainable Development Goals. Ecosystem services, 29, 70-82.\nSonter, L. J., Johnson, J. A., Nicholson, C. C., Richardson, L. L., Watson, K. B., & Ricketts, T. H. (2017). Multi-site interactions: Understanding the offsite impacts of land use change on the use and supply of ecosystem services. Ecosystem Services, 23, 158-164.\nJohnson, J. A., Runge, C. F., Senauer, B., & Polasky, S. (2016). Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification. Land Economics, 92(4), 579-592.\nCarlson, K., Gerber, J., Mueller, N., Herrero, M., MacDonald, G., Brauman, K., Havlik, P., O’Connell, C., Johnson, J.A., Saatchi, S., West, P. (2016). Greenhouse gas emissions intensity of global croplands. Nature Climate Change.\nVogl, A. L., Dennedy-Frank, P. J., Wolny, S., Johnson, J. A., Hamel, P., Narain, U., & Vaidya, A. (2016). Managing forest ecosystem services for hydropower production. Environmental Science & Policy, 61, 221-229.\nPolasky, S., Bryant, B., Hawthorne, P., Johnson, J. A., Keeler, B., & Pennington, D. (2015). Inclusive wealth as a metric of sustainable development. Annual Review of Environment and Resources, 40, 445-466.\nChaplin-Kramer, R., Sharp, R.P., Mandle, L., Sim, S., Johnson, J. A., Butnar, I., i Canals, L.M., Eichelberger, B.A., Ramler, I., Mueller, C. and McLachlan, N. (2015). Spatial patterns of agricultural expansion determine impacts on biodiversity and carbon storage. Proceedings of the National Academy of Sciences, 112(24), 7402-7407.\nJohnson, J. A, Runge, C. F., Senauer, B., Foley, J., & Polasky, S. (2014). Global agriculture and carbon trade-offs. Proceedings of the National Academy of Sciences, 111(34), 12342-12347.\nRunge, C. F., & Johnson, J. A. (2014). Are we in this together? Risk bearing and collective action. Journal of Natural Resources Policy Research, 6(1), 71-76.\nRunge, C. F., Sheehan, J. J., Senauer, B., Foley, J., Gerber, J., Johnson, J. A., ... & Runge, C. P. (2012). Assessing the comparative productivity advantage of bioenergy feedstocks at different latitudes. Environmental Research Letters, 7(4), 045906.\nRunge, C. F., Johnson, J. A, & Runge, C. P. (2011). Better milk than cola: soft drink taxes and substitution effects. Choices, 26(3), 3."
  },
  {
    "objectID": "publications/index.html#peer-reviewed-journal-publications",
    "href": "publications/index.html#peer-reviewed-journal-publications",
    "title": "Publications",
    "section": "",
    "text": "*Authorship notes: Student/advisee names in underline. Shared lead-author is noted with an asterisk.\nJohnson, J. A., Baldos, U. L., Corong, E., Hertel, T., Polasky, P., Cervigni, R., Roxburgh, T., Ruta, G., Salemi, C., and Thakrar, S. “Investing in Nature Can Improve Equity and Economic Returns.” Proceedings of the National Academy of Sciences 120, no. 27 (July 4, 2023): e2220401120. https://doi.org/10.1073/pnas.2220401120.\nKim, H., Peterson, G. D., Cheung, W. W., Ferrier, S., Alkemade, R., Arneth, A., … Johnson, J. A., ...  & Pereira, H. M. (2023). Towards a better future for biodiversity and people: Modelling Nature Futures. Global Environmental Change, 82, 102681.\nvon Jeetze, P. J., Weindl, I., Johnson, J. A., Borrelli, P., Panagos, P., Molina Bacca, E. J., ... & Popp, A. (2023). Projected landscape-scale repercussions of global action for climate and biodiversity protection. Nature Communications, 14(1), 2515.\nCisneros-Pineda, A., Dukes, J. S., Johnson, J. A., Brouder, S., Ramankutty, N., Corong, E., & Chaudhary, A. (2023). The missing markets link in global-to-local-to-global analyses of biodiversity and ecosystem services. Environmental Research Letters, 18(4), 041003.\nKamarajugedda, S. A., Johnson, J. A., McDonald, R., & Hamel, P. (2023). Carbon storage and sequestration in Southeast Asian urban clusters under future land cover change scenarios (2015–2050). Frontiers in Environmental Science, 11, 1105759.\nIsbell, F., Balvanera, P., Mori, A. S., He, J. S., Bullock, J. M., Regmi, G. R., ... Johnson, J. A., ... & Palmer, M. S. (2023). Expert perspectives on global biodiversity loss and its drivers and impacts on people. Frontiers in Ecology and the Environment, 21(2), 94-103.\nJohnson, J. A., Brown, M. E., Corong, E., Dietrich, J. P., Henry, R., Jeetze, P. J. V., ... & Williams, D. R. (2023). The meso scale as a frontier in interdisciplinary modeling of sustainability from local to global scales. Environmental Research Letters.\nChaplin-Kramer, R., Neugarten, R. A., Sharp, R. P., Collins, P. M., Polasky, S., Hole, D., ... Johnson, J. A., ...  & Watson, R. A. (2023). Mapping the planet’s critical natural assets. Nature Ecology & Evolution, 7(1), 51-61.\nJohnson, J. A., & Salemi, C. (2022). Agents on a Landscape: Simulating Spatial and Temporal Interactions in Economic and Ecological Systems. Frontiers in Ecology and Evolution, 524.\nRunge, C. F., Johnson, J. A., Nelson, E., & Redish, A. D. (2022). A neuroscience-based analysis of impacts of disaster memory on economic valuation. Journal of Neuroscience, Psychology, and Economics.\nWeiskopf, Sarah R., Bonnie JE Myers, Maria Isabel Arce-Plata, Julia L. Blanchard, Simon Ferrier, Elizabeth A. Fulton, Mike Harfoot et al. “A Conceptual Framework to Integrate Biodiversity, Ecosystem Function, and Ecosystem Service Models.” BioScience (2022).\nWeiskopf, S. R., Harmáčková, Z., Johnson, C. G., Londoño-Murcia, M. C., Miller, B. W., Myers, B. J., ... Johnson, J. A., … & Rosa, I. M. (2022). Increasing the uptake of ecological model results in policy decisions to improve biodiversity outcomes. Environmental Modelling & Software, 105318.\nJohnson, J. A., Kennedy, C. M., Oakleaf, J. R., Baruch-Mordo, S., Polasky, S., & Kiesecker, J. (2021). Energy matters: Mitigating the impacts of future land expansion will require managing energy and extractive footprints. Ecological Economics, 187, 107106.\nBagstad, K. J., Ingram, J. C., Shapiro, C. D., La Notte, A., Maes, J., Vallecillo, S., ... Johnson, J.A., … & Voigt, B. (2021). Lessons learned from development of natural capital accounts in the United States and European Union. Ecosystem Services, 52, 101359.\nMandle, L., Shields-Estrada, A., Chaplin-Kramer, R., Mitchell, M. G., Bremer, L. L., Gourevitch, J. D., … Johnson, J. A., ... & Ricketts, T. H. (2021). Increasing decision relevance of ecosystem service science. Nature Sustainability, 4(2), 161-169.\nSuh, S., Johnson, J. A., Tambjerg, L., Sim, S., Broeckx-Smith, S., Reyes, W., & Chaplin-Kramer, R. (2020). Closing yield gap is crucial to avoid potential surge in global carbon emissions. Global Environmental Change, 63, 102100.\nJohnson, J. A., Jones, S., Wood, S., Chaplin-Kramer, R., Hawthorne, P., Mulligan, M., Pennington, D., DeClerck, F. (2019). Mapping Ecosystem Services to Human Well-being: a toolkit to support integrated landscape management for the SDGs. Ecological Applications.\nChaplin-Kramer, R., Sharp, R. P., Weil, C., Bennett, E. M., Pascual, U., Arkema, K. Johnson, J.A., … & Hamann, M. (2019). Global modeling of nature’s contributions to people. Science, 366(6462), 255-258.\nOakleaf, J. R., Kennedy, C. M., Baruch-Mordo, S., Gerber, J. S., West, P. C., Johnson, J. A., & Kiesecker, J. (2019). Mapping global development potential for renewable energy, fossil fuels, mining and agriculture sectors. Scientific Data, 6(1), 101.\nSmith, W. K., Nelson, E., Johnson, J. A., Polasky, S., Milder, J. C., Gerber, J. S., ... & Arbuthnot, M. (2019). Voluntary sustainability standards could significantly reduce detrimental impacts of global agriculture. Proceedings of the National Academy of Sciences, 116(6), 2130-2137.\nKeeler, B. L., Hamel, P., McPhearson, T., Hamann, M. H., Donahue, M. L., Prado, K. A. M., … Johnson, J. A., ... & Guerry, A. D. (2019). Social-ecological and technological factors moderate the value of urban nature. Nature Sustainability, 2(1), 29.\nBoyd, J. W., Bagstad, K. J., Ingram, J. C., Shapiro, C. D., Adkins, J. E., Casey, C. F., … Johnson, J. A., ... & Hass, J. L. (2018). The Natural Capital Accounting Opportunity: Let’s Really Do the Numbers. BioScience, 68(12), 940-943.\nKim, H., Rosa, I., Alkemade, R., Leadley, P., Hurtt, G., Popp, A., … Johnson, J. A., ... & Caton, E. (2018). A protocol for an intercomparison of biodiversity and ecosystem services models using harmonized land-use and climate scenarios. Geoscientific Model Development, 11(11), 4537-4562.\nWood, S. L., Jones, S. K., Johnson, J. A. (shared lead-author), Brauman, K. A., Chaplin-Kramer, R., Fremier, A., ... & Mulligan, M. (2018). Distilling the role of ecosystem services in the Sustainable Development Goals. Ecosystem services, 29, 70-82.\nSonter, L. J., Johnson, J. A., Nicholson, C. C., Richardson, L. L., Watson, K. B., & Ricketts, T. H. (2017). Multi-site interactions: Understanding the offsite impacts of land use change on the use and supply of ecosystem services. Ecosystem Services, 23, 158-164.\nJohnson, J. A., Runge, C. F., Senauer, B., & Polasky, S. (2016). Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification. Land Economics, 92(4), 579-592.\nCarlson, K., Gerber, J., Mueller, N., Herrero, M., MacDonald, G., Brauman, K., Havlik, P., O’Connell, C., Johnson, J.A., Saatchi, S., West, P. (2016). Greenhouse gas emissions intensity of global croplands. Nature Climate Change.\nVogl, A. L., Dennedy-Frank, P. J., Wolny, S., Johnson, J. A., Hamel, P., Narain, U., & Vaidya, A. (2016). Managing forest ecosystem services for hydropower production. Environmental Science & Policy, 61, 221-229.\nPolasky, S., Bryant, B., Hawthorne, P., Johnson, J. A., Keeler, B., & Pennington, D. (2015). Inclusive wealth as a metric of sustainable development. Annual Review of Environment and Resources, 40, 445-466.\nChaplin-Kramer, R., Sharp, R.P., Mandle, L., Sim, S., Johnson, J. A., Butnar, I., i Canals, L.M., Eichelberger, B.A., Ramler, I., Mueller, C. and McLachlan, N. (2015). Spatial patterns of agricultural expansion determine impacts on biodiversity and carbon storage. Proceedings of the National Academy of Sciences, 112(24), 7402-7407.\nJohnson, J. A, Runge, C. F., Senauer, B., Foley, J., & Polasky, S. (2014). Global agriculture and carbon trade-offs. Proceedings of the National Academy of Sciences, 111(34), 12342-12347.\nRunge, C. F., & Johnson, J. A. (2014). Are we in this together? Risk bearing and collective action. Journal of Natural Resources Policy Research, 6(1), 71-76.\nRunge, C. F., Sheehan, J. J., Senauer, B., Foley, J., Gerber, J., Johnson, J. A., ... & Runge, C. P. (2012). Assessing the comparative productivity advantage of bioenergy feedstocks at different latitudes. Environmental Research Letters, 7(4), 045906.\nRunge, C. F., Johnson, J. A, & Runge, C. P. (2011). Better milk than cola: soft drink taxes and substitution effects. Choices, 26(3), 3."
  },
  {
    "objectID": "publications/index.html#peer-reviewed-books-and-reports",
    "href": "publications/index.html#peer-reviewed-books-and-reports",
    "title": "Publications",
    "section": "Peer-Reviewed Books and Reports",
    "text": "Peer-Reviewed Books and Reports\nJohnson, J. A., Giovanni Ruta, Uris Baldos, Raffaello Cervigni, Shun Chonabayashi, Erwin Corong, Olga Gavryliuk et al. “The Economic Case for Nature.” World Bank Flagship Report. (2021). https://openknowledge.worldbank.org/handle/10986/35882\nJohnson, J. A., Baldos, U., Hertel, T., Liu, J., Nootenboom, C., Polasky, S., and Roxburgh, T. (2020). Global Futures: modelling the global economic impacts of environmental change to support policy-making. World Wildlife Fund Technical Paper. https://www.gtap.agecon.purdue.edu/resources/res_display.asp?RecordID=6186\nRoxburgh, T., Ellis, K., Johnson, J. A., Baldos, U. L., Hertel, T., Nootenboom, C., & Polasky, S. (2020). Global Future: Assessing The Global Economic Impacts of Environmental Change to Support Policy-Making. World Wildlife Fund Report. https://www.gtap.agecon.purdue.edu/resources/res_display.asp?RecordID=5987"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software",
    "section": "",
    "text": "Software\nPage for indexing all software."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "I currently teach:\n\nBig Data Methods in Economics (Ph.D., APEC8222)\nIntroduction to Economics (Undergrad, APEC1101)\nEnvironment and Natural Resource Economics (Ph.D., APEC8601)\nEnvironmental Economics (Undergrad, APEC 3611w)"
  },
  {
    "objectID": "teaching/index.html#teaching-overview",
    "href": "teaching/index.html#teaching-overview",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "I currently teach:\n\nBig Data Methods in Economics (Ph.D., APEC8222)\nIntroduction to Economics (Undergrad, APEC1101)\nEnvironment and Natural Resource Economics (Ph.D., APEC8601)\nEnvironmental Economics (Undergrad, APEC 3611w)"
  },
  {
    "objectID": "teaching/index.html#teaching-philosophy",
    "href": "teaching/index.html#teaching-philosophy",
    "title": "Justin Andrew Johnson",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nMy philosophy for teaching economics is that it should be an interactive experience that exposes students to the core principles of decision making and economic growth, builds students' competence with quantitative and computational methods, and prepares them for future research or jobs. I believe students learn best when they address directly the ways in which economics is connected to underlying moral questions about how a society should be organized. This allows the student to build a better sense of how economics can illuminate policy debates or decision making with analysis techniques and quantitative methods. I then build on this understanding to increase students' mastery of the mathematical, theoretical and computational skills necessary to enable future use of economic models.\nThe reason I emphasize interactive experience is that I find relating abstract economic concepts to specific policy questions or real-life experiences dramatically increases the intuitive (and then eventually, methodological) skills a student can obtain. An example of this I used in the past is conducting game-theory based exercises in class in which students compete to have the highest semester-running score through a series of games. After each version of the game we play, I engage the students in a discussion of why they played the strategy they did and award extra points for describing how their strategy is based on the economic theory. I find that these discussions cement previously discussed lecture material very well. Another interactive method I use is conducting mock-negotiations or debates. For example, in a course I taught on the economics of climate change, each student represented a country and we held an international climate negotiation. Students immediately connected with this approach and were motivated enough to use relatively sophisticated integrated assessment models to support their negotiations with economic analysis. Both of these examples also used new communication and web technologies to improve interactivity and feedback."
  },
  {
    "objectID": "teaching/index.html#teaching-experience",
    "href": "teaching/index.html#teaching-experience",
    "title": "Justin Andrew Johnson",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nI was an adjunct professor and visiting lecturer at St. Olaf College, Minnesota, where I taught undergraduate courses, including Principles of Economics (which covered both microeconomics and macroeconomics) and the Economics of Climate Change. I successfully proposed teaching the latter course at St. Olaf College and prepared an entirely new curriculum of course materials that the economics department still uses. I had extremely positive student evaluations (see justinandrewjohnson.com/teaching for a comprehensive set of evaluation forms). In my role as Senior Scientist at The University of Minnesota, I also guest lectured many times for undergraduate, masters and Ph.D. level courses. In addition to traditional course instruction, I launched a short-course for faculty and staff at The Institute on the Environment entitled \"Python Programming and Big Data for Sustainability Science and Environmental Economics.\""
  },
  {
    "objectID": "teaching/index.html#new-course-development",
    "href": "teaching/index.html#new-course-development",
    "title": "Justin Andrew Johnson",
    "section": "New Course Development",
    "text": "New Course Development\nAs discussed above, I have experience creating and teaching new courses. I want to continue this, proposing the development of new courses. An example of this is proposing a new course on data science that uses the R statistical package and Python programming language to build new economic models based on large online data sources and satellite imagery (an extension of the course I proposed in my current position)."
  },
  {
    "objectID": "teaching/principles_of_microeconomics.html",
    "href": "teaching/principles_of_microeconomics.html",
    "title": "Principles of Microeconomics",
    "section": "",
    "text": "Lecture: Tuesday/Thursday from 1:30 pm - 2:45 pm in Ruttan Hall B35\nDiscussion:\n002 Friday, 9:35 am - 10:25 am in Ruttan Hall 143 003 Friday, 10:40 am - 11:30 am in Ruttan Hall 143 004 Friday, 11:45 am - 12:35 pm in Ruttan Hall 143 005 Friday, 12:50 pm - 1:40 pm in Ruttan Hall 143"
  },
  {
    "objectID": "teaching/principles_of_microeconomics.html#timelocation",
    "href": "teaching/principles_of_microeconomics.html#timelocation",
    "title": "Principles of Microeconomics",
    "section": "",
    "text": "Lecture: Tuesday/Thursday from 1:30 pm - 2:45 pm in Ruttan Hall B35\nDiscussion:\n002 Friday, 9:35 am - 10:25 am in Ruttan Hall 143 003 Friday, 10:40 am - 11:30 am in Ruttan Hall 143 004 Friday, 11:45 am - 12:35 pm in Ruttan Hall 143 005 Friday, 12:50 pm - 1:40 pm in Ruttan Hall 143"
  }
]