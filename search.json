[
  {
    "objectID": "teaching/undergraduate_environmental_economics.html",
    "href": "teaching/undergraduate_environmental_economics.html",
    "title": "Undergraduate Environmental Economics",
    "section": "",
    "text": "Undergraduate Environmental Economics\nLast offered Fall 2023."
  },
  {
    "objectID": "teaching/natural_resource_economics.html",
    "href": "teaching/natural_resource_economics.html",
    "title": "APEC 8601: Natural Resource Economics",
    "section": "",
    "text": "Location: B36 Ruttan Hall\nMeeting Times: Tuesday/Thursday 8:45 - 10:00 am\nCanvas link: https://canvas.umn.edu/courses/412194\nCourse repository: https://github.com/jandrewjohnson/apec_8601_2024\n\n\nThis course has two main objectives:\n\nTo acquaint you with the major issues and seminal literature in natural resource economics. We will cover issues related to the use of renewable resources such as fisheries, aquifers, and timber, and non-renewable resources such as oil and minerals. We will also cover issues related to the conservation of biodiversity and provision of ecosystem services, climate change, and sustainability. We will analyze the issue of efficient use of resources over time and under what conditions market equilibrium achieves an efficient outcome, intergenerational equity and discounting, common property resources, imperfect competition, spatial modeling, uncertainty, and irreversible decisions. We will also consider new research on global integrated assessment modeling and creation of earth-economy models.\nTo increase your ability to do economic research. We will do a set of activities to increase your ability to think critically and formulate specific researchable questions, as well as improving your modeling and analytical skills.\n\n\n\n\nAPEC 8000-8004 or equivalent (graduate level microeconomic theory). It is possible to take microeconomic theory concurrently. If you haven’t had such classes (or are taking them concurrently) please talk to me about what you will need to do to keep up. We will adjust the course based on how familiar students are with the techniques of dynamic optimization (optimal control theory and dynamic programming)."
  },
  {
    "objectID": "teaching/natural_resource_economics.html#course-content-and-objectives",
    "href": "teaching/natural_resource_economics.html#course-content-and-objectives",
    "title": "APEC 8601: Natural Resource Economics",
    "section": "",
    "text": "This course has two main objectives:\n\nTo acquaint you with the major issues and seminal literature in natural resource economics. We will cover issues related to the use of renewable resources such as fisheries, aquifers, and timber, and non-renewable resources such as oil and minerals. We will also cover issues related to the conservation of biodiversity and provision of ecosystem services, climate change, and sustainability. We will analyze the issue of efficient use of resources over time and under what conditions market equilibrium achieves an efficient outcome, intergenerational equity and discounting, common property resources, imperfect competition, spatial modeling, uncertainty, and irreversible decisions. We will also consider new research on global integrated assessment modeling and creation of earth-economy models.\nTo increase your ability to do economic research. We will do a set of activities to increase your ability to think critically and formulate specific researchable questions, as well as improving your modeling and analytical skills."
  },
  {
    "objectID": "teaching/natural_resource_economics.html#prerequisites",
    "href": "teaching/natural_resource_economics.html#prerequisites",
    "title": "APEC 8601: Natural Resource Economics",
    "section": "",
    "text": "APEC 8000-8004 or equivalent (graduate level microeconomic theory). It is possible to take microeconomic theory concurrently. If you haven’t had such classes (or are taking them concurrently) please talk to me about what you will need to do to keep up. We will adjust the course based on how familiar students are with the techniques of dynamic optimization (optimal control theory and dynamic programming)."
  },
  {
    "objectID": "teaching/big_data_ml_and_ai_for_economists.html",
    "href": "teaching/big_data_ml_and_ai_for_economists.html",
    "title": "Big Data, Machine Learning and AI for Economists",
    "section": "",
    "text": "Big Data, Machine Learning and AI for Economists\nNext scheduled offering: 2025 Fall."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "I am an environmental economist who focuses on defining and assessing sustainability at global to local scales. Specifically, I focus on natural capital (the ecosystems, biodiversity and other economically-critical earth-systems processes) and ecosystem services (the valuable flow of services from natural capital to humans). My research informs environmental decision-making in contexts where the economy affects the environment, but also vice-versa, identifying where changes in ecosystem services have macroeconomic effects.\nI have launched a new model combining the Global Trade Analysis Project (GTAP) computable general equilibrium model with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model from the Natural Capital Project. This new model, GTAP-InVEST, builds on our new work published in Science that is able to calculate ecosystem service flows globally, at very high resolution (10m to 300m grid-cells), under a variety of future scenarios. These ecosystem services, in turn, are expressed as shocks to the global economy. Observing how the new economic equilibrium changes, we identify useful impacts from nature, such as effects on GDP, trade flows, employment and commodity prices.\nWe use GTAP-InVEST to answer a variety of important questions:"
  },
  {
    "objectID": "research/index.html#connecting-ecosystem-services-and-natural-capital-to-computable-general-equilibrium",
    "href": "research/index.html#connecting-ecosystem-services-and-natural-capital-to-computable-general-equilibrium",
    "title": "Research",
    "section": "1. Connecting ecosystem services and natural capital to computable general equilibrium",
    "text": "1. Connecting ecosystem services and natural capital to computable general equilibrium\nI launched and now lead the research team behind GTAP-InVEST, a new model that connects natural capital to macroeconomics at the global scale. Specifically, this model combines the Global Trade Analysis Project (GTAP) computable general equilibrium model with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model, a high-resolution, process-based ecosystem model from the Natural Capital Project. Prior work on ecosystem services focused on partial equilibrium modeling and non-market valuation techniques. My work, conversely, considers how ecosystem services and natural capital affect the economy through general equilibrium effects, imputing the value of ecosystem services through shocks on the production system.\nGTAP-InVEST builds on work I published with coauthors in Science (Chaplin-Kramer et al. 2019) that was the first peer-reviewed work to calculate ecosystem services globally with high resolution data (300m grid-cells). Even before this work was published, I launched a parallel research effort that aimed to use these global ecosystem service estimates in a macroeconomic context to assess how valuable such services were to the broader economy. This work was originally published in two high-impact reports for the World Wildlife Fund (Johnson et al. 2020) and the World Bank in “The Economic Case for Nature” (Johnson et al. 2021). More recently, we published this work in the Proceedings of the National Academy of Science (Johnson et al. 2023b), where we assessed five different sustainability policies and their effect on both ecosystem services macroeconomic activity. This allowed us to identify the impact of ecosystem services on indicators such as real GDP, trade flows, employment and commodity prices. Although this project started very recently, we have already seen exponential growth in the number of organizations interested in working with us and raised significant new funding, launching engagements with more than 10 multilateral development banks, sovereign wealth funds and federal reserve systems.\nThe approach used in GTAP-InVEST is deeply connected to Applied Economics and the assessment of land as a resource. We leveraged advances in computable general equilibrium (CGE) modeling, creating a new version of GTAP-AEZ (Agro-Ecological Zones) that differentiated land as an input to production based on the quality and availability of land for use as cropland, pastureland and managed forestry. Previous versions of GTAP-AEZ had assumed that the amount of land available to the economy was fixed, thus allowing different sectors to compete over its use, but not allowing conversion of natural land into economic use. We created and parameterized a novel land-supply function based on remotely-sensed data (Leijten et al. 2023) that calibrated the elasticity of substitution uniquely for each region and AEZ. This innovation let us identify the opportunity cost of establishing protected areas and understand important equity implications for low-income countries."
  },
  {
    "objectID": "research/index.html#predicting-land-use-change-and-environmental-impacts",
    "href": "research/index.html#predicting-land-use-change-and-environmental-impacts",
    "title": "Research",
    "section": "2. Predicting land-use change and environmental impacts",
    "text": "2. Predicting land-use change and environmental impacts\nThe reason no previous models had linked ecosystem services to a global CGE is due both to the computational challenge of computing models at this scale (approximately 8 billion grid-cells) and because the models have to operate across different spatial scales. Although the GTAP-AEZ model we used was able to endogenously predict land-use change, this was only at the Region-AEZ level (n=341). To compute ecosystem services, conversely, we needed this data at 300-meter (or finer) resolution. In order to solve this challenge, I created a land-use change model, the Spatial Economic Allocation Landscape Simulator (SEALS) model, that could allocate regional estimates of land-use change to specific grid-cells at the appropriate resolution. This model used a time-series of land-use, land-cover (LULC) data from the European Space Agency, training the model on data from 1992 to 2010 and validating on the years 2010-2020. The underlying allocation algorithm defined flexible functional forms for 40 spatial covariates, including adjacency relationships between land-use classes, and then employed a stochastic gradient descent algorithm to define the coefficients that best predicted land-use change in the out-of-sample data.\nThe initial SEALS algorithm was described in Suh et al. (2020) and more recently in von Jeetze et al. (2023). It is worth noting that I have published many papers, such as these, which were led by a different author, but in which I was the technical and/or methodological expert that enabled the work. More broadly, this indicates that one of my important academic contributions is producing public goods research, software and algorithms (SEALS now has 14 active users). Since launching this research tract, I have expanded the SEALS model to include more sectors, including energy and extractives (Johnson et al. 2021), and other land-intensive economic activities. This is important because the transition to renewable energy has a very large spatial footprint that could undermine other conservation objectives. We found, however, that because there is a high-degree of flexibility on where renewable energy could be cited (which is the opposite of fossil-fuel energy), it was possible to optimize renewable expansion to minimize environmental impact.\nCritical to making good land-use change predictions with SEALS is the question of where agricultural expansion is possible and how this might change in the future. Very recent work, which I just published with coauthors in Nature Food (Gerber et al. 2024), shows a high degree of spatial heterogeneity in where yield gaps exist and where future crop yield stagnation puts certain regions at risk. These results are already being used in SEALS to identify where land-use change might be exacerbated by lower-than-expected yields."
  },
  {
    "objectID": "research/index.html#advancing-ecosystem-service-methodology",
    "href": "research/index.html#advancing-ecosystem-service-methodology",
    "title": "Research",
    "section": "3. Advancing ecosystem service methodology",
    "text": "3. Advancing ecosystem service methodology\nIn addition to connecting ecosystem service modeling to economics, I have made many fundamental advances in the underlying approach to estimating ecosystem service value in three key areas: improving and expanding estimation methodology, working with large scale model intercomparison projects to conduct model ensemble and assessment activities, and increasing policy relevance of ecosystem service results. On the methodology front, one example contribution is from Johnson and Salemi (2022), in which I incorporated microeconomic decision rules in an agent-based simulation to assess how competition for a scarce public good (firewood) contributed to the value of forest ecosystems in Tanzania. Another example is from Thakrar, Johnson and Polasky (2023), in which we identify how ecosystems and corresponding land-use decisions have large impacts on air quality. We are currently in the process of coding this as an ecosystem service model in InVEST and then incorporating it as a shock to labor productivity into GTAP-InVEST.\nI have also contributed extensively to many large-scale ecosystem service assessments and model intercomparison projects. For example, I was a key participant in the BES-SIM project reported recently in Science (Pereira et al. 2024) that coordinated running of multiple different ecosystem service models on common input data to create ensemble results. Model intercomparison of this type has been an important part of the climate change modeling community and I have had the opportunity to push for this in the ecosystem services community to increase the clarity and validity of our models. In so doing, I have also been extensively involved in recent work to increase the number of ecosystem services that can be computed at the global scale, including in Neugarten et al. (2023), Isbell et al. (2023), Chaplin-Kramer et al. (2023), Mandle et al. (2021), and Kim et al. (2021), among others.\nFinally, I have published several high-impact papers indicating how ecosystem service analysis can improve policy outcomes and how best to provide decision support to policy makers. For example, I led a project to create a new a new decision support tool named MESH (Mapping Ecosystem Services to Humans), documented in Johnson et al. (2019). This was used extensively in a Science for Nature and People Partnership (SNAPP) grant that used the model with 20 decision makers from ministries of finance and forestry in Burkina Faso, Ghana and Tanzania. This work also translated results from ecosystem service models into estimates of how different Sustainable Development Goals were affected, documented in another publication in which I was a co-lead author (Wood et al. 2018)."
  },
  {
    "objectID": "research/index.html#using-machine-learning-and-ai-to-accelerate-economic-sustainability-research",
    "href": "research/index.html#using-machine-learning-and-ai-to-accelerate-economic-sustainability-research",
    "title": "Research",
    "section": "4. Using machine learning and AI to accelerate economic sustainability research",
    "text": "4. Using machine learning and AI to accelerate economic sustainability research\nAs discussed above, I am the lead for Applied Economics on a successful National Science Foundation grant for $20,000,000 to launch the AI-CLIMATE institute at UMN. From this experience, using AI and machine learning has also become a key component of my research agenda. Starting with one of my earliest publications (Johnson et al. 2014, PNAS), I have assessed how to optimize the tradeoff between food production and environmental quality. This research area has grown to incorporate solving for the optimal “tradeoff frontier” between multiple environmental and non-environmental goods, identifying where pareto-improving alterations to the landscape can be made. These tradeoff frontiers are very time-consuming to construct, however, and thereby have limited options for assessing uncertainty. I am currently leading a team within the AI-CLIMATE group to use AI-guided optimization methods that greatly accelerate identification of optimal outcomes. Another part of AI-CLIMATE central to my research is model emulation techniques, which use AI to create reduced-form approximations of more complex models, such as GTAP-InVEST. These emulated models can be run in near-real-time, enabling deeper exploration of complex solution spaces and improved integration with policymakers via decision-support tools."
  },
  {
    "objectID": "research/index.html#new-methods-in-linking-general-equilibrium-to-earth-systems-models",
    "href": "research/index.html#new-methods-in-linking-general-equilibrium-to-earth-systems-models",
    "title": "Research",
    "section": "1. New methods in linking general equilibrium to earth systems models",
    "text": "1. New methods in linking general equilibrium to earth systems models\nIn the Johnson-Polasky research lab (and soon under our new departmental center, NatCap TEEMs), we refer to a new type of work called Earth-Economy modeling. We define this type of model as one that combines general equilibrium modeling with high-resolution earth-systems modeling, defined with sufficient sectoral and spatial detail to do policy relevant analysis on global to local scales. There has been an explosion in interest in this type of work, but many new research challenges will need to be solved for this to keep progressing. First, I intend to convert our Earth-Economy model, GTAP-InVEST, to a recursive-dynamic formulation, rather than the current comparative-static approach. This is especially important when analyzing regime-changes and tipping points, which have become some of the most cited aspects of my prior work. It will also enable analysis of additional economic phenomena, such as endogenous capital accumulation and savings behavior. This research is well underway, supported by an ongoing grant from the World Bank and their Changing Wealth of Nations project. Second, I will increase the level of detail in the energy sectors in my model, with particular emphasis on renewable resources. In the paper I led in Ecological Economics (Johnson et al. 2021), we showed that renewable energy has a large spatial footprint, which could undermine conservation objectives. For this work, we took projections from the International Energy Agency, but future work could instead model the expansion and contraction of different energy sectors endogenously by linking to GTAP-InVEST. My model is particularly well-suited for this type of work because it is one of the only global CGEs with endogenous competition over land. Third, I will incorporate explicit climate change emissions and damages into the model. We have already created a custom version of the DICE model and have incorporated it into GTAP-InVEST, but improved iterations will use more detailed climate integrated assessment models, like the FUND model. Finally, I expect to pair GTAP-InVEST with other types of models used throughout the economics discipline. Multiple central banks, including the U.S. Federal Reserve, have already reached out to me to inquire about linking my CGE approach with their monetary models and dynamic stochastic general equilibrium models. Their primary objective is to assess how resilient their banking polices are to large, systemic environmental shocks. Additionally, we have almost finalized another large contract with World Bank to connect our GTAP-InVEST model to their structural econometric fiscal model (MFMOD), which will further enhance the relevance of nature-aware macroeconomic modeling."
  },
  {
    "objectID": "research/index.html#what-is-possible-versus-how-to-get-there",
    "href": "research/index.html#what-is-possible-versus-how-to-get-there",
    "title": "Research",
    "section": "2. What is possible versus how to get there?",
    "text": "2. What is possible versus how to get there?\nThere are two parts of my research that, to date, have been separate but have great potential for being joined. The first part is research that creates pareto-efficiency curves to describe environmental and economic tradeoffs. These curves describe a range of efficient outcomes and show that when the current landscape is not on the efficiency frontier, there exist win-win improvements for environmental and economic objectives. Frontier analysis shows what is biophysically possible, but it does not specify how to move from the current, interior point in frontier space outwards towards efficient points. The second part of my research, CGE modeling work with GTAP-InVEST, is well-positioned to specify how different policies move us towards (or away) from efficient outcomes. Different policy scenarios, such as those assessed in our PNAS piece (Johnson et al. 2023b), can be scored in frontier space to identify how or if they move towards more efficient outcomes. Combining natural efficiency frontiers with GTAP-InVEST will enable analysis of both what is possible and how to get there."
  },
  {
    "objectID": "research/index.html#natural-capital-accounting-and-private-sector-investing",
    "href": "research/index.html#natural-capital-accounting-and-private-sector-investing",
    "title": "Research",
    "section": "3. Natural capital accounting and private sector investing",
    "text": "3. Natural capital accounting and private sector investing\nGreat interest has arisen from governmental and private sector agents who want to know how changes in natural capital and ecosystem services can be quantified as in accounting frameworks compatible with systems of national accounts (SNAs). The System of Environmental-Economic Accounting (SEEA) has grown in prominence, garnering support from President Biden in an executive order in 2022. I have already been involved in this work, participating in the first natural capital accounting working group hosted by the U.S. Geological Survey, and I intend to continue in this direction. One of the reasons why SNAs have been useful for guiding policy is that they define clear and consistent performance metrics (like Gross Domestic Product) that can be projected by forward-looking economic models, like CGEs. As natural capital accounting grows in importance, my nature-aware Earth-Economy model is well positioned to serve a similar role for environmental indicators. This type of information is also demanded by private firms, particularly investors, who want to know how their investment choices might be affected by degradation of natural capital. For example, the Norwegian Sovereign Wealth Fund and Central Bank have commissioned us to provide estimates of how changes in natural capital can be expressed as investment exposure for their funds. This work will continue, aiming to provide clear and methodologically rigorous estimates ready for use in corporate sustainability reporting."
  },
  {
    "objectID": "research/index.html#formalization-of-multiscale-spatial-analysis",
    "href": "research/index.html#formalization-of-multiscale-spatial-analysis",
    "title": "Research",
    "section": "4. Formalization of multiscale spatial analysis",
    "text": "4. Formalization of multiscale spatial analysis\nI published an article in Environmental Research Letters (Johnson et al. 2023a) that argued an important frontier in sustainability research was identifying cross-scale interactions in global-to-local research. In particular, I argued that many phenomena operate at a meso scale between global and local levels, and that often these effects are both important and hard to analyze. Since publishing this paper, I have become increasingly focused on quantifying how cross-scale analysis can raise important new insights. For example, most prior work in estimating the value of ecosystem services has focused on bottom-up analysis, including hedonic approaches, contingent valuation, replacement cost, and a variety of other approaches. These approaches, however, are hard to scale to global contexts while remaining consistent, especially when policies are very large and have general equilibrium effects. An alternative approach that I will explore is thinking about top-down identification of ecosystem service value. Specifically, with GTAP-InVEST, it is possible to impute the value of ecosystem services by calculating the change in welfare that occurs after a change in the provision of ecosystem services. For example, instead of estimating the value of pollination services by asking individual farmers how much they would be willing to pay for pollination, one could calculate the overall reduction in yield from reduced pollination services and input this as a shock into a general equilibrium model to calculate the reduction in sectoral profitability, GDP, or other relevant metrics."
  },
  {
    "objectID": "research/index.html#spatial-regression-ai-and-land-use-change-prediction",
    "href": "research/index.html#spatial-regression-ai-and-land-use-change-prediction",
    "title": "Research",
    "section": "5. Spatial regression, AI and land-use change prediction",
    "text": "5. Spatial regression, AI and land-use change prediction\nCreating the SEALS algorithm, described above, was the key innovation that allowed linking GTAP to InVEST. The algorithm validates well on withheld data and performs the required task of increasing LULC resolution, but there is an opportunity to improve the accuracy of its predictions. SEALS represents spatial adjacency via convolutions (2-dimensional moving-window transformations) in its regression, but it does not take the logical next step of using a deep convolutional neural network. Convolutional neural networks are widely used in image processing and many other domains similar to land-use change prediction, and so they might hold promise for enhancing the accuracy of the SEALS predictions. More generally, however, there exist image-generation techniques that could improve predictions further, including the use of generative adversarial networks (GANs). Prior work in this area has assessed security implications for how GANs could be used to create false satellite imagery (Marín and Escalera, 2021), but I believe such approaches could be used not to falsify such imagery but to predict it. I intend to couple this with Knowledge Guided Machine Learning (KGML) techniques (e.g., Liu et al. 2022), pioneered by the Principal Investigators of the AI-CLIMATE grant here at UMN, to push these methods towards producing LULC maps that provide both accurate, cross-validated predictions, but also improve how much they resemble the general spatial patterns evident from satellite data.\nSpatial regression more generally also will be an important element of my future research. I recently published a book chapter (Weil, Johnson and Chaplin-Kramer, 2021) that highlights this direction, showing how a common machine learning algorithm, XGBOOST, could more accurately predict spatial patterns of crop production. Separately, I have developed algorithms that allow for regression on very large, gridded datasets. Most spatial regression models experience performance constraints when applied to datasets with more than 1,000 spatial points. By imposing structure on the spatial relationships between covariates, namely by defining them via different convolutions, it is possible to retain information on adjacency effects among variables while also scaling the size of the regression to millions of spatial points. This approach is already documented in a submitted manuscript and will be supported by R and Python packages to assist others in implementing these algorithms."
  },
  {
    "objectID": "mesh/index.html",
    "href": "mesh/index.html",
    "title": "MESH: Mapping Ecosystem Services to Human well-being",
    "section": "",
    "text": "Mapping Ecosystem Services to Human well-being – MESH – is an ecosystem service assessment, mapping and reporting toolkit to enable sustainable decision making among governments, non-profits and businesses. MESH is an integrative modelling framework that calculates ecosystem service production functions and maps ecosystem service provision under different landscape management scenarios. The base model of MESH integrates and extends ecosystem service models from the Natural Capital Project's 'InVEST' toolkit into a graphical framework (Figure 1) and includes methods to automatically create input data, define scenarios and visualize outputs (without the need to use, e.g., ArcGIS).\nMESH is hosted by The Natural Capital Project at Stanford University were you can download the most recent stable release. Please see forums.naturalcapitalproject.org to discuss MESH or request support. The most recent development release of MESH and links to data can be found on this page.\n\nMESH 5-Minute Tutorial\nHere's a horribly low-quality demo video I  created. Note that it's on a slightly dated version of MESH but still illustrates the basic work-flow."
  },
  {
    "objectID": "gtap_invest/user_guide/scenarios.html",
    "href": "gtap_invest/user_guide/scenarios.html",
    "title": "3. Scenarios and Policy specification",
    "section": "",
    "text": "3. Scenarios and Policy specification\nTo understand different aspects of the earth-economy linkage, we compare multiple scenarios on different policies, economic assumptions, and conceptions of “business as usual” (BAU). In this section, we report how we used the created model with multiple scenarios to assess various aspects of the earth-economy linkage.\nBefore running policy-relevant scenarios, our first step was to update the baseline economy from 2014 to 2021 to be closer to the present year. To do this, we ran a “Base Year Calibration” scenario that projects the global economy from Y2014 – the latest reference year of the GTAP v.10 database – to Y2021, which is the base year used in this paper. We then apply all policies and other shocks to this 2021 snapshot of the economy. To generate the 2021 economy, we specified how the exogenous factors in our model would evolve from 2014 to 2021 and then applied them as a shock to the 2014 equilibrium. We use a similar approach for scenarios that proceed to 2030 (represented in Figure S.3.1 as the “Economic Drivers” arrow on the right-hand side).\nWith the 2021 baseline economy defined, we then define two sets of future scenarios that project from Y2021 to Y2030. Below, we discuss these two sets of scenarios, namely the BAU scenarios (right-hand side of Figure S.3.1) and the Policy Scenarios (left-hand side).\n\nFigure S.3.1: Summary of scenarios and policies analyzed\n\n3.1. BAU scenarios\nOur BAU scenarios include “no Ecosystem Services”, “Ecosystem Services”, “Economic Rigidities” and “Ecosystem Collapse” scenarios. To create Figure 1 in the main text, we calculated the difference between the “no Ecosystem Services” scenario, which projects basic economic activity to 2030 but ignores all changes in ecosystem services, and the “Ecosystems Services” scenario, which incorporates shocks to the economy from changes in pollination services, timber carbon stock and fisheries stock. The difference between these scenarios identifies a subset of estimates on “how much nature matters” and gives us as sense of how wrong our calculations can be if we ignore earth-economy linkages.\n3.1.1. Economic Rigidity\nOur BAU scenarios also consider two important sensitivity analyses. First, in the “Economic Rigidities” scenario, we assess what happens to the economy if it exhibits more rigidity and less substitutability. Specifically, this scenario builds on the “Ecosystem Services” scenario and uses smaller elasticity values for key economic parameters in GTAP-AEZ model. These include: (a) Elasticity of transformation between land cover types, (b) Elasticity of cropland transformation among crops, (c) Constant elasticity of substitution (CES) between primary factors in production, (d) Armington CES for regional allocation of imports and (e) Armington CES for domestic/imported allocation. We obtain lowers bound of the 95% confidence intervals for the Armington CES for regional allocation of imports and Armington CES for domestic/imported allocation based on adjusted estimates from Hertel et al (2007) and Hertel et al (2006) respectively. For the other parameters, standard values are deflated by 50%.\n3.1.2. Ecosystem Collapse\nWe also analyzed what happens to the economy under an “Ecosystem Collapse” scenario, which analyzes very large changes in ecosystems services rather than just small shocks from currently projected land-use change, as analyzed in the main manuscript. These ecosystem collapse shocks are a highly simplified representation of what might happen when key relationships in the ecosystem pass ecological tipping-points. We draw from the tipping-point scenarios defined in the Dasgupta Review of the Economics of Biodiversity (2021, pp. 375, Box 14.3), which used an earlier version of the GTAP-InVEST model. The three specific elements shocked in the collapse scenario are widespread dieback of tropical forests, collapse of wild pollinator populations, and severe losses in marine fishery populations.\nA large body of literature suggests that regime changes may happen because large parts of the biosphere are close to tipping points (Rockström et al. 2009; Steffen et al. 2015). According to these studies, it is possible that when some (probably unknown) ecological thresholds are passed, it might trigger large, non-linear, systemic change in the health of entire ecosystems.  It is extremely challenging to predict when tipping points might be crossed. Instead, these results are not predicted to happen at some specific point in time, but are presented to explore scenarios and economic implications of large-scale ecological change. We suggest that our readers interpret these results with some caution because the exact thresholds are unknown and what happens beyond the threshold remains poorly understood, especially at large spatial scales (Lenton 2013).\nConceptually, it is important to consider such non-linear changes and estimate how they might have further, possibly non-linear, effects in the economy. Traditional CGE models ameliorate many negative impacts to some degree by substitution away from affected sectors. However, it might be the case that such substitution is limited in overall quantity and that very large changes could cause decreased flexibility within the economy, amplifying negative effects. We discussed this possibility above in our section on economic rigidity.\nTo determine which tipping points we wanted to assess, we reviewed the “Regime Changes” database produced by the Stockholm Resilience Center[1]. We identified three scenarios that we were able to evaluate in GTAP-InVEST. These include assessing wide-spread collapse of tropical forests that results in forests converting into grasslands and shrubs, global pollinator collapse, and climate-related reductions in fisheries output.\nWe define the specific shocks that we impose on the GTAP-InVEST model in Table S.3.1, reproduced with permission from Johnson et al. (2021).\n\n\n\n\n\n\n\nScenario\nMethod to calculate shock\n\n\nWild pollination collapse\nThe BAU and policy scenario considers how pollination services would change when different levels of pollinator habitat are present near pollinator-dependent crops. It does not consider what happens if broad-scale reductions in pollinator colony health result in additional changes unrelated to LULC configuration. To model this extended pollinator collapse scenario, we modified the pollination scenario to also contain a 90 percent reduction in pollination sufficiency. The 90 percent reduction is less severe than other attempts to model pollinator collapse, e.g., Bauer and Wang (2016) evaluate a scenario where most, but not all, species of wild pollinators cease to provide service. Note that this shock means crops only partially dependent on pollination services will not see yield reductions as large as the pollinator collapse. Assessing the extent to which markets shift, consumers substitute to non-pollinator crops, and producers substitute to non-pollinator intermediate goods will test the global market’s ability to absorb such a large shock.\n\n\nMarine fisheries collapse\nThe model relies on the Fisheries and Marine Ecosystem Model Intercomparison Project data (Lotze et al. 2019). To simulate the regime shift, the model assumes a severe climate change scenario (8.5 instead of RCP4.5) and further takes the worst-case outcome in terms of climate change impact reported in the uncertainty bounds and sensitivity analysis. The model simulates severe disruptions of fish migration that lead to a reduced total catch biomass, which in turn impacts the economic model.\nThis type of collapse would reflect when, for example, fish populations are blocked from migrating north or south to keep a constant habitat. The reduced fisheries impact the model by lowering Total Catch Biomass in the projections, which registers as a technology-neutral productivity change in the fisheries sector.\n\n\nWidespread conversion of tropical forests to savannah\nTo create this regime-change shock, we used the SEALS model to project a landscape where 88% of the forests in tropical regions (specifically AEZs 5 and 6) were converted to grassland or shrubland. The landscape generated by this calculation had much less forest cover, so when it was used as an input to InVEST, the relative sizes of ecosystem service impacts were much larger.\nAs with all of these tipping-point scenarios, the precise magnitude of forest dieback is unknown, and instead we aimed to provide illustrative values that can then be processed through the rest of the GTAP-InVEST model. l\n\n\n\nTable S.3.1: Shocks applied to the GTAP-InVEST model\nNote that the definition of this ecosystem-collapse scenario is subtly different than in Johnson et al. (2021). In the previous study, the shocks to update the global economy from 2021 to 2030 (see Figure S.3.1) were imposed along with the ecosystem-collapse shocks. With this approach, the contractionary impacts of ecosystem-collapse interacted with economic growth, thereby resulting in higher losses ($2.7 trillion in the previous study versus $2.0 trillion here). In this article, we applied the ecosystem-collapse shock after the economy grew to the 2030 level. We did this to isolate the impacts of ecosystem-collapse on direct economic activity rather than in combination with growth effects, which makes attribution to ecosystem services more challenging. Ecosystem-service and economic growth interactions remain important, however, so future research is needed to further identify this interrelation.\n3.2. Policy Scenarios\nThe second set of scenarios are policy scenarios that define several nature-smart policies to see how they impact the outcomes of the GTAP-InVEST model. Our approach draws on four different policies: (i) removing agricultural subsidies, (ii) domestic carbon forest payments, (iii) global carbon forest payments, and (iv) agricultural research and development (R&D). Removing agricultural subsidies, sometimes referred to as “decoupling support to farmers”, requires that certain agricultural subsidies are replaced with a direct payment to landowners using the “savings” obtained from not paying the subsidy. Carbon forest payments can be either domestically or globally managed. In the domestic case, governments compensate landowners to preserve land instead of converting it for agriculture or other purposes. For globally managed forest payments, wealthier countries contribute to a global pool based on their historical emissions. The pool is allocated to developing countries, compensating them to limit land use in a manner equivalent to the payment received. Research and development investments focus on increasing the efficiency of land already converted for agricultural purposes, meaning that supply can rise without expansions in land use.\nReferring back to Figure S.3.1, the “Policies” scenarios include subsidy repurposing and payments to ecosystem services. Input and output subsidies paid to the agricultural sector are reallocated as land input subsidies (“Land Payments”) or are reinvested into public agricultural R&D (“Agricultural R&D”). Calculations under “Agricultural R&D” scenario relies heavily on the framework from Baldos et al. (2019), which estimates the public R&D spending increase required to offset agricultural productivity losses from climate change. The payment schemes to ecosystem services include “Local PES” and “Global PES”. Under the “Global PES” scenario, high-income countries fund a global budget via income transfers. The budget compensates income losses in countries that set aside land for natural use. In the “Local PES” scheme, the budget is based on the amount of input and output subsidies in agriculture for each region. Finally, the “Combined Policies” scenario incorporates the methods from the “Agricultural R&D” and “Global PES” scenarios",
    "crumbs": [
      "3. Scenarios and Policy specification"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/model_summary.html",
    "href": "gtap_invest/user_guide/model_summary.html",
    "title": "2. GTAP-InVEST Model Summary",
    "section": "",
    "text": "2. GTAP-InVEST Model Summary\nIn this section, we describe how we link together the models that underlie GTAP-InVEST and discuss the new elements added to each model to make this possible. This section assumes the reader is familiar with the underlying GTAP and InVEST models. If this is not the case, see Sections 4 and 5, which document these two models in more depth.\n\n2.1. Overall model structure\nGTAP-InVEST is based on several existing models, though a number of new advances were necessary to make the model linkage. These necessary advances fall into two categories: 1. modifications of the underlying models so that they compute what is needed by the other models; and 2., creation of “linkage” code that expresses the outputs of one model as inputs to another model. Figure S.2.1 summarizes the overall structure of the model.\n\nFigure S.2.1: Overall model linkages within GTAP-InVEST\nThe first step in the model is to project how the economy will evolve between the base year and the terminal year while ignoring impacts from ecosystem services. We refer to this as GTAP Run 1 or the “economic projections run.” It is summarized in Column 1 of Figure S.2.1 (see SI Section 3 for details on the exact input assumptions). The economic projections run calculates a business-as-usual scenario to 2030 that does not yet account for ecosystem services (we label these results as BAU-noES throughout). Because many of the policies rely on detailed consideration of how changing market forces will endogenously drive land-use change and conversion of natural land into economically-utilized land, the GTAP model has expanded representation of heterogeneous land and can endogenously calculate land-use change at the GTAP-AEZ scale. See Section 2.2.3 for a description of how we created this new version of GTAP-AEZ. The results of GTAP Run 1 provide projections of regional land-use change for cropland, pastureland, managed forests and natural land. The next step in GTAP-InVEST is to downscale these regional results (Column 2) using the Spatial Economic Allocation Landscape Simulator (SEALS) from 341 regions to 8.4 billion grid-cells (10 arc-second resolution, or roughly 300 meters at the equator). This is necessary because the models of biodiversity and ecosystem services we used, specifically the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model, require very high-resolution data to capture local dynamics (Column 3). Finally, the outputs of InVEST are fed back into a second run of the economic model (Column 4), referred to as the GTAP Run 2, or the “ecosystem services impact run.” Run 2 assesses how changes in ecosystem services will have feedback effects on the 2030 economy.  In our results, outputs from this second run are labeled with “allES” (or “aES” for short-name variables) to denote that the impacts on the economy from changed ecosystem services are now included. The difference between GTAP Run 1 and Run 2 identifies how much ecosystem services matter (see Figure 1 in the manuscript). The outputs of GTAP Run 2 provide detailed macroeconomic results described in figures 1-3 of the manuscript and in section 7 of this supplemental information.\n\n\n2.2. Input Assumptions\nThe primary source of inputs for assumptions on exogenous drivers come from the Shared Socioeconomic Pathway (SSP) framework from Riahi et al. (2017) and the associated Representative Concentration Pathways (RCPs), such as for population and global surface temperature change (Figure S.2.2). We take population change, technology change (represented as total-factor productivity projections) and other exogenous factors from the literature, following the approach documented in a special issue of the Journal of Global Economic Analysis (Dellink et al., 2020).\nSpecifically, these economic drivers include future growth rates for key economic factors, namely Real GDP, Capital Stock, Population, Unskilled Labor and Skilled Labor from Econmap (v2.4) (Fouré et al., 2013) based on the SSP2 scenario. Sector specific productivity growth for Crops, Ruminants and Non-Ruminants are based on per annum growth rates from Ludena et al (2007) over 2001-2040. Because Ludena et al (2007) does not estimate the growth of the managed forestry sector, we infer growth in this sector based on global agricultural productivity growth as defined in Ludena et al (2007). We impose a 2% productivity growth rate for the Manufactures sector to reflect the productivity gap between the Manufactures and Service sectors (Chateau et al., 2020).\n\nFigure S.2.2: Input Assumptions from the Shared Socioecononomic Pathways. Source: IIASA (2015). SSP Database 2012-2015. Preliminary scenarios. Note:Available from https://tntcat.iiasa.ac.at/SspDb\n\n\n2.3. GTAP-AEZ economic methods\nThe variant of the GTAP model that we use as the starting point for our model extensions is GTAP-AEZ, which builds on the standard model and database by introducing competition for land resources across crops, pasture, and forestry, as well as heterogeneous land use and land endowments within each region and within each Agro-Ecological Zone (AEZ) suitable for a sector’s use. The AEZs are defined in terms of 60 day-long length-of-growing periods, of which there are six, each differentiated by climatic zone (tropical, temperate and boreal). We derive AEZ-level crop production information from Monfreda et al. (2008), while managed forest sector production is based on Sohngen et al. (2009). We draw on multiple sources for land cover information. We use cropland and pasture cover data from Ramankutty et al (2008), urban land cover data from Schneider et al (2009, 2010) and potential vegetation information from Ramankutty (1999). The GTAP-AEZ database is updated to the latest version of the standard GTAP database using national level data from FAOSTAT following the methods described in Baldos (2017). One purpose for defining AEZs is that it lets us specific region-specific endowments of land, which are used by producers as an input to production.\nFigure S.2.3.1 shows the modified production structure in the GTAP AEZ. Note that these changes are limited to managed forestry, ruminant livestock and crops sectors - sectors that use land endowments. Following the standard GTAP model, we estimate sectoral output using a constant elasticities of scale (CES) production structure which minimizes cost of production by combining intermediate and value-added inputs. The former are raw commodity inputs used in the production process while the latter includes factors such as land, labor, capital and natural resources. Our model combines skilled and unskilled labor, land, capital and natural resources under the value-added input CES sub-nest, with land endowments in across AEZs pooled under the land input CES sub-nest. Supply of land across each crop sectors and across land cover types are illustrated in Figure S.2.3.2.\n\nFigure S.2.3.1. Production structure in GTAP-AEZ\n\nFigure S.2.3.2. Land supply in GTAP-AEZ\nIn the model, the regional household owns land endowments and maximizes total returns to land by allocating their endowment across different uses. Starting at the bottom of the constant elasticity of transformation (CET) function land supply nest (Figure S.2.3.2), the regional household allocates the land endowment within its AEZ to managed forestland, cropland and pastureland based on maximizing returns to land. Managed forestland represents land endowments for the forestry sector, while the ruminant livestock sector uses pastureland. Within the crops sector, the household can allocate available cropland for use in the production of each 8 GTAP crop aggregates, depending on changes in land rents for each use. The model computes returns to land endowments based on the cost shares in the database.\n2.3.1. Modifications of GTAP-AEZ\nIn the original GTAP-AEZ model, we assume the supply of land in each AEZ is fixed, though different sectors are able to compete over how land will be used. This means that additional land demanded by one sector needs to come from other sectors. Although this method represented a great advance in the literature on making computable general equilibrium (CGE) models accurately represent land conversion, it is limited in its ability to assess new land being brought into economic use, converted from natural land cover. Our primary modification of GTAP-AEZ in this paper was to add land supply curves, uniquely identified and parameterized for each AEZ-Region, in order to endogenize land supply in the GTAP-AEZ model. Following the approach of Eickhout et al (2009), which the MAGNET model also uses (Woltjer and Kuiper, 2014), land supply in each AEZ is defined as a function of real land rental rates as well as an asymptote which captures maximum available land for use:\n\nThe alpha and beta parameters are taken directly from Woltjer and Kuiper (2014) and the maximum land available is calculated as described below. Using this specification, land supply increases when there are positive increases in land rents. Likewise, if land rents fall, then the supply of land also declines, and we assume that any land not being used is allocated back to natural cover. With this specification, it is possible to set aside land for natural use by reducing the maximum area of available land, as long as these reductions are relatively small (see Dixon et al. 2012).\nThe determination of maximum arable land is very important in this structure. Many existing approaches exist in the literature for defining arable land (e.g., from the Food and Agriculture Organization, but also from the authors of the MAGNET model (Woltjer et al. 2014)). In this paper, we updated the land-supply curves using more recent and higher resolution data on available land. We calculated this using 300 meter resolution data with global availability. In this approach, we also improved the consistency in land-use availability between the competing uses of cropland, pastureland and managed forest land.\nSpecifically, GTAP-InVEST calculates the land-supply curve for each region using the following approach. We combined data on soil suitability and soil-based growth constraints (Fisher et al., 2008), existing crop caloric yield on nearby areas (Johnson et al. 2016), topographic roughness (authors’ calculations, based on data from Hijmans et al. 2008), and existing LULC (based on the European Space Agency’s Climate Change Initiative data, henceforth the ESACCI). For the soils data, which are based on the Harmonized World Soils Database, the method excluded any land that had constraints worse than class 4 (where 1 is unconstrained and 7 is completely constrained).  After eliminating land based on soil constraints, the methodology then further excluded the following areas: land that had less than 20 billion kilocalories produced within the 5 km of the target cell, land that had a topographic roughness index greater than 20, and land with an overall crop suitability lower than 30 percent. Finally, the methodology excluded urban, water, barren, ice and rocky land-use types. See the GTAP-InVEST GitHub repository for more details. This process resulted in AEZ-Region specific values as presented in the supplemental file land_supply_parameters.csv.\nTo download the full defintitions of the 18 AEZs, 37 aggregated GTAP regions and the corresponding ISO3 codes as a vector file (.gpkg), see here GTAP37_AEZ18_ISO3\n\n\n2.4. SEALS downscaling methods\nIn order to analyze ecosystem services, we need to know where within the country the specific changes happen with sufficiently high-resolution data. To enable this, we downscale the regional projections of change produced by the GTAP Run 1 model to a fine (10 arc-second, ~300m) resolution using the Spatial Econometric Allocation Landscape Simulator (SEALS, published in Suh et al. 2020 and extended in Johnson et al. 2020, 2021).\nTo generate useful results, we also ensured that our downscaled results were consistent with medium-resolution land-use change products currently being used by the global sustainability community. Specifically, we used results reported by the Land-Use Harmonization 2 (LUH2), which provides yearly measures of land-use change for 13 classes under each of 5 different Shared Socioeconomic Pathway (SSP) scenarios used by IPBES, reported at a medium (30km) resolution. We combined the LUH2 data with the AEZ-Region results from GTAP Run 1 by scaling the medium resolution LUH2 data up/down in each AEZ-Region so that (1) the total exactly matched that projected by GTAP Run 1 and (2) within the AEZ-Region, the spatial distribution at the medium resolution matched that of the LUH2 data. See Johnson et al. (2023) for more details along with the code documenting this calculation in the GTAP-InVEST repository.\nThese results that combine the GTAP-AEZ projections with the LUH2 data, however, are still not at a fine enough resolution to be used in the InVEST ecosystem service tools. Thus, we used SEALS to downscale from the combined 30km LULC data described above to the necessary 300m resolution. Figure S.2.4.1 illustrates how SEALS downscales the medium resolution data to the finer scale. Specifically, the top panel of the figure shows the spatial distribution of five LUC transitions in the LUH2 data, focusing on grassland conversion, and then zooms in to show which 300m grid-cells change in the downscaled data. The bottom panel shows the projections for agricultural land-use change but illustrates both the contraction (brown) and expansion (green) that happens on the landscape. Note that while initially SEALS modeled the expansion of single land-use types, such as maize expansion (Suh et al. 2020), we expanded from the algorithm defined in Suh et al. so that the SEALS model can consider all land-use changes simultaneously.\n\n\nFigure 2.4.1: Downscaling using the SEALS model\n\n2.4.1.  Allocation Algorithm\nWe report a simplified explanation of the SEALS algorithm for convenience, drawn from previous publications. See the specific publications for more details (e.g., Suh et al. 2020, Johnson et al. 2020, 2021). SEALS uses a simplified LULC classification scheme that is a hierarchically-defined subset of the ESACCI classes (ESA, 2017).The simplification was used because many relationships were not statistically different among similar class specifications (e.g., between deciduous broadleaf and deciduous needle-leaf forests).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSEALS LULC Types\nid\nCombined ESA LULC Types\n\n\nUrban\n1\n190\n\n\nCropland\n2\n10, 11, 12, 20, 30\n\n\nPasture/Grassland\n3\n130\n\n\nForest\n4\n40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100\n\n\nNon-forest vegetation\n5\n110, 120, 121, 122, 140\n\n\nWater\n6\n210\n\n\nBarren or Other\n7\n150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220\n\n\nNo-data\n255\n\n\n\n\nTable S.2.4.1: ESA LULC simplification scheme\nSEALS allocates land-use change by identifying the net change of each LULC class required in each coarse region, then identifying a net change vector , where each entry represents the net change for the i-th land-use type in the coarse cell. The allocation algorithm then takes an n by i matrix of coefficients for how each n-th spatial input affects the probability of i-th expansion in each grid-cell. We provide an example of the table and specification of the functional forms in Table A-4.2. The coefficients actually used are obtained by iteratively solving the allocation algorithm to search for the parameters that minimize the difference between observed change and projected change.\nSEALS Allocation Algorithm\n\n\n\n\n2.4.2 Coefficients used\nTable S.2.4.2. defines the specific coefficients used. These were established using the calibration algorithm described in the next section, averaging calibrated coefficients from 10 different sampled regions.\n\n\n\n\n\n\n\n\n\n\n\n\nspatial_regressor_name\ntype\nclass_1\nclass_2\nclass_3\nclass_4\nclass_5\n\n\nclass_1_constraint\nmultiplicative\n0\n0\n0\n0\n0\n\n\nclass_2_constraint\nmultiplicative\n1\n0\n1\n1\n1\n\n\nclass_3_constraint\nmultiplicative\n1\n1\n0\n1\n1\n\n\nclass_4_constraint\nmultiplicative\n1\n1\n1\n0\n1\n\n\nclass_5_constraint\nmultiplicative\n1\n1\n1\n1\n0\n\n\nclass_6_constraint\nmultiplicative\n0\n0\n0\n0\n0\n\n\nclass_7_constraint\nmultiplicative\n1\n1\n1\n1\n1\n\n\nclass_1_binary\nadditive\n0\n-0.032222222\n0.013888889\n-0.013888889\n-0.016666667\n\n\nclass_2_binary\nadditive\n-0.027777778\n0\n0.016666667\n0.011111111\n0.004333333\n\n\nclass_3_binary\nadditive\n0.005555556\n0.018888889\n0\n0.041666667\n-0.026111111\n\n\nclass_4_binary\nadditive\n-0.019444444\n-0.016666667\n-0.002666667\n0\n0.033444444\n\n\nclass_5_binary\nadditive\n0.01\n0.144444444\n0.060111111\n0.02\n0\n\n\nclass_6_binary\nadditive\n0\n0\n0\n0\n0\n\n\nclass_7_binary\nadditive\n-1.119444444\n0.001666667\n0.126666667\n0.061111111\n-0.023333333\n\n\nclass_1_gaussian_1\ngaussian_parametric_1\n1.713888889\n-1122.233444\n-11.13055556\n0.041666667\n-1122.241667\n\n\nclass_2_gaussian_1\ngaussian_parametric_1\n0.105555556\n0.333444444\n0.022222222\n0\n-11.24444444\n\n\nclass_3_gaussian_1\ngaussian_parametric_1\n0.054444444\n0.005444444\n0.38\n0.018222222\n0.085555556\n\n\nclass_4_gaussian_1\ngaussian_parametric_1\n-0.122222222\n1111.065556\n-0.011\n0.276666667\n-0.022233333\n\n\nclass_5_gaussian_1\ngaussian_parametric_1\n0.010888889\n0\n0.019444444\n-0.122222222\n0.466666667\n\n\nclass_6_gaussian_1\ngaussian_parametric_1\n0.036555556\n-112.2027778\n-112.24\n-1.105555556\n-1133.322333\n\n\nclass_7_gaussian_1\ngaussian_parametric_1\n-0.127777778\n-112.2555556\n-1112.144444\n-1111.133333\n0.005555556\n\n\nclass_1_gaussian_5\ngaussian_parametric_1\n-0.072222222\n-11.52222222\n-111.2638889\n-0.093333333\n-0.087777778\n\n\nclass_2_gaussian_5\ngaussian_parametric_1\n0.068888889\n0.162333333\n-0.016666667\n0.122222222\n0.065555556\n\n\nclass_3_gaussian_5\ngaussian_parametric_1\n0.100222222\n-0.025\n0.431111111\n-0.026677778\n-0.041666667\n\n\nclass_4_gaussian_5\ngaussian_parametric_1\n0.133222222\n0.367777778\n0.076333333\n0.281777778\n0.113333333\n\n\nclass_5_gaussian_5\ngaussian_parametric_1\n0\n-0.073111111\n0.024444444\n-0.005555556\n0.152777778\n\n\nclass_6_gaussian_5\ngaussian_parametric_1\n0.091666667\n0.005\n-1111.105444\n-1.092777778\n-0.002222222\n\n\nclass_7_gaussian_5\ngaussian_parametric_1\n0.045555556\n0.15\n-1111.077778\n-0.008333333\n-110.89\n\n\nclass_1_gaussian_30\ngaussian_parametric_1\n-0.066666667\n-0.073333333\n0.077777778\n-0.026111111\n0\n\n\nclass_2_gaussian_30\ngaussian_parametric_1\n0.011111111\n0.034888889\n-0.081666667\n-0.016666667\n-0.037777778\n\n\nclass_3_gaussian_30\ngaussian_parametric_1\n-0.017222222\n-0.006\n0.308333333\n0.009444444\n0.024333333\n\n\nclass_4_gaussian_30\ngaussian_parametric_1\n-0.016111111\n0.155555556\n0.108888889\n0.056777778\n0.153444444\n\n\nclass_5_gaussian_30\ngaussian_parametric_1\n0.005555556\n-0.021111111\n0.137222222\n0.143444444\n0.105555556\n\n\nclass_6_gaussian_30\ngaussian_parametric_1\n-0.021111111\n0.036555556\n0.152444444\n0\n0.055555556\n\n\nclass_7_gaussian_30\ngaussian_parametric_1\n0.025\n1109.978889\n0.204444444\n-1.080555556\n-0.034555556\n\n\nsoil_organic_content_1m_30s\nadditive\n0.027777778\n-0.15\n110.9777778\n-111.14\n-0.027777778\n\n\nbio_12\nadditive\n11.11944444\n-0.994444444\n1.14\n-1.075\n11.00444444\n\n\nalt\nadditive\n-0.104444444\n0.085\n-0.024888889\n-0.037788889\n0.01\n\n\nbio_1\nadditive\n-0.022111111\n0.044444444\n-0.011111111\n-0.01\n-0.001111111\n\n\nminutes_to_market_30s\nadditive\n0.016122222\n0.21\n0.005555556\n1111.077778\n-0.034333333\n\n\npop_30s\nadditive\n0\n0\n0\n0\n0\n\n\nbulk_density_1m_30s\nadditive\n1.15\n1.122222222\n-0.016666667\n22.18444444\n-11.1\n\n\nCEC_1m_30s\nadditive\n0\n-0.016666667\n0\n111.0722222\n0\n\n\nclay_percent_1m_30s\nadditive\n-0.051111111\n0.02\n-0.186111111\n-0.046222222\n1.084333333\n\n\nph_1m_30s\nadditive\n0\n0.1\n0\n0\n0\n\n\nsand_percent_1m_30s\nadditive\n0.034444444\n0.018333333\n-0.037777778\n-0.048888889\n-0.001111111\n\n\nsilt_percent_1m_30s\nadditive\n-0.012777778\n-0.165\n0\n-0.059011111\n-0.146111111\n\n\n\nTable S.2.4.2. Coefficients used in the SEALS algorithm\n\n\n2.4.3. Calibration\nA key component in SEALS is that it downscales according to observed relationships present in time-series input data. SEALS uses a spatial allocation approach that has been calibrated on the ESACCI 1992-2015 time series using an iterative Gaussian L1-loss function minimization approach. The approach is documented in Figure S.2.4.3.1. as per the following algorithm:\n\nDefine a baseline condition (Panel A, using the year 2000 for this example).\nDefine a projection year in the set of observed years after the baseline year (2010), shown in Panel B, and calculate the net-change between the two years for each coarse resolution (30km) grid-cell. This defines the amount of change in each LULC class that our allocation algorithm will predict.\nAllocate the net change of each LULC class using only the baseline map and a spatial allocation algorithm, S(p1), where p1 is the parameter set used in the allocation and is initially set to an arbitrary value.\nCalculate how accurate the projected LULC map for 2010 (Panel C) compares to the observed 2010 LULC map. Specifically, calculate the difference score, which is the summation of 5 L1-difference functions, one for each LULC transition, that calculates how different (in terms of Gaussian-blurred distance) each class is in the projected map compared to the observed map. This generates a score for the quality of fit for the current set of parameters (Panel D).\nIteratively for each parameter in p1_i, increase the parameter by X percent (initially 10), rerun step 4 with the new parameter, observe the new similarity score, then decrease it by 10 percent and rerun.\nAfter calculating the change in fit from each parameter increase and decrease in Step 5, identify which change had the greatest improvement in the similarity score. Update the parameter set to include the single best change, and then repeat Steps 3-6 until no additional improvements can be made.\n\nFigure 2.4.3.2. shows more detail on the calibration process, highlighting where specific transitions are projected versus where they actually happen, along with the difference score implied.\n\nFigure 2.4.3.1. SEALS calibration process\n\nFigure 2.4.3.2. Assessment of prediction quality-of-fit for 1 LULC class\n\n\n2.4.4. Current Limitations within SEALS\nDue to the computationally heavy nature of calibration, the calibration was only done on a subset of the input data. Subsequent work can improve this by running on more (or potentially all) regions and applying unique values for each region downscaled, instead of averaging the coefficients.\nAdditionally, in our downscaling approach, we chose to match exactly the results from the LUH2 project. This had some downsides, such as locating massive agricultural expansion in the northern Sahara. In locations where the change projected by LUH2 is well outside any observed changes, the calibration is not effective, and visible artifacting is present. In these locations, no allocation method based on the sparse observed data is likely to produce realistic outputs unless the underlying input LUH2 data is modified. We chose not to modify the input LUH2 data in these locations in order to stay consistent with existing approaches, though other applications of this data may benefit from versions that modify the input data. Future research directions should include dynamic updating between the coarse and fine resolutions to resolve the underlying problem.\nIt is also important to note that the modelling outputs (including LULC change maps, InVEST outputs, and GTAP outputs) are not meant to be accurate predictions of future change. Instead, they are illustrations of possible future outcomes given the assumptions used. Furthermore, the modelling approaches used in this project are a first step in exploring how the integration of ecosystem service models (InVEST) and economic models (GTAP) can be connected to help explore the implications of large-scale implementation of global conservation goals, and these methods will be further refined over time.",
    "crumbs": [
      "2. GTAP-InVEST Model Summary"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/index.html",
    "href": "gtap_invest/user_guide/index.html",
    "title": "GTAP-InVEST User Guide",
    "section": "",
    "text": "This is the User Guide for GTAP-InVEST, a new earth-economy model introduced in “Investing in nature can improve equity and economic returns” (Johnson et al. 2023, PNAS). The content here originally derived from the supplemental information document included with PNAS article. However, development on the model is progressing rapidly, so please refer to this document, which will remain up-to-date. For additional details, please see the GTAP-InVEST Github repository (github.com/jandrewjohnson/gtap_invest). All code and results of the model are available and licensed under a permissive open-source license. Full results may be downloaded at justinandrewjohnson.com/gtap_invest/results.",
    "crumbs": [
      "GTAP-InVEST User Guide"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/index.html#introduction",
    "href": "gtap_invest/user_guide/index.html#introduction",
    "title": "GTAP-InVEST User Guide",
    "section": "",
    "text": "This is the User Guide for GTAP-InVEST, a new earth-economy model introduced in “Investing in nature can improve equity and economic returns” (Johnson et al. 2023, PNAS). The content here originally derived from the supplemental information document included with PNAS article. However, development on the model is progressing rapidly, so please refer to this document, which will remain up-to-date. For additional details, please see the GTAP-InVEST Github repository (github.com/jandrewjohnson/gtap_invest). All code and results of the model are available and licensed under a permissive open-source license. Full results may be downloaded at justinandrewjohnson.com/gtap_invest/results.",
    "crumbs": [
      "GTAP-InVEST User Guide"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html",
    "title": "Variables in v11",
    "section": "",
    "text": "Extracted from https://www.gtap.agecon.purdue.edu/models/setsVariables.asp\n\n\n\n\n\nVariable Label\nVariable Name\n\n\n\n\nNSAV_COMM\nNon-Savings Commodities (NS)\n\n\nTRAD_COMM\nTraded Commodities (TC)\n\n\nDEMD_COMM\nDemanded Commodities (DC)\n\n\nPROD_COMM\nProduced Commodities (PC)\n\n\nENDW_COMM\nEndowment Commodities (EC)\n\n\nENDWS_COMM\nSluggish Endowment Commodities (ECS)\n\n\nENDWM_COMM\nMobile Endowment Commodities (ECM)\n\n\nCGDS_COMM\nCapital Goods Commodities (“cgds”)\n\n\nENDWC_COMM\nCapital Endowment Commodity (“capital”)\n\n\n\n\n\n\n\n\n\nVariable Label\nVariable Name\n\n\n\n\nPROD_COMM\nin NSAV_COMM\n\n\nDEMD_COMM\nin NSAV_COMM\n\n\nCGDS_COMM\nin NSAV_COMM\n\n\nENDW_COMM\nin DEMD_COMM\n\n\nTRAD_COMM\nin DEMD_COMM\n\n\nTRAD_COMM\nin PROD_COMM\n\n\nCGDS_COMM\nin PROD_COMM\n\n\nENDWS_COMM\nin ENDW_COMM\n\n\nENDWM_COMM\nin ENDW_COMM\n\n\nENDWC_COMM\nin NSAV_COMM\n\n\nENDWC_COMM\nin ENDW_COMM\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Label\nVariable Name\n\n\n\n\nREG\n= {usa, eu, row}\n\n\nNSAV_COMM\n= {land, labor, capital, food, manufacturing, services, capital goods}\n\n\nTRAD_COMM\n= {food, manufacturing, services}\n\n\nDEMD_COMM\n= {land, labor, capital, food, manufacturing, services}\n\n\nPROD_COMM\n= {food, manufacturing, services, capital goods}\n\n\nENDW_COMM\n= {land, labor, capital}\n\n\nENDWS_COMM\n= {land, capital}\n\n\nENDWM_COMM\n= {labor}\n\n\nCGDS_COMM\n= {capital goods}\n\n\nENDWC_COMM\n= {capital}\n\n\n\n\n\n\nEVOAir value of endowment commodity i output or supplied, in region r, evaluated at agent’s prices\n\nEVOAi,r = PSi,r * QOi,r with i = ENDW_COMM and r\n\nEVFAijr value of purchases of demanded commodity i by firms in sector j, in region r, evaluated at agent’s prices\n\nEVFAijr = PFEijr * QFEijr        i = TRAD_COMM; j = ENDW_COMM; r = REG\n\nVDFAijr value of purchases of domestic tradeable commodity i by firms in sector j of region r evaluated at agents’ prices\n\nVDFAijr = PFDijr * QFDijr        i = TRAD_COMM; j = ENDW_COMM; r = REG\n\nVIFAijr value of purchases of imported tradeable commodity i by firms in sector j of region r evaluated at agents’ prices\n\nVIFAijr = PFMijr * QFMijr        i = TRAD_COMM; j = ENDW_COMM; r = REG\n\nVDPAir value of expenditure on domestic tradable commodity i, by private household in region r evaluated at agent’s prices\n\nVDPAir = PPDir * QPDir        i = TRAD_COMM; r = REG\n\nVIPAir value of expenditure on imported tradeable commodity i by firms in sector j of region r evaluated at agents’ prices\n\nVIPAir = PPMir * QPMir        i = TRAD_COMM; r = REG\n\nVDGAi value of government’s expenditure on domestic tradable commodity i, in region r, evaluated at agent’s prices\n\nVDGAir = PGDir * QGDir        i = TRAD_COMM; r = REG\n\nVIGAi value of government’s expenditure on imported tradable commodity i, in region r, evaluated at agent’s prices\n\nVIGAir = PGMir * QGMir        i = TRAD_COMM; r = REG\n\nSAVEr value of (net) savings in region r\n\nSAVEr = PSAVEr * QSAVEr        r = REG\n\nVKBr value of beginning-of-period capital stock in region r\n\nVKBr = PCGDSr * KBr        r = REG\n\nVDEPr value of capital depreciation expenditure in region r\n\nVDEPr = PCGDSr * KBr        r = REG\n\n\n\n\nVFMijr value of purchases of endowment commodity i by firms in sector j in region r, evaluated at market prices\n\nVFMijr = PMir * QFEijr        i = ENDWM_COMM; j = PROD_COMM; r = REG\nVFMijr = PMESijr * QFEijr        i = ENDWS_COMM; j = PROD_COMM; r = REG\n\nVDFMijr value of purchases of domestic tradable commodity i by firms in sector j in region r, evaluated at market prices\n\nVDFMijr = PMir * QFDijr        i = TRAD_COMM; j = PROD_COMM\n\nVIFMijr value of purchases of imported tradable commodity i by firms in sector j in region r, evaluated at market prices\n\nVIFMijr = PIMir * QFMijr        i = TRAD_COMM; j = PROD_COMM; r = REG\n\nVDPMir value of private household’s purchases of domestic tradable commodity i in region r, evaluated at market prices\n\nVDPMir = PMir * QPDir        i = TRAD_COMM; r = REG\n\nVIPMir value of private household’s purchases of imported tradable commodity i in region r, evaluated at market prices\n\nVIPMir = PIMir * QPMir        i = TRAD_COMM; r = REG\n\nVDGMir value of government’s expenditure on domestic tradable commodity i, in region r, evaluated at market prices\n\nVDGMir = PMir * QGDir        i = TRAD_COMM; r = REG\n\nVIGMir value of government’s expenditure on imported tradable commodity i, in region r, evaluated at market prices\n\nVIGMir = PIMir * QGMir        i = TRAD_COMM; r = REG\n\nVXMDirs value of exports of tradable commodity i from source r to destination s, evaluated at (exporter’s) market prices\n\nVXMDirs = PMir * QXSirs        i = TRAD_COMM; r = REG; s= REG\n\nVIMSirs value of imports of tradable commodity i from source r to destination s, evaluated at (importer’s) market prices\n\nVIMSirs = PMSir * QXSirs        i = TRAD_COMM; r = REG; s = REG\n\nVSTir value of sales of tradable commodity i to the international transport sector in region r, evaluated at market prices\n\nVSTir = PMir * QSTir        i = MARG_COMM; r = REG\n\n\n\n\nVXWDirs value of exports of tradable commodity i from source r to destination s, evaluated at world (fob) prices\n\nVXWDirs = PFOBirs * QXSirs        i = TRAD_COMM; r = REG; s = REG\n\nVIWSirs value of imports of tradable commodity i from source r to destination s, evaluated at world (cif) prices\n\nVIWSirs = PCIFirs * QXSirs        i = TRAD_COMM; r = REG; s = REG\n\n\n\n\nESUBVAj with j = PROD_COMM substitution parameter between primary factors in the CES value-added nest of the nested CES production function of sector j of all regions (i.e. elasticity of substitution between primary production factors; for example, in a situation where land doesn’t play a role in production, hence labor and capital are the only primary factors, this parameter indicates how sensitive is the K/L ratio to a 1 percent change in the wage/rental ratio)\nESUBDi    with i = TRAD_COMM substitution parameter between domestic and composite imported commodities in the Armington utility/production structure of agent/sector i in all regions\nESUBMi    with i = TRAD_COMM substitution parameter among imported commodities from different sources in the Armington utility/production structure of agent/sector i in all regions.\n\n\n\nSUBPARir    with i = TRAD_COMM; r = REG substitution parameter for tradable commodity i in the CDE minimum expenditure function of region r\nINCPARir    with i = TRAD_COMM; r = REG expansion parameter for tradable commodity i in the CDE minimum expenditure function of region r.\nALPHAir 1 - sub. parameter in the CDE minimum expenditure function\n\nALPHAir = 1 - SUBPARir        i = TRAD_COMM; r = REG\n\nAPEikr Allen partial elst.of sub between composite i and k in r\n\nAPEikr = ALPHAir + ALPHAkr- SnCONSHRnr*ALPHAnr        i = k= n = TRAD_COMM; r = REG\n\nAPEiir = 2.0*ALPHAir - SnCONSHRnr*ALPHAnr - ALPHAir / CONSHRir\nEYir income elasticity of private household demand for i in r\n\nEYir = (1/Sn CONSHRnr*INCPARnr) * (INCPARir * (1.0 - ALPHAir) + SnCONSHRnr*INCPARnr*ALPHAnr) + (ALPHAir - SnCONSHRnr*ALPHAnr) i = n = TRAD_COMM; r = REG\n\nEPikr uncompensated cross-price elasticity private household demand for i wrt k in r\n\nEPikr = (APEikr - EYir)*CONSHRkr        i = k = TRAD_COMM; r = REG\n\n\n\n\nETRAEi    with i = ENDW_COMM transformation parameter between uses for a sluggish primary factor i in the one level CET production function (i.e. this parameter indicates how easy/difficult it is to transfer a sluggish factor (say, capital) from one sector to another; the closer is ETRAE to zero, the more immobile is the factor between alternative uses)\nRORFLEXr    with r = REG flexibility of expected net rate of return on capital stock in region r with respect to investment (if a region’s capital stock increases by 1%, then it is expected that the net rate of return on capital will decline by RORFLEX%)\nRORDELTA binary coefficient that determines the mechanism of allocating investment across regions (when RORDELTA=1, investment is allocated across regions to equate the change in the expected rates of return, rore(r), when RORDELTA=0, investment is allocated across regions to maintain the existing composition of capital stocks).\n\n\n\nVOAir value of non-saving commodity i output or supplied in region r evaluated at agents’ prices\n\nVOAir = EVOAir        i = ENDW_COMM; r = REG\nVOAir = SjVFAjir        i = PROD_COMM; j = DEMD_COMM; r = REG\n\nVFAijr value of purchases of demanded commodity i by firms in sector j of regin r evaluated at agents’ prices\n\nVFAijr = EVFAijr        i = ENDW_COMM; j = PROD_COMM; r = REG\nVFAijr = VDFAijr + VIFAijr        i =TRAD_COMM; j = PROD_COMM; r = REG\n\nVOMir value of non-saving commodity i output or supplied in region r evaluated at market prices\n\nVOMir = SjVFMijr        i = ENDW_COMM;j = PROD_COMM; r = REG\nVOMir = VDMir + VSTir + SsVXMDirs        i = MARG_COMM; r = REG; s = REG\nVOMir = VDMir + SsVXMDirs        i = NMRG_COMM; r = REG; s = REG\nVOMir = VOAir        i = CGDS_COMM; r = REG\n\nVDMir value of domestic sales of tradable commodity i in region r evaluated at market prices\n\nVDMir = VDPMir + VDGMir + SjVDFMijr        i = TRAD_COMM; j = PROD_COMM; r = REG\n\nVIMir value of aggregate imports of tradable commodity i in region r evaluated at market prices\n\nVIMir = VIPMir + VIGMir + SjVIFMijr        i = TRAD_COMM; j = PROD_COMM; r = REG\n\nVPAir value of private household expenditure on tradable commodity i in region r evaluated at agents’ prices\n\nVPAir = VDPAir + VIPAir        i = TRAD_COMM; r = REG\n\nPRIVEXPr total consumption expenditure by private household in regin r\n\nPRIVEXPr = SiVPAir        i = TRAD_COMM\n\nVGAir value of government expenditure on tradable commodity i in region r evaluated at agents’ prices\n\nVGAir = VDGAir + VIGAir        i = TRAD_COMM; r = REG\n\nGOVEXPr total consumption expenditure by government in region r\n\nGOVEXPr = SiVGAir        i = TRAD_COMM; r = REG\n\nINCOMEr total expenditure equals net income in region r\n\nINCOMEr = PRIVEXPr + GOVEXPr + SAVEr        r = REG\n\nREGINVr gross investment in region r that equals value of output of “capital goods” sector\n\nREGINVr = SkVOAkr        k = CGDS_COMM\n\nNETINVr net investment in region r\n\nNETINVr = SkVOAkr - VDEPr        k = CGDS_COMM\n\nGLOBINV global net investment\n\nGLOBINV = SrNETINV = SrSAVEr        r = REG\n\nINVKERATIOr ratio of gross investment to end-of-period capital stock in region r\n\nINVKERATIOr = REGINVr / (VKBr + NETINVr)        r = REG\n\nGRNETRATIOr ratio of gross to net rate of return on capital in regin r (VOAcapital,r is gross return to capital)\n\nGRNETRATIOr = SkVOAkr / (SkVOAkr - VDEPr)        with k = ENDWC_COMM; r = REG\n\nGDPr gross domestic product in region r (trade is valued at worl prices)\n\nGDPr = SiVPAir + SiVGAir + SkVOAkr + SiSsVXWDirs+ SmVSTms - SiSsVIWSirs        i = TRAD_COMM; k = CGDS_COMM; r = REG; s = REG\n\nVT international margin suply\n\nVT = SiSrVSTir        i = TRAD_COMM; r = REG\n\nVXWir value of exports of tradable commodity i from source r evaluated at world (fob) prices\n\nVXWir = SsVXWDirs+VSTir        i = MARG_COMM; r = REG\nVXWir = SsVXWDirs        i = NMRG_COMM; r = REG\n\nVXWREGIONr value of exports from source r evaluated at world (fob) prices\n\nVXWREGIONr = SiVXWir        i = TRAD_COMM; r = REG\n\nVXWCOMMODi value of exports of tradable commodity i evaluated at world (fob) prices\n\nVXWCOMMODi= SrVXWir        i = TRAD_COMM; r = REG\n\nVIWir value of imports of tradable commodity i into region r evaluated at world (cif) prices\nVIWir= SsVIWSisr        i = TRAD_COMM; r = REG; s = REG\nVIWREGIONr value of imports into region r evaluated at world (cif) prices\nVIWREGIONr = SiVIWir        i = TRAD_COMM; r = REG\nVIWCOMMODi value of imports of tradable commodity i evaluated at world (cif) prices\nVIWCOMMODi= SrVIWir        i = TRAD_COMM; r = REG\nVXWLD value of worldwide commodity exports evaluated at world (fob) prices\nVXWLD= SrVXWREGIONr        r = REG\nPW_PMir ratio of world (fob) to domestic market prices for tradable commodity i in region\nPW_PMir = SsVXWDirs / SsVXMDirs        i = TRAD_COMM; r = REG; s = REG\nVOWir value of output of tradable commodity i in region r, evaluated at world (fob) prices\nVOWir = VDMir * PW_PMir + SsVXWDirs + VSTir        i = MARG_COMM; r = REG; s = REG\nVOWir = VDMir * PW_PMir + SsVXWDirs        i = NMRG_COMM; r = REG; s = REG\nVWOWi value of world supply of tradable commodity i evaluated at world (fob) prices\nVWOWi = SrVOWiri = TRAD_COMM; r = REG\nVTMFSDmirs int’l margin usage, by margin, freight, source, and destination m = MARG_COMM;i = TRAD_COMM; r = REG; s = REG\nVTFSDirs aggregate value of svces in the shipment of i from r to s\nVTFSDirs = SmVTMFSDmirs       In a balanced data base VIWSirs = VXWDirs + VTFSDirs i = TRAD_COMM; m = MARG_COMM; r = REG; s = REG\nVTMPROVm international margin services provision\nVTMPROVm = SrVSTmr        m = MARG_COMM; r = REG\nVTRPROVr international margin supply, by region\nVTRPROVr = SmVSTmr        m = MARG_COMM; r = REG\nVTMUSEm international margin services usage, by type\nVTMUSEm = SiSrSsVTMFSDmirs        m = MARG_COMM; i = TRAD_COMM; r = s = REG       In a balanced data base, VTMPROV = VTMUSE\nVTMUSESHRmirs share of i,r,s usage in global demand for m\nVTMUSESHRmirs = VTFSDirs / VT        m = MARG_COMM; i = TRAD_COMM; r = s = REG if VTMUSEm &lt;&gt; 0.0 then VTMUSESHRmirs = VTMFSDmirs / VTMUSEm\nVTSUPPSHRmr share of region r in global supply of margin m\nVTSUPPSHRmr = VTRPROVr / VT        if VTMPROVm &lt;&gt; 0.0 then VTSUPPSHRmr = VSTmr / VTMPROVm m = MARG_COMM; r = REG\nVTUSE international margin services usage;\nVTUSE=SmSiSrSsVTMFSDmirs        m = MARG_COMM; i = TRAD_COMM; r = s = REG\nVWOUi value of world output of i at user prices\nVWOUi = Ss[(VPAis + VGAis) + SjVFAijs] s = REG; j = PROD_COMM\nVENDWREGr value of primary factors, at mkt prices, by region\nVENDWREGr = Si VOMir        i = ENDOW_COMM; r = REG\nVENDWWLD value of primary factors, at mkt prices, worldwide\nVENDWWLD = Sr VENDWREGr r = REG\nVPAEVir private household expend. on i in r valued at agent’s prices, for EV calculation\nVPAEVir = qpevir        i = TRAD_COMM, r = REG\nVPAREGEVr private consumption expenditure in region r, for EV calculation\nVPAREGEVr = SiVPAEVir        i = TRAD_COMM, r = REG YGr regional government consumption expenditure, in region r        r = REG\n\n\n\nSHRDFMijr share of domestic sales of tradable commodity i used by firms in sector j of regin r evaluated at market prices\nSHRFMijr = VDFMijr / VDMir        i = TRAD_COMM; j = PROD_COMM; r = REG\nSHRDPMi share of domestic sales of commodity i used by private household in region r evaluated at market prices\nSHRDPMir = VDPMir / VDMir        i = TRAD_COMM; r = REG\nSHRDGMi share of domestic sales of commodity i used by the government in region r evaluated at market prices\nSHRDGMir = VDGMir / VDMir        i = TRAD_COMM; r = REG\nSHRIFMijr share of aggregate imports of tradable commodity i used by firms in sector j of regin r evaluated at market prices\nSHRIFMijr = VIFMijr / VIMir        i = TRAD_COMM; j = PROD_COMM; r = REG\nSHRIPMir share of aggregate imports of commodity i used by private household in region r evaluated at market prices\nSHRIPMir = VIPMir / VIMir        i = TRAD_COMM; r = REG\nSHRIGMir share of aggregate imports of commodity i used by the government in region r evaluated at market prices\nSHRIGMir = VIGMir / VIMir        i = TRAD_COMM; r = REG\nFMSHRijr share of imports in the composite for tradable commodity i used by firms in sector j of region r evaluated at agents’ prices\nFMSHRijr = VIFAijr / VFAijr        i = TRAD_COMM; j = PROD_COMM; r = REG\nPMSHRir share of imports in the composite for tradable commodity i used by private household in region r evaluated at agents’ prices\nPMSHRir = VIPAir / VPAir        i = TRAD_COMM; r = REG\nGMSHRir share of impotrs in the composite for tradable commodity i used by the government in region r evaluated at agents’ prices\nGMSHRir = VIGAir / VGAir        i = TRAD_COMM; r = REG\nCONSHRir budget share of the composite for tradable i in total private household expenditure in regin r evaluated at agents’ prices\nCONSHRir = VPAir / PRIVEXPr        i = TRAD_COMM; r = REG\nMSHRSirs market share of source r in the aggregate imports of tradable commodity i in region s evaluated at market prices\nMSHRSirs = VIMSirs / SrVIMSirs        i = TRAD_COMM; r = s = REG\nSVAijr share of endowment commodity i in value-added of sector j of region r evaluated at agents’ prices\nSVAijr= VFAijr / SkVFAkjr        i = ENDW_COMM; j = PROD_COMM; k = ENDW_COMM; r = REG\nREVSHRijr share of endowment commodity i used by fiorms in sector j of regin r evaluated at market prices\nREVSHRijr= VFMijr / SkVFMkjr        i = ENDW_COMM; j = PROD_COMM; k = PROD_COMM; r = REG\nFOBSHRirs share of fob price in the cif price for tradable commodity i exported from source r to destination s\nFOBSHRirs= VXWDirs / VIWSCOSTirs        i = TRAD_COMM; r = s = REG\nXSHRPRIVr private expenditure share in regional income\nXSHRPRIVr = PRIVEXPr/INCOMEr        r = REG\nXSHRGOVr government expenditure share in regional income\nXSHRGOVr = GOVEXPr/INCOMEr        r = REG\nXSHRSAVEr saving share in regional income\nXSHRSAVEr = SAVEr/INCOMEr        r = REG\nSHRDMir share of domestic sales of i in r        i = TRAD_COMM; r = REG\nSHRDMir = VDMir / VOMir\nSHRSTmr share of sales of m to global transport services in r        m = MARG_COMM; r = REG\nSHRSTmr = VSTmr / VOMmr\nSHRXMDirs share of export sales of i to s in r        i = TRAD_COMM; r = s = REG\nSHRXMDirs = VXMDirs / VOMir        i = TRAD_COMM; r = s = REG\nSHREMijr share of mobile endowments i used by sector j at mkt prices\nSHREMijr = VFMijr / VOMir        i = ENDWM_COMM; j = PROD_COMM; r = REG\nTRNSHRirs share of transport price in the cif price for tradable commodity i exported from source r to destination s\nTRSHRirs= VTFSDirs / VIWSCOSTirs i = TRAD_COMM; r = s = REG\nXWCONSHRir expansion-parameter-weighted consumption share\nXWCONSHRir = CONSHRir*INCPARir/UELASPRIVr        i = TRAD_COMM; r = REG\nXSHRPRIVEVr private expenditure share in regional income, for EV calculation        r = REG\nXSHRPRIVEVr = PRIVEXPEVr/INCOMEEVr        r = REG\nXSHRGOVEVr government expenditure share in regional income, for EV calculation        r = REG\nXSHRGOVEVr = GOVEXPEVr/INCOMEEVr        r = REG\nXSHRSAVEEVr saving share in regional income, for EV calculation        r = REG\nXSHRSAVEEVr = SAVEEVr/INCOMEEVr        r = REG\nXWCONSHREVir expansion-parameter-weighted consumption share, for EV calculation\nXWCONSHREVir = CONSHREVir*INCPARir/UELASPRIVEVr        i = TRAD_COMM, r = REG\nCONSHREVir share of private household consn devoted to good i in r, for EV calculation\nCONSHREVir = VPAEVir/VPAREGEVr\nSTCijr share of i in total costs of j in r\nSTCijr = VFAijr / Sk VFAkjr        i = k = DEMD_COMM; j = PROD_COMM; r = REG\n\n\n\nQOir quantity of non-saving commodity i output or supplied in region r | i = NSAV_COMM r = REG\nQOESijr quantity of sluggish endowment i supplied to sector j firm of region r | i = ENDWS_COMM; j = PROD_COMM; r = REG\nQDSir quantity of domestic sales of tradable commodity i in region r | i = TRAD_COMM; r = REG\nQXSirs quantity of exports of tradable commodity i from source r to destination s | i = TRAD_COMM; r = s = REG\nQSTir quantity of sales of marginal commodity i to the international transport sector in region r | i = MARG_COMM; r = REG\nQFEijr quantity of endowment i demanded by sector j firm in region r | i = ENDW_COMM; j = PROD_COMM; r = REG\nQVAjr quantity index of land-labor-capital composite (value-added) in sector j firm in region r | j = PROD_COMM; r = REG\nQFijr quantity of composite tradable commodity i demanded by sector j firm in region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nQFDijr quantity of domestic tradable i demanded by sector j firm in region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nQFMijr quantity of imported tradable i demanded by sector j firm in region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nQPir quantity of composite tradable i demanded by private household in region r | i = TRAD_COMM; r = REG\nQPDir quantity of domestic tradable i demanded by private household in region r | i = TRAD_COMM; r = REG\nQPMir quantity of imported tradable i demanded by private household in region r | i = TRAD_COMM; r = REG\nQGir quantity of composite tradable commodity i demanded by government household in region r | i = TRAD_COMM; r = REG\nQGDir quantity of domestic tradable commodity i demanded by government household in region r | i = TRAD_COMM; r = REG\nQGMir quantity of imported tradable commodity i demanded by government household in region r | i = TRAD_COMM; r = REG\nQIMir quantity of aggregate imports of tradable commodity i demanded by region r using market prices as weights | i = TRAD_COMM; r = REG\nQIWir quantity of aggregate imports of tradable commodity i demanded by region r using cif prices as weights | i = TRAD_COMM; r = REG\nQXWir quantity of aggregate exports of tradable commodity i supplied from region r using fob prices as weights | i = TRAD_COMM; r = REG\nQIWREGr volume of merchandise imports demanded by region r | r = REG\nQXWREGr volume of merchandise exports supplied by region r | r = REG\nQIWCOMi volume of global merchandise imports of tradable commodity i | i = TRAD_COMM; r = REG\nQXWCOMi volume of global merchandise exports of tradable commodity i | i = TRAD_COMM; r = REG\nQXWWLD volume of world trade\nQOWi quantity index for world supply of tradable commodity i | i = TRAD_COMM\nEXPANDir Change in investment levels relative to endowment stock | i = ENDWC_COMM; r = REG\nQOWUirquantity index for world supply of good i at user prices i = TRAD_COMM; r = REG\nQTMFSDmirs international usage margin m on i from r to s m = MARG_COMM; i = TRAD_COMM; r = s = REG\nQTMm global margin usage m = MARG_COMM\nQCGDSr quantity of capital goods sector supplied in region r r = REG\nQSAVEr quantity of savings demanded in region r r = REG\nGLOBALCGDS quantity of global supply of capital for net investment\nKSVCESr quantity of capital services in region r r = REG\nKBr quantity of beginning-of-period capital stock in region r r = REG\nKEr quantity of end-of-period capital stock in region r r = REG\nQGDPr quantity index for GDP in region r r = REG\nWALRAS_DEM quantity demanded in the omitted market - equals global demand for savings\nWALRAS_SUP quantity supplied in the omitted market - equals global supply of new capital goods composite\nPOPr population in region r r = REG\nCOMPVALADir composition of value added for good i and region r i = TRAD_COMM; r = REG\n\n\n\nPMir market price of non-saving commodity i in region r i = NSAV_COMM; r = REG\nPMESijr market price for sluggish endowment i supplied to firm j in region r i = ENDWS_COMM; j = PROD_COMM; r = REG\nPSir supply price of non-saving commodity i in region r i = NSAV_COMM; r = REG\nPFEijr demand price for endowment i by firms in sector j of region r i = ENDW_COMM; j = PROD_COMM; r = REG\nPVAjr price of value-added in sector j j = PROD_COMM; r = REG\nPFijr demand price for composite tradable i by firms in sector j of region r i = TRAD_COMM; j = PROD_COMM; r = REG\nPFDijr demand price for domestic tradable i by firms in sector j of region r i = TRAD_COMM; j = PROD_COMM; r = REG\nPFMijr demand price for imported tradable i by firms in sector j of region r i = TRAD_COMM; j = PROD_COMM; r = REG\nPPir private household’s demand price for composite tradable i in region r i = TRAD_COMM; r = REG\nPPDir private household’s demand price for domestic tradable i in region r i = TRAD_COMM; r = REG\nPPMir private household’s demand price for imported tradable i in region r i = TRAD_COMM; r = REG\nPGir government household’s demand price for composite tradable i in region r i = TRAD_COMM; r = REG\nPGDir government household’s demand price for domestic tradable i in region r i = TRAD_COMM; r = REG\nPGMir government household’s demand price for imported tradable i in region r i = TRAD_COMM; r = REG\nPSAVEr price of savings in region r r = REG\nPCGDSr price of investment good in region r - equals PS(cgds,r) r = REG\nPGDPr price index for GDP in region r r = REG\nPPRIVr price index for private household expenditure in region r r = REG\nPGOVr price index for government household expenditure in region r r = REG\nPFOBirs world fob price of tradable commodity i exported from source r to destination s (prior to including transport margins) i = TRAD_COMM; r = s = REG\nPCIFirs world cif price of tradable commodity i imported from source r to destination s ) after including transport margins) i = TRAD_COMM; r = s = REG\nPMSirs market price by source of tradable commodity i imported from source r to destination s i = TRAD_COMM; r = s = REG\nPIMir market price of aggregate imports of tradable commodity i in region r i = TRAD_COMM; r = REG\nPIWir world price of aggregate imports of tradable commodity i in region r i = TRAD_COMM; r = REG\nPXWir price index for aggregate exports of tradable commodity i from region r i = TRAD_COMM; r = REG\nPIWREGr price index of merchandise imports in region r r = REG\nPXWREGr price index of merchandise exports from region r r = REG\nPIWCOMi price index of global merchandise imports of tradable commodity i i = TRAD_COMM\nPXWCOMi price index of global merchandise exports of tradable commodity i i = TRAD_COMM\nPXWWLD price index of world trade\nPRir ratio of domestic market price to market price of imports for tradable commodity i in region r i = TRAD_COMM; r = REG\nPWi world price index for total supply of tradable commodity i in region r i = TRAD_COMM\nPSWr price index received for tradable commodities produced in region r including sales of net investment to the global bank r = REG\nPDWr price index paid for tradable commodities used in region r including purchases of savings from the global bank r = REG\nTOTr terms of trade for region r r = REG\nPTm price of margin services supplied m = MARG_COMM\nPCGDSWLD world average price of capital goods (net investment weights)\nPWUi world price index for total good i supplies at user prices i = TRAD_COMM\nPTRANSirs cost index for international transport of i from r to s i = TRAD_COMM; r = s = REG\nRENTALr rental rate on capital stock in regin r - equals PS(CGDS,r) r = REG\nRORCr current net rate of return on capital stock in region r r = REG\nROREr expected net rate of return on capital stock in region r r = REG\nRORG global net rate of return on capital stock\nPr price index for disposition of income by regional household\nPFACTREALir ratio of return to primary factor i to cpi in r i = ENDOW_COMM; r = REG\nPFACTORr market price index of primary factors, by region\nPFACTWLDworld price index of primary factors\n\n\n\nAOir output augmenting technical change in sector j of region r | j = PROD_COMM; r = REG\nAFEijr primary factor i augmenting technical change in sector j of region r | i = ENDW_COMM; j = PROD_COMM; r = REG\nAFijr intermediate input i augmenting technical change in sector j of region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nAVAjr value-added augmenting technical change in sector j of region r | j = PROD_COMM; r = REG\nAMSirs import i from region r augmenting tech change in region s | i = TRAD_COMM, r = s = REG\nAOSECj output tech change of sector j, worldwide | j = PROD_COMM\nAOREGr output tech change in region r | r = REG\nAOALLjr output augmenting technical change in sector j of r | j = PROD_COMM; r = REG\nESUBTj elasticity of substitution among composite intermediate inputs in production | j = PROD_COMM\nAFCOMi intermediate tech change of input i, worldwide | i = TRAD_COMM\nAFSECj intermediate tech change of sector j, worldwide | j = PROD_COMM\nAFREGr intermediate tech change in region r | r = REG\nAFALLijr intermediate input i augmenting tech change by j in r | i = TRAD_COMM,j = PROD_COMM; r = REG\nAFECOMi factor input tech change of input i, worldwide | i = ENDW_COMM\nAFESECj factor input tech change of sector j, worldwide | j = PROD_COMM\nAFEREGr factor input tech change in region r | r = REG\nAFEALLijr primary factor i augmenting tech change sector j in r | i = ENDW_COMM; j = PROD_COMM; r = REG\nAMSirs reduction in effective price associated with delivery of i from r to s | i = TRAD_COMM; r = REG; s = REG\nATMFSDmirs tech change in m’s shipping of i from region r to s | m = MARG_COMM; i = TRAD_COMM; r = s = REG\nATMm tech change in mode m, worldwide; | m = TRAD_COMM\nATFi tech change shipping of i, worldwide | i = TRAD_COMM\nATSr tech change shipping from region r | r = REG\nATDs tech change shipping to s | s = REG\nATALLmirs tech change in m’s shipping of i from region r to s | m = MARG_COMM; i = TRAD_COMM; r = REG; s = REG\nAUr input-neutral shift in utility function | r = REG\n\n\n\nTOir power of the tax on output (or income) of non-savings commodity i in region r | i = NSAV_COMM; r = REG\nTFijr power of the tax on endowment commodity i demanded by sector j of regin r | i = ENDW_COMM; j = PROD_COMM; r = REG\nTFDijr power of the tax on domestic tradable commodity i demanded by sector j of region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nTFMijr power of the tax on imported tradable commodity i demanded by sector j of region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nTPDir power of the tax on domestic tradable commodity i purchased by private household in region r | i = TRAD_COMM; r = REG\nTPMir power of the tax on imported tradable commodity i purchased by private household in region r | i = TRAD_COMM; r = REG\nTGDir power of the tax on domestic tradable commodity i purchased by government household in region r | i = TRAD_COMM; r = REG\nTGMir power of the tax on imported tradable commodity i purchased by government household in region r | i = TRAD_COMM; r = REG\nTXSirs power of the tax on exports of tradable commodity i from source r to destination r (levied in region r) | i = TRAD_COMM; r = s = REG\nTMSirs power of the tax on imports of tradable commodity i from source r to destination s (levied in region s) | i = TRAD_COMM; r = s = REG\nTXir power of the variable export tax on exports of tradable commodity from region r - destination generic | i = TRAD_COMM; r = REG\nTMir power of the variable import tax (levy) on imports of tradable commodity i in region s - source generic | i = TRAD_COMM; r = REG\nTPr region-wide shock to tax on purchases by private household in region r | r = REG\nATPMir actual tax on imported traded commodity i purchased by private households in region r | i = TRAD_COMM; r = REG\nATPDir actual tax on domestic traded commodity i purchased by private households in region r | i = TRAD_COMM; r = REG\nDGTAXir tax on government consumption of domestic good i in region r | r = REG\nIGTAXir tax on government consumption of imported good i in region r | r = REG\nTGCr government consumption tax payments in r\nTGCr = SiDGTAXir + IGTAXir i = TRAD_COMM; r = REG\nDPTAXir tax on private consumption of domestic good i in region r\nDPTAXir = VDPAir - VDPMir i = TRAD_COMM; r = REG\nIPTAXir tax on private consumption of imported good i in region r\nIPTAXir = VIPAir - VIPMir i = TRAD_COMM; r = REG\nTPCr private consumption tax payments in r\nTPCr = SiDPTAXir + IPTAXir i = TRAD_COMM; r = REG\nDFTAXijr tax on use of domestic intermediate good i by j in r\nDFTAXijr = VDFAijr - VDFMijr i = TRAD_COMM; j = PROD_COMM; r = REG\nIFTAXijr tax on use of imported intermediate good i by j in r\nIFTAXijr = VIFAijr - VIFMijr i = TRAD_COMM; j = PROD_COMM; r = REG\nTIUr firms’ tax payments on intermediate goods usage in r\nTIUr = SjSi(DFTAXijr + IFTAXijr) i = TRAD_COMM; j = PROD_COMM; r = REG\nETAXijr tax on use of endowment good i by industry j in region r\nETAXijr = VFAijr - VFMijr i = ENDW_COMM; j = PROD_COMM; r = REG\nTFUr firms’ tax payments on primary factor usage in r\nTFUr = SiSjETAXijr i = ENDW_COMM; j = PROD_COMM; r = REG\nPTAXir output tax on good i in region r\nPTAXir = VOMir - VOAir i = NSAV_COMM; r = REG\nTOUTr production tax payments in r\nTOUTr = SiPTAXir i = PROD_COMM; r = REG\nXTAXDirs tax on exports of good i from source r to destination s\nXTAXDirs = VXWDirs - VXMDirs i = TRAD_COMM; r = s = REG\nTEXr export tax payments in r\nTEXr = SiSsXTAXDirs i = TRAD_COMM; r = s = REG\nMTAXirs tax on imports of good i from source r in destination s\nMTAXirs = VIMSirs - VIWSirs i = TRAD_COMM; r = s = REG\nTIMr import tax payments in r\nTIMr = SiSsMTAXisr i = TRAD_COMM; r = s = REG\nTOTTAXr Total tax receipts in r\nTOTTAXr = TPCr + TGCr + TIUr + TFUr + TOUTr + TEXr + TIMr + TINCr r = REG\nTGCRr change in ratio of government consumption tax to INCOME in region r r = REG\nTPCRr change in ratio of private consumption tax to INCOME in region r r = REG\nTIURr change in ratio of tax on intermediate usage to INCOME in region r r = REG\nTFURr change in ratio of tax on primary factor usage to INCOME in region r r = REG\nTOUTRr change in ratio of output tax to INCOME in region r nbsp; r = REG\nTEXPRr change in ratio of export tax to INCOME in region r r = REG\nTIMPRr change in ratio of import tax to INCOME in region r r = REG\nTINCr income tax payments in r\nTINCr = SiPTAXir i = ENDOW_COMM; r = REG\nTINCRr change in ratio of income tax to INCOME in region r r = REG\nDTAXRr change in ratio of taxes to INCOME in r r = REG\n\n\n\nDPARPRIVr private consumption distribution parameter\nDPARPRIVr = UELASPRIVr*XSHRPRIVr/UTILELASr r = REG\nDPARGOVr government consumption distribution parameter\nDPARGOVr = XSHRGOVr/UTILELASr r = REG\nDPARSAVEr saving distribution parameter\nDPARSAVEr = XSHRSAVEr/UTILELASr r = REG\nDPARSUMr sum of distribution parameters r = REG\ndpavr change in average distribution parameter r = REG\ndpsumr change in sum of the distribution parameters r = REG\ndpprivr change in private consumption distribution parameter r = REG\ndpgovr change in government consumption distribution parameter r = REG\ndpsaver change in saving distribution parameter r = REG\n\n\n\nprofitslackjc slack variable in the ZEROPROFITS equation (this is exogenous as long as output level, QO(j), is determined endogenously) | j = PROD_COMM; r = REG\ncgdslackr slack variable in the CAPGOODS equation (this is exogenous as long as output level of new capital goods, QO(“cgds”), is determined endogenously | r = REG\nendwslackir slack variable in the MKTCLENDWM and ENDW_SUPPLY equations (this is exogenous as long as primary factor rental rates, PMi and PMESij are determined endogenously) | i = ENDWS_COMM; r = REG\ntradslackir slack variable in the MKTCLTRD equation (this is exogenous as long as market price of tradable, PMi, is determined endogenously) | i = TRAD_COMM; r = REG\nincomeslackr slack variable in the INCOME equation (this is exogenous as long as income, Y, is determined endogenously) | r = REG\npsaveslackr slack variable in the SAVINGS equation (this is exogenous as long as level of savings, QSAVE, is determined endogenously) | r = REG\nwalraslack slack variable in the WALRAS equation (this is endogenous as long as price of savings, PSAVE, is determined exogenously which is the case in a standard GE closure. When any one of the GE links is broken, this is swapped with PSAVE, the numeraire price, thereby forcing global savings to equal global investment).\nincomeslackr slack variable in the expression for regional income | r = REG\n\n\n\nvxwfobir percentage change in value of exports of tradable commodity i from source region r using fob weights (is identical to the linearized form of VXWir) | i = TRAD_COMM; r = REG\nvxwregr percentage change in value of merchandise exports from region r using fob weights (is identical to the linearized form of VXWREGIONr) | r = REG\nvxwcomi percentage change in value of global merchandise exports of tradable commodity i using fob weights (is identical to the linearized form of VXWCOMMODi) | i = TRAD_COMM\nviwcifir percentage change in value of imports of tradable commodity i into region r using fob weights (is identical to the linearized form of VIWir) | i = TRAD_COMM; r = REG\nviwregr percentage change in value of merchandise imports into region r using cif weights (is identical to the linearized form of VIWREGIONir) | r = REG\nviwcomi percentage change in value of global merchandise imports of tradable commodity i using fob weights (is identical to the linearized form of VIWCOMMODi) | i = TRAD_COMM\nvxwwld percentage change in value of worlwide commodity exports using fob weights (is identical to the linearized form of VXWLD)\nvaluewi percentage change in value of global supply of tradable commodity i using fob weights (is identical to the linearized form of VWOWi) | i = TRAD_COMM\nvaluewui value of world supply of good i at user prices | i = TRAD_COMM\nvgdpr percentage change in value of GDP in region r (is identical to the linearized form of GDPr) | r = REG\nyr percentage change in regional household income (is identical to the linearized form of INCOME) | r = REG\nypr percentage change in private household expenditure (is identical to the linearized form of PRIVEXP). | r = REG\nFYr primary factor income in r net of depreciation\nFYr = SiVOMir - VDEPr | i = ENDW_COMM; r = REG\nfincomer pc change in factor income variable in r net of depreciation r = REG\n\n\n\nUr per capita utility from aggregate household expenditure in region r r = REG\nUPr per capita utility from private household expenditure in region r r = REG\nUGr aggregate utility from government household expenditure in region r r = REG\nUTILELASr elasticity of cost of utility wrt utility\nUTILELASr = (UELASPRIVr*XSHRPRIVr + XSHRGOVr + XSHRSAVEr)/DPARSUMr r = REG\nuelasr pc change in elasticity of cost of utility wrt utility r = REG\nueprivr pc change in utility elasticity of private cons expenditure r = REG\nuelasevr pc change in elasticity of cost of utility wrt utility, for EV calculation r = REG\nueprivevr pc change in utility elasticity of private cons expenditure, for EV calculation r = REG\n\n\n\nEVr equivalent variation, in $ US million (positive figure indicates welfare improvement) r = REG\nWEV equivalent variation, in $ US million, for the world (positive figure indicates welfare improvement)\nugevr per capita utility from gov’t expend., for EV calculation r = REG\nupevr per capita utility from private expend., for EV calculation r = REG\nqsaveevr total quantity of savings demanded, for EV calculation r = REG\nypevr private consumption expenditure, in region r, for EV calculation r = REG\nygevr government consumption expenditure, in region r, for EV calculation r = REG\nqpevir private household demand for commodity i in region r, for EV calculation i = TRAD_COMM, r = REG\nyevr regional household income, in region r, for EV calculation\nysaveevr NET savings expenditure, for EV calculation\ndpavevr average distribution parameter shift, for EV calculation r = REG\n\n\n\nDTBALr change in trade balance of region r , in US$ million (positive figure indicates increases in exports exceeds increases in imports) | r = REG\nDTBALiir change in trade balance for tradable commodity i in region r , in US$ million (positive figure indicates increases in exports exceeds increases in imports) | i = TRAD_COMM; r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#sets",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#sets",
    "title": "Variables in v11",
    "section": "",
    "text": "Variable Label\nVariable Name\n\n\n\n\nNSAV_COMM\nNon-Savings Commodities (NS)\n\n\nTRAD_COMM\nTraded Commodities (TC)\n\n\nDEMD_COMM\nDemanded Commodities (DC)\n\n\nPROD_COMM\nProduced Commodities (PC)\n\n\nENDW_COMM\nEndowment Commodities (EC)\n\n\nENDWS_COMM\nSluggish Endowment Commodities (ECS)\n\n\nENDWM_COMM\nMobile Endowment Commodities (ECM)\n\n\nCGDS_COMM\nCapital Goods Commodities (“cgds”)\n\n\nENDWC_COMM\nCapital Endowment Commodity (“capital”)",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#subsets",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#subsets",
    "title": "Variables in v11",
    "section": "",
    "text": "Variable Label\nVariable Name\n\n\n\n\nPROD_COMM\nin NSAV_COMM\n\n\nDEMD_COMM\nin NSAV_COMM\n\n\nCGDS_COMM\nin NSAV_COMM\n\n\nENDW_COMM\nin DEMD_COMM\n\n\nTRAD_COMM\nin DEMD_COMM\n\n\nTRAD_COMM\nin PROD_COMM\n\n\nCGDS_COMM\nin PROD_COMM\n\n\nENDWS_COMM\nin ENDW_COMM\n\n\nENDWM_COMM\nin ENDW_COMM\n\n\nENDWC_COMM\nin NSAV_COMM\n\n\nENDWC_COMM\nin ENDW_COMM",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#example-the-3x3-3-regions-x-3-sectors-economy",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#example-the-3x3-3-regions-x-3-sectors-economy",
    "title": "Variables in v11",
    "section": "",
    "text": "Variable Label\nVariable Name\n\n\n\n\nREG\n= {usa, eu, row}\n\n\nNSAV_COMM\n= {land, labor, capital, food, manufacturing, services, capital goods}\n\n\nTRAD_COMM\n= {food, manufacturing, services}\n\n\nDEMD_COMM\n= {land, labor, capital, food, manufacturing, services}\n\n\nPROD_COMM\n= {food, manufacturing, services, capital goods}\n\n\nENDW_COMM\n= {land, labor, capital}\n\n\nENDWS_COMM\n= {land, capital}\n\n\nENDWM_COMM\n= {labor}\n\n\nCGDS_COMM\n= {capital goods}\n\n\nENDWC_COMM\n= {capital}",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows-evaluated-at-agents-prices",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows-evaluated-at-agents-prices",
    "title": "Variables in v11",
    "section": "",
    "text": "EVOAir value of endowment commodity i output or supplied, in region r, evaluated at agent’s prices\n\nEVOAi,r = PSi,r * QOi,r with i = ENDW_COMM and r\n\nEVFAijr value of purchases of demanded commodity i by firms in sector j, in region r, evaluated at agent’s prices\n\nEVFAijr = PFEijr * QFEijr        i = TRAD_COMM; j = ENDW_COMM; r = REG\n\nVDFAijr value of purchases of domestic tradeable commodity i by firms in sector j of region r evaluated at agents’ prices\n\nVDFAijr = PFDijr * QFDijr        i = TRAD_COMM; j = ENDW_COMM; r = REG\n\nVIFAijr value of purchases of imported tradeable commodity i by firms in sector j of region r evaluated at agents’ prices\n\nVIFAijr = PFMijr * QFMijr        i = TRAD_COMM; j = ENDW_COMM; r = REG\n\nVDPAir value of expenditure on domestic tradable commodity i, by private household in region r evaluated at agent’s prices\n\nVDPAir = PPDir * QPDir        i = TRAD_COMM; r = REG\n\nVIPAir value of expenditure on imported tradeable commodity i by firms in sector j of region r evaluated at agents’ prices\n\nVIPAir = PPMir * QPMir        i = TRAD_COMM; r = REG\n\nVDGAi value of government’s expenditure on domestic tradable commodity i, in region r, evaluated at agent’s prices\n\nVDGAir = PGDir * QGDir        i = TRAD_COMM; r = REG\n\nVIGAi value of government’s expenditure on imported tradable commodity i, in region r, evaluated at agent’s prices\n\nVIGAir = PGMir * QGMir        i = TRAD_COMM; r = REG\n\nSAVEr value of (net) savings in region r\n\nSAVEr = PSAVEr * QSAVEr        r = REG\n\nVKBr value of beginning-of-period capital stock in region r\n\nVKBr = PCGDSr * KBr        r = REG\n\nVDEPr value of capital depreciation expenditure in region r\n\nVDEPr = PCGDSr * KBr        r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows-evaluated-at-market-prices",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows-evaluated-at-market-prices",
    "title": "Variables in v11",
    "section": "",
    "text": "VFMijr value of purchases of endowment commodity i by firms in sector j in region r, evaluated at market prices\n\nVFMijr = PMir * QFEijr        i = ENDWM_COMM; j = PROD_COMM; r = REG\nVFMijr = PMESijr * QFEijr        i = ENDWS_COMM; j = PROD_COMM; r = REG\n\nVDFMijr value of purchases of domestic tradable commodity i by firms in sector j in region r, evaluated at market prices\n\nVDFMijr = PMir * QFDijr        i = TRAD_COMM; j = PROD_COMM\n\nVIFMijr value of purchases of imported tradable commodity i by firms in sector j in region r, evaluated at market prices\n\nVIFMijr = PIMir * QFMijr        i = TRAD_COMM; j = PROD_COMM; r = REG\n\nVDPMir value of private household’s purchases of domestic tradable commodity i in region r, evaluated at market prices\n\nVDPMir = PMir * QPDir        i = TRAD_COMM; r = REG\n\nVIPMir value of private household’s purchases of imported tradable commodity i in region r, evaluated at market prices\n\nVIPMir = PIMir * QPMir        i = TRAD_COMM; r = REG\n\nVDGMir value of government’s expenditure on domestic tradable commodity i, in region r, evaluated at market prices\n\nVDGMir = PMir * QGDir        i = TRAD_COMM; r = REG\n\nVIGMir value of government’s expenditure on imported tradable commodity i, in region r, evaluated at market prices\n\nVIGMir = PIMir * QGMir        i = TRAD_COMM; r = REG\n\nVXMDirs value of exports of tradable commodity i from source r to destination s, evaluated at (exporter’s) market prices\n\nVXMDirs = PMir * QXSirs        i = TRAD_COMM; r = REG; s= REG\n\nVIMSirs value of imports of tradable commodity i from source r to destination s, evaluated at (importer’s) market prices\n\nVIMSirs = PMSir * QXSirs        i = TRAD_COMM; r = REG; s = REG\n\nVSTir value of sales of tradable commodity i to the international transport sector in region r, evaluated at market prices\n\nVSTir = PMir * QSTir        i = MARG_COMM; r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows-evaluated-at-world-prices",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows-evaluated-at-world-prices",
    "title": "Variables in v11",
    "section": "",
    "text": "VXWDirs value of exports of tradable commodity i from source r to destination s, evaluated at world (fob) prices\n\nVXWDirs = PFOBirs * QXSirs        i = TRAD_COMM; r = REG; s = REG\n\nVIWSirs value of imports of tradable commodity i from source r to destination s, evaluated at world (cif) prices\n\nVIWSirs = PCIFirs * QXSirs        i = TRAD_COMM; r = REG; s = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#technology-parameters",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#technology-parameters",
    "title": "Variables in v11",
    "section": "",
    "text": "ESUBVAj with j = PROD_COMM substitution parameter between primary factors in the CES value-added nest of the nested CES production function of sector j of all regions (i.e. elasticity of substitution between primary production factors; for example, in a situation where land doesn’t play a role in production, hence labor and capital are the only primary factors, this parameter indicates how sensitive is the K/L ratio to a 1 percent change in the wage/rental ratio)\nESUBDi    with i = TRAD_COMM substitution parameter between domestic and composite imported commodities in the Armington utility/production structure of agent/sector i in all regions\nESUBMi    with i = TRAD_COMM substitution parameter among imported commodities from different sources in the Armington utility/production structure of agent/sector i in all regions.",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#cde-preference-parameters-and-associated-elasticities",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#cde-preference-parameters-and-associated-elasticities",
    "title": "Variables in v11",
    "section": "",
    "text": "SUBPARir    with i = TRAD_COMM; r = REG substitution parameter for tradable commodity i in the CDE minimum expenditure function of region r\nINCPARir    with i = TRAD_COMM; r = REG expansion parameter for tradable commodity i in the CDE minimum expenditure function of region r.\nALPHAir 1 - sub. parameter in the CDE minimum expenditure function\n\nALPHAir = 1 - SUBPARir        i = TRAD_COMM; r = REG\n\nAPEikr Allen partial elst.of sub between composite i and k in r\n\nAPEikr = ALPHAir + ALPHAkr- SnCONSHRnr*ALPHAnr        i = k= n = TRAD_COMM; r = REG\n\nAPEiir = 2.0*ALPHAir - SnCONSHRnr*ALPHAnr - ALPHAir / CONSHRir\nEYir income elasticity of private household demand for i in r\n\nEYir = (1/Sn CONSHRnr*INCPARnr) * (INCPARir * (1.0 - ALPHAir) + SnCONSHRnr*INCPARnr*ALPHAnr) + (ALPHAir - SnCONSHRnr*ALPHAnr) i = n = TRAD_COMM; r = REG\n\nEPikr uncompensated cross-price elasticity private household demand for i wrt k in r\n\nEPikr = (APEikr - EYir)*CONSHRkr        i = k = TRAD_COMM; r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#mobility-parameter",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#mobility-parameter",
    "title": "Variables in v11",
    "section": "",
    "text": "ETRAEi    with i = ENDW_COMM transformation parameter between uses for a sluggish primary factor i in the one level CET production function (i.e. this parameter indicates how easy/difficult it is to transfer a sluggish factor (say, capital) from one sector to another; the closer is ETRAE to zero, the more immobile is the factor between alternative uses)\nRORFLEXr    with r = REG flexibility of expected net rate of return on capital stock in region r with respect to investment (if a region’s capital stock increases by 1%, then it is expected that the net rate of return on capital will decline by RORFLEX%)\nRORDELTA binary coefficient that determines the mechanism of allocating investment across regions (when RORDELTA=1, investment is allocated across regions to equate the change in the expected rates of return, rore(r), when RORDELTA=0, investment is allocated across regions to maintain the existing composition of capital stocks).",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-flows",
    "title": "Variables in v11",
    "section": "",
    "text": "VOAir value of non-saving commodity i output or supplied in region r evaluated at agents’ prices\n\nVOAir = EVOAir        i = ENDW_COMM; r = REG\nVOAir = SjVFAjir        i = PROD_COMM; j = DEMD_COMM; r = REG\n\nVFAijr value of purchases of demanded commodity i by firms in sector j of regin r evaluated at agents’ prices\n\nVFAijr = EVFAijr        i = ENDW_COMM; j = PROD_COMM; r = REG\nVFAijr = VDFAijr + VIFAijr        i =TRAD_COMM; j = PROD_COMM; r = REG\n\nVOMir value of non-saving commodity i output or supplied in region r evaluated at market prices\n\nVOMir = SjVFMijr        i = ENDW_COMM;j = PROD_COMM; r = REG\nVOMir = VDMir + VSTir + SsVXMDirs        i = MARG_COMM; r = REG; s = REG\nVOMir = VDMir + SsVXMDirs        i = NMRG_COMM; r = REG; s = REG\nVOMir = VOAir        i = CGDS_COMM; r = REG\n\nVDMir value of domestic sales of tradable commodity i in region r evaluated at market prices\n\nVDMir = VDPMir + VDGMir + SjVDFMijr        i = TRAD_COMM; j = PROD_COMM; r = REG\n\nVIMir value of aggregate imports of tradable commodity i in region r evaluated at market prices\n\nVIMir = VIPMir + VIGMir + SjVIFMijr        i = TRAD_COMM; j = PROD_COMM; r = REG\n\nVPAir value of private household expenditure on tradable commodity i in region r evaluated at agents’ prices\n\nVPAir = VDPAir + VIPAir        i = TRAD_COMM; r = REG\n\nPRIVEXPr total consumption expenditure by private household in regin r\n\nPRIVEXPr = SiVPAir        i = TRAD_COMM\n\nVGAir value of government expenditure on tradable commodity i in region r evaluated at agents’ prices\n\nVGAir = VDGAir + VIGAir        i = TRAD_COMM; r = REG\n\nGOVEXPr total consumption expenditure by government in region r\n\nGOVEXPr = SiVGAir        i = TRAD_COMM; r = REG\n\nINCOMEr total expenditure equals net income in region r\n\nINCOMEr = PRIVEXPr + GOVEXPr + SAVEr        r = REG\n\nREGINVr gross investment in region r that equals value of output of “capital goods” sector\n\nREGINVr = SkVOAkr        k = CGDS_COMM\n\nNETINVr net investment in region r\n\nNETINVr = SkVOAkr - VDEPr        k = CGDS_COMM\n\nGLOBINV global net investment\n\nGLOBINV = SrNETINV = SrSAVEr        r = REG\n\nINVKERATIOr ratio of gross investment to end-of-period capital stock in region r\n\nINVKERATIOr = REGINVr / (VKBr + NETINVr)        r = REG\n\nGRNETRATIOr ratio of gross to net rate of return on capital in regin r (VOAcapital,r is gross return to capital)\n\nGRNETRATIOr = SkVOAkr / (SkVOAkr - VDEPr)        with k = ENDWC_COMM; r = REG\n\nGDPr gross domestic product in region r (trade is valued at worl prices)\n\nGDPr = SiVPAir + SiVGAir + SkVOAkr + SiSsVXWDirs+ SmVSTms - SiSsVIWSirs        i = TRAD_COMM; k = CGDS_COMM; r = REG; s = REG\n\nVT international margin suply\n\nVT = SiSrVSTir        i = TRAD_COMM; r = REG\n\nVXWir value of exports of tradable commodity i from source r evaluated at world (fob) prices\n\nVXWir = SsVXWDirs+VSTir        i = MARG_COMM; r = REG\nVXWir = SsVXWDirs        i = NMRG_COMM; r = REG\n\nVXWREGIONr value of exports from source r evaluated at world (fob) prices\n\nVXWREGIONr = SiVXWir        i = TRAD_COMM; r = REG\n\nVXWCOMMODi value of exports of tradable commodity i evaluated at world (fob) prices\n\nVXWCOMMODi= SrVXWir        i = TRAD_COMM; r = REG\n\nVIWir value of imports of tradable commodity i into region r evaluated at world (cif) prices\nVIWir= SsVIWSisr        i = TRAD_COMM; r = REG; s = REG\nVIWREGIONr value of imports into region r evaluated at world (cif) prices\nVIWREGIONr = SiVIWir        i = TRAD_COMM; r = REG\nVIWCOMMODi value of imports of tradable commodity i evaluated at world (cif) prices\nVIWCOMMODi= SrVIWir        i = TRAD_COMM; r = REG\nVXWLD value of worldwide commodity exports evaluated at world (fob) prices\nVXWLD= SrVXWREGIONr        r = REG\nPW_PMir ratio of world (fob) to domestic market prices for tradable commodity i in region\nPW_PMir = SsVXWDirs / SsVXMDirs        i = TRAD_COMM; r = REG; s = REG\nVOWir value of output of tradable commodity i in region r, evaluated at world (fob) prices\nVOWir = VDMir * PW_PMir + SsVXWDirs + VSTir        i = MARG_COMM; r = REG; s = REG\nVOWir = VDMir * PW_PMir + SsVXWDirs        i = NMRG_COMM; r = REG; s = REG\nVWOWi value of world supply of tradable commodity i evaluated at world (fob) prices\nVWOWi = SrVOWiri = TRAD_COMM; r = REG\nVTMFSDmirs int’l margin usage, by margin, freight, source, and destination m = MARG_COMM;i = TRAD_COMM; r = REG; s = REG\nVTFSDirs aggregate value of svces in the shipment of i from r to s\nVTFSDirs = SmVTMFSDmirs       In a balanced data base VIWSirs = VXWDirs + VTFSDirs i = TRAD_COMM; m = MARG_COMM; r = REG; s = REG\nVTMPROVm international margin services provision\nVTMPROVm = SrVSTmr        m = MARG_COMM; r = REG\nVTRPROVr international margin supply, by region\nVTRPROVr = SmVSTmr        m = MARG_COMM; r = REG\nVTMUSEm international margin services usage, by type\nVTMUSEm = SiSrSsVTMFSDmirs        m = MARG_COMM; i = TRAD_COMM; r = s = REG       In a balanced data base, VTMPROV = VTMUSE\nVTMUSESHRmirs share of i,r,s usage in global demand for m\nVTMUSESHRmirs = VTFSDirs / VT        m = MARG_COMM; i = TRAD_COMM; r = s = REG if VTMUSEm &lt;&gt; 0.0 then VTMUSESHRmirs = VTMFSDmirs / VTMUSEm\nVTSUPPSHRmr share of region r in global supply of margin m\nVTSUPPSHRmr = VTRPROVr / VT        if VTMPROVm &lt;&gt; 0.0 then VTSUPPSHRmr = VSTmr / VTMPROVm m = MARG_COMM; r = REG\nVTUSE international margin services usage;\nVTUSE=SmSiSrSsVTMFSDmirs        m = MARG_COMM; i = TRAD_COMM; r = s = REG\nVWOUi value of world output of i at user prices\nVWOUi = Ss[(VPAis + VGAis) + SjVFAijs] s = REG; j = PROD_COMM\nVENDWREGr value of primary factors, at mkt prices, by region\nVENDWREGr = Si VOMir        i = ENDOW_COMM; r = REG\nVENDWWLD value of primary factors, at mkt prices, worldwide\nVENDWWLD = Sr VENDWREGr r = REG\nVPAEVir private household expend. on i in r valued at agent’s prices, for EV calculation\nVPAEVir = qpevir        i = TRAD_COMM, r = REG\nVPAREGEVr private consumption expenditure in region r, for EV calculation\nVPAREGEVr = SiVPAEVir        i = TRAD_COMM, r = REG YGr regional government consumption expenditure, in region r        r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#shares",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#shares",
    "title": "Variables in v11",
    "section": "",
    "text": "SHRDFMijr share of domestic sales of tradable commodity i used by firms in sector j of regin r evaluated at market prices\nSHRFMijr = VDFMijr / VDMir        i = TRAD_COMM; j = PROD_COMM; r = REG\nSHRDPMi share of domestic sales of commodity i used by private household in region r evaluated at market prices\nSHRDPMir = VDPMir / VDMir        i = TRAD_COMM; r = REG\nSHRDGMi share of domestic sales of commodity i used by the government in region r evaluated at market prices\nSHRDGMir = VDGMir / VDMir        i = TRAD_COMM; r = REG\nSHRIFMijr share of aggregate imports of tradable commodity i used by firms in sector j of regin r evaluated at market prices\nSHRIFMijr = VIFMijr / VIMir        i = TRAD_COMM; j = PROD_COMM; r = REG\nSHRIPMir share of aggregate imports of commodity i used by private household in region r evaluated at market prices\nSHRIPMir = VIPMir / VIMir        i = TRAD_COMM; r = REG\nSHRIGMir share of aggregate imports of commodity i used by the government in region r evaluated at market prices\nSHRIGMir = VIGMir / VIMir        i = TRAD_COMM; r = REG\nFMSHRijr share of imports in the composite for tradable commodity i used by firms in sector j of region r evaluated at agents’ prices\nFMSHRijr = VIFAijr / VFAijr        i = TRAD_COMM; j = PROD_COMM; r = REG\nPMSHRir share of imports in the composite for tradable commodity i used by private household in region r evaluated at agents’ prices\nPMSHRir = VIPAir / VPAir        i = TRAD_COMM; r = REG\nGMSHRir share of impotrs in the composite for tradable commodity i used by the government in region r evaluated at agents’ prices\nGMSHRir = VIGAir / VGAir        i = TRAD_COMM; r = REG\nCONSHRir budget share of the composite for tradable i in total private household expenditure in regin r evaluated at agents’ prices\nCONSHRir = VPAir / PRIVEXPr        i = TRAD_COMM; r = REG\nMSHRSirs market share of source r in the aggregate imports of tradable commodity i in region s evaluated at market prices\nMSHRSirs = VIMSirs / SrVIMSirs        i = TRAD_COMM; r = s = REG\nSVAijr share of endowment commodity i in value-added of sector j of region r evaluated at agents’ prices\nSVAijr= VFAijr / SkVFAkjr        i = ENDW_COMM; j = PROD_COMM; k = ENDW_COMM; r = REG\nREVSHRijr share of endowment commodity i used by fiorms in sector j of regin r evaluated at market prices\nREVSHRijr= VFMijr / SkVFMkjr        i = ENDW_COMM; j = PROD_COMM; k = PROD_COMM; r = REG\nFOBSHRirs share of fob price in the cif price for tradable commodity i exported from source r to destination s\nFOBSHRirs= VXWDirs / VIWSCOSTirs        i = TRAD_COMM; r = s = REG\nXSHRPRIVr private expenditure share in regional income\nXSHRPRIVr = PRIVEXPr/INCOMEr        r = REG\nXSHRGOVr government expenditure share in regional income\nXSHRGOVr = GOVEXPr/INCOMEr        r = REG\nXSHRSAVEr saving share in regional income\nXSHRSAVEr = SAVEr/INCOMEr        r = REG\nSHRDMir share of domestic sales of i in r        i = TRAD_COMM; r = REG\nSHRDMir = VDMir / VOMir\nSHRSTmr share of sales of m to global transport services in r        m = MARG_COMM; r = REG\nSHRSTmr = VSTmr / VOMmr\nSHRXMDirs share of export sales of i to s in r        i = TRAD_COMM; r = s = REG\nSHRXMDirs = VXMDirs / VOMir        i = TRAD_COMM; r = s = REG\nSHREMijr share of mobile endowments i used by sector j at mkt prices\nSHREMijr = VFMijr / VOMir        i = ENDWM_COMM; j = PROD_COMM; r = REG\nTRNSHRirs share of transport price in the cif price for tradable commodity i exported from source r to destination s\nTRSHRirs= VTFSDirs / VIWSCOSTirs i = TRAD_COMM; r = s = REG\nXWCONSHRir expansion-parameter-weighted consumption share\nXWCONSHRir = CONSHRir*INCPARir/UELASPRIVr        i = TRAD_COMM; r = REG\nXSHRPRIVEVr private expenditure share in regional income, for EV calculation        r = REG\nXSHRPRIVEVr = PRIVEXPEVr/INCOMEEVr        r = REG\nXSHRGOVEVr government expenditure share in regional income, for EV calculation        r = REG\nXSHRGOVEVr = GOVEXPEVr/INCOMEEVr        r = REG\nXSHRSAVEEVr saving share in regional income, for EV calculation        r = REG\nXSHRSAVEEVr = SAVEEVr/INCOMEEVr        r = REG\nXWCONSHREVir expansion-parameter-weighted consumption share, for EV calculation\nXWCONSHREVir = CONSHREVir*INCPARir/UELASPRIVEVr        i = TRAD_COMM, r = REG\nCONSHREVir share of private household consn devoted to good i in r, for EV calculation\nCONSHREVir = VPAEVir/VPAREGEVr\nSTCijr share of i in total costs of j in r\nSTCijr = VFAijr / Sk VFAkjr        i = k = DEMD_COMM; j = PROD_COMM; r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#quantity-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#quantity-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "QOir quantity of non-saving commodity i output or supplied in region r | i = NSAV_COMM r = REG\nQOESijr quantity of sluggish endowment i supplied to sector j firm of region r | i = ENDWS_COMM; j = PROD_COMM; r = REG\nQDSir quantity of domestic sales of tradable commodity i in region r | i = TRAD_COMM; r = REG\nQXSirs quantity of exports of tradable commodity i from source r to destination s | i = TRAD_COMM; r = s = REG\nQSTir quantity of sales of marginal commodity i to the international transport sector in region r | i = MARG_COMM; r = REG\nQFEijr quantity of endowment i demanded by sector j firm in region r | i = ENDW_COMM; j = PROD_COMM; r = REG\nQVAjr quantity index of land-labor-capital composite (value-added) in sector j firm in region r | j = PROD_COMM; r = REG\nQFijr quantity of composite tradable commodity i demanded by sector j firm in region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nQFDijr quantity of domestic tradable i demanded by sector j firm in region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nQFMijr quantity of imported tradable i demanded by sector j firm in region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nQPir quantity of composite tradable i demanded by private household in region r | i = TRAD_COMM; r = REG\nQPDir quantity of domestic tradable i demanded by private household in region r | i = TRAD_COMM; r = REG\nQPMir quantity of imported tradable i demanded by private household in region r | i = TRAD_COMM; r = REG\nQGir quantity of composite tradable commodity i demanded by government household in region r | i = TRAD_COMM; r = REG\nQGDir quantity of domestic tradable commodity i demanded by government household in region r | i = TRAD_COMM; r = REG\nQGMir quantity of imported tradable commodity i demanded by government household in region r | i = TRAD_COMM; r = REG\nQIMir quantity of aggregate imports of tradable commodity i demanded by region r using market prices as weights | i = TRAD_COMM; r = REG\nQIWir quantity of aggregate imports of tradable commodity i demanded by region r using cif prices as weights | i = TRAD_COMM; r = REG\nQXWir quantity of aggregate exports of tradable commodity i supplied from region r using fob prices as weights | i = TRAD_COMM; r = REG\nQIWREGr volume of merchandise imports demanded by region r | r = REG\nQXWREGr volume of merchandise exports supplied by region r | r = REG\nQIWCOMi volume of global merchandise imports of tradable commodity i | i = TRAD_COMM; r = REG\nQXWCOMi volume of global merchandise exports of tradable commodity i | i = TRAD_COMM; r = REG\nQXWWLD volume of world trade\nQOWi quantity index for world supply of tradable commodity i | i = TRAD_COMM\nEXPANDir Change in investment levels relative to endowment stock | i = ENDWC_COMM; r = REG\nQOWUirquantity index for world supply of good i at user prices i = TRAD_COMM; r = REG\nQTMFSDmirs international usage margin m on i from r to s m = MARG_COMM; i = TRAD_COMM; r = s = REG\nQTMm global margin usage m = MARG_COMM\nQCGDSr quantity of capital goods sector supplied in region r r = REG\nQSAVEr quantity of savings demanded in region r r = REG\nGLOBALCGDS quantity of global supply of capital for net investment\nKSVCESr quantity of capital services in region r r = REG\nKBr quantity of beginning-of-period capital stock in region r r = REG\nKEr quantity of end-of-period capital stock in region r r = REG\nQGDPr quantity index for GDP in region r r = REG\nWALRAS_DEM quantity demanded in the omitted market - equals global demand for savings\nWALRAS_SUP quantity supplied in the omitted market - equals global supply of new capital goods composite\nPOPr population in region r r = REG\nCOMPVALADir composition of value added for good i and region r i = TRAD_COMM; r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#price-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#price-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "PMir market price of non-saving commodity i in region r i = NSAV_COMM; r = REG\nPMESijr market price for sluggish endowment i supplied to firm j in region r i = ENDWS_COMM; j = PROD_COMM; r = REG\nPSir supply price of non-saving commodity i in region r i = NSAV_COMM; r = REG\nPFEijr demand price for endowment i by firms in sector j of region r i = ENDW_COMM; j = PROD_COMM; r = REG\nPVAjr price of value-added in sector j j = PROD_COMM; r = REG\nPFijr demand price for composite tradable i by firms in sector j of region r i = TRAD_COMM; j = PROD_COMM; r = REG\nPFDijr demand price for domestic tradable i by firms in sector j of region r i = TRAD_COMM; j = PROD_COMM; r = REG\nPFMijr demand price for imported tradable i by firms in sector j of region r i = TRAD_COMM; j = PROD_COMM; r = REG\nPPir private household’s demand price for composite tradable i in region r i = TRAD_COMM; r = REG\nPPDir private household’s demand price for domestic tradable i in region r i = TRAD_COMM; r = REG\nPPMir private household’s demand price for imported tradable i in region r i = TRAD_COMM; r = REG\nPGir government household’s demand price for composite tradable i in region r i = TRAD_COMM; r = REG\nPGDir government household’s demand price for domestic tradable i in region r i = TRAD_COMM; r = REG\nPGMir government household’s demand price for imported tradable i in region r i = TRAD_COMM; r = REG\nPSAVEr price of savings in region r r = REG\nPCGDSr price of investment good in region r - equals PS(cgds,r) r = REG\nPGDPr price index for GDP in region r r = REG\nPPRIVr price index for private household expenditure in region r r = REG\nPGOVr price index for government household expenditure in region r r = REG\nPFOBirs world fob price of tradable commodity i exported from source r to destination s (prior to including transport margins) i = TRAD_COMM; r = s = REG\nPCIFirs world cif price of tradable commodity i imported from source r to destination s ) after including transport margins) i = TRAD_COMM; r = s = REG\nPMSirs market price by source of tradable commodity i imported from source r to destination s i = TRAD_COMM; r = s = REG\nPIMir market price of aggregate imports of tradable commodity i in region r i = TRAD_COMM; r = REG\nPIWir world price of aggregate imports of tradable commodity i in region r i = TRAD_COMM; r = REG\nPXWir price index for aggregate exports of tradable commodity i from region r i = TRAD_COMM; r = REG\nPIWREGr price index of merchandise imports in region r r = REG\nPXWREGr price index of merchandise exports from region r r = REG\nPIWCOMi price index of global merchandise imports of tradable commodity i i = TRAD_COMM\nPXWCOMi price index of global merchandise exports of tradable commodity i i = TRAD_COMM\nPXWWLD price index of world trade\nPRir ratio of domestic market price to market price of imports for tradable commodity i in region r i = TRAD_COMM; r = REG\nPWi world price index for total supply of tradable commodity i in region r i = TRAD_COMM\nPSWr price index received for tradable commodities produced in region r including sales of net investment to the global bank r = REG\nPDWr price index paid for tradable commodities used in region r including purchases of savings from the global bank r = REG\nTOTr terms of trade for region r r = REG\nPTm price of margin services supplied m = MARG_COMM\nPCGDSWLD world average price of capital goods (net investment weights)\nPWUi world price index for total good i supplies at user prices i = TRAD_COMM\nPTRANSirs cost index for international transport of i from r to s i = TRAD_COMM; r = s = REG\nRENTALr rental rate on capital stock in regin r - equals PS(CGDS,r) r = REG\nRORCr current net rate of return on capital stock in region r r = REG\nROREr expected net rate of return on capital stock in region r r = REG\nRORG global net rate of return on capital stock\nPr price index for disposition of income by regional household\nPFACTREALir ratio of return to primary factor i to cpi in r i = ENDOW_COMM; r = REG\nPFACTORr market price index of primary factors, by region\nPFACTWLDworld price index of primary factors",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#technical-change-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#technical-change-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "AOir output augmenting technical change in sector j of region r | j = PROD_COMM; r = REG\nAFEijr primary factor i augmenting technical change in sector j of region r | i = ENDW_COMM; j = PROD_COMM; r = REG\nAFijr intermediate input i augmenting technical change in sector j of region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nAVAjr value-added augmenting technical change in sector j of region r | j = PROD_COMM; r = REG\nAMSirs import i from region r augmenting tech change in region s | i = TRAD_COMM, r = s = REG\nAOSECj output tech change of sector j, worldwide | j = PROD_COMM\nAOREGr output tech change in region r | r = REG\nAOALLjr output augmenting technical change in sector j of r | j = PROD_COMM; r = REG\nESUBTj elasticity of substitution among composite intermediate inputs in production | j = PROD_COMM\nAFCOMi intermediate tech change of input i, worldwide | i = TRAD_COMM\nAFSECj intermediate tech change of sector j, worldwide | j = PROD_COMM\nAFREGr intermediate tech change in region r | r = REG\nAFALLijr intermediate input i augmenting tech change by j in r | i = TRAD_COMM,j = PROD_COMM; r = REG\nAFECOMi factor input tech change of input i, worldwide | i = ENDW_COMM\nAFESECj factor input tech change of sector j, worldwide | j = PROD_COMM\nAFEREGr factor input tech change in region r | r = REG\nAFEALLijr primary factor i augmenting tech change sector j in r | i = ENDW_COMM; j = PROD_COMM; r = REG\nAMSirs reduction in effective price associated with delivery of i from r to s | i = TRAD_COMM; r = REG; s = REG\nATMFSDmirs tech change in m’s shipping of i from region r to s | m = MARG_COMM; i = TRAD_COMM; r = s = REG\nATMm tech change in mode m, worldwide; | m = TRAD_COMM\nATFi tech change shipping of i, worldwide | i = TRAD_COMM\nATSr tech change shipping from region r | r = REG\nATDs tech change shipping to s | s = REG\nATALLmirs tech change in m’s shipping of i from region r to s | m = MARG_COMM; i = TRAD_COMM; r = REG; s = REG\nAUr input-neutral shift in utility function | r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#policy-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#policy-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "TOir power of the tax on output (or income) of non-savings commodity i in region r | i = NSAV_COMM; r = REG\nTFijr power of the tax on endowment commodity i demanded by sector j of regin r | i = ENDW_COMM; j = PROD_COMM; r = REG\nTFDijr power of the tax on domestic tradable commodity i demanded by sector j of region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nTFMijr power of the tax on imported tradable commodity i demanded by sector j of region r | i = TRAD_COMM; j = PROD_COMM; r = REG\nTPDir power of the tax on domestic tradable commodity i purchased by private household in region r | i = TRAD_COMM; r = REG\nTPMir power of the tax on imported tradable commodity i purchased by private household in region r | i = TRAD_COMM; r = REG\nTGDir power of the tax on domestic tradable commodity i purchased by government household in region r | i = TRAD_COMM; r = REG\nTGMir power of the tax on imported tradable commodity i purchased by government household in region r | i = TRAD_COMM; r = REG\nTXSirs power of the tax on exports of tradable commodity i from source r to destination r (levied in region r) | i = TRAD_COMM; r = s = REG\nTMSirs power of the tax on imports of tradable commodity i from source r to destination s (levied in region s) | i = TRAD_COMM; r = s = REG\nTXir power of the variable export tax on exports of tradable commodity from region r - destination generic | i = TRAD_COMM; r = REG\nTMir power of the variable import tax (levy) on imports of tradable commodity i in region s - source generic | i = TRAD_COMM; r = REG\nTPr region-wide shock to tax on purchases by private household in region r | r = REG\nATPMir actual tax on imported traded commodity i purchased by private households in region r | i = TRAD_COMM; r = REG\nATPDir actual tax on domestic traded commodity i purchased by private households in region r | i = TRAD_COMM; r = REG\nDGTAXir tax on government consumption of domestic good i in region r | r = REG\nIGTAXir tax on government consumption of imported good i in region r | r = REG\nTGCr government consumption tax payments in r\nTGCr = SiDGTAXir + IGTAXir i = TRAD_COMM; r = REG\nDPTAXir tax on private consumption of domestic good i in region r\nDPTAXir = VDPAir - VDPMir i = TRAD_COMM; r = REG\nIPTAXir tax on private consumption of imported good i in region r\nIPTAXir = VIPAir - VIPMir i = TRAD_COMM; r = REG\nTPCr private consumption tax payments in r\nTPCr = SiDPTAXir + IPTAXir i = TRAD_COMM; r = REG\nDFTAXijr tax on use of domestic intermediate good i by j in r\nDFTAXijr = VDFAijr - VDFMijr i = TRAD_COMM; j = PROD_COMM; r = REG\nIFTAXijr tax on use of imported intermediate good i by j in r\nIFTAXijr = VIFAijr - VIFMijr i = TRAD_COMM; j = PROD_COMM; r = REG\nTIUr firms’ tax payments on intermediate goods usage in r\nTIUr = SjSi(DFTAXijr + IFTAXijr) i = TRAD_COMM; j = PROD_COMM; r = REG\nETAXijr tax on use of endowment good i by industry j in region r\nETAXijr = VFAijr - VFMijr i = ENDW_COMM; j = PROD_COMM; r = REG\nTFUr firms’ tax payments on primary factor usage in r\nTFUr = SiSjETAXijr i = ENDW_COMM; j = PROD_COMM; r = REG\nPTAXir output tax on good i in region r\nPTAXir = VOMir - VOAir i = NSAV_COMM; r = REG\nTOUTr production tax payments in r\nTOUTr = SiPTAXir i = PROD_COMM; r = REG\nXTAXDirs tax on exports of good i from source r to destination s\nXTAXDirs = VXWDirs - VXMDirs i = TRAD_COMM; r = s = REG\nTEXr export tax payments in r\nTEXr = SiSsXTAXDirs i = TRAD_COMM; r = s = REG\nMTAXirs tax on imports of good i from source r in destination s\nMTAXirs = VIMSirs - VIWSirs i = TRAD_COMM; r = s = REG\nTIMr import tax payments in r\nTIMr = SiSsMTAXisr i = TRAD_COMM; r = s = REG\nTOTTAXr Total tax receipts in r\nTOTTAXr = TPCr + TGCr + TIUr + TFUr + TOUTr + TEXr + TIMr + TINCr r = REG\nTGCRr change in ratio of government consumption tax to INCOME in region r r = REG\nTPCRr change in ratio of private consumption tax to INCOME in region r r = REG\nTIURr change in ratio of tax on intermediate usage to INCOME in region r r = REG\nTFURr change in ratio of tax on primary factor usage to INCOME in region r r = REG\nTOUTRr change in ratio of output tax to INCOME in region r nbsp; r = REG\nTEXPRr change in ratio of export tax to INCOME in region r r = REG\nTIMPRr change in ratio of import tax to INCOME in region r r = REG\nTINCr income tax payments in r\nTINCr = SiPTAXir i = ENDOW_COMM; r = REG\nTINCRr change in ratio of income tax to INCOME in region r r = REG\nDTAXRr change in ratio of taxes to INCOME in r r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#regional-household-preference-parameters-and-shifters",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#regional-household-preference-parameters-and-shifters",
    "title": "Variables in v11",
    "section": "",
    "text": "DPARPRIVr private consumption distribution parameter\nDPARPRIVr = UELASPRIVr*XSHRPRIVr/UTILELASr r = REG\nDPARGOVr government consumption distribution parameter\nDPARGOVr = XSHRGOVr/UTILELASr r = REG\nDPARSAVEr saving distribution parameter\nDPARSAVEr = XSHRSAVEr/UTILELASr r = REG\nDPARSUMr sum of distribution parameters r = REG\ndpavr change in average distribution parameter r = REG\ndpsumr change in sum of the distribution parameters r = REG\ndpprivr change in private consumption distribution parameter r = REG\ndpgovr change in government consumption distribution parameter r = REG\ndpsaver change in saving distribution parameter r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#slack-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#slack-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "profitslackjc slack variable in the ZEROPROFITS equation (this is exogenous as long as output level, QO(j), is determined endogenously) | j = PROD_COMM; r = REG\ncgdslackr slack variable in the CAPGOODS equation (this is exogenous as long as output level of new capital goods, QO(“cgds”), is determined endogenously | r = REG\nendwslackir slack variable in the MKTCLENDWM and ENDW_SUPPLY equations (this is exogenous as long as primary factor rental rates, PMi and PMESij are determined endogenously) | i = ENDWS_COMM; r = REG\ntradslackir slack variable in the MKTCLTRD equation (this is exogenous as long as market price of tradable, PMi, is determined endogenously) | i = TRAD_COMM; r = REG\nincomeslackr slack variable in the INCOME equation (this is exogenous as long as income, Y, is determined endogenously) | r = REG\npsaveslackr slack variable in the SAVINGS equation (this is exogenous as long as level of savings, QSAVE, is determined endogenously) | r = REG\nwalraslack slack variable in the WALRAS equation (this is endogenous as long as price of savings, PSAVE, is determined exogenously which is the case in a standard GE closure. When any one of the GE links is broken, this is swapped with PSAVE, the numeraire price, thereby forcing global savings to equal global investment).\nincomeslackr slack variable in the expression for regional income | r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-and-income-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#value-and-income-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "vxwfobir percentage change in value of exports of tradable commodity i from source region r using fob weights (is identical to the linearized form of VXWir) | i = TRAD_COMM; r = REG\nvxwregr percentage change in value of merchandise exports from region r using fob weights (is identical to the linearized form of VXWREGIONr) | r = REG\nvxwcomi percentage change in value of global merchandise exports of tradable commodity i using fob weights (is identical to the linearized form of VXWCOMMODi) | i = TRAD_COMM\nviwcifir percentage change in value of imports of tradable commodity i into region r using fob weights (is identical to the linearized form of VIWir) | i = TRAD_COMM; r = REG\nviwregr percentage change in value of merchandise imports into region r using cif weights (is identical to the linearized form of VIWREGIONir) | r = REG\nviwcomi percentage change in value of global merchandise imports of tradable commodity i using fob weights (is identical to the linearized form of VIWCOMMODi) | i = TRAD_COMM\nvxwwld percentage change in value of worlwide commodity exports using fob weights (is identical to the linearized form of VXWLD)\nvaluewi percentage change in value of global supply of tradable commodity i using fob weights (is identical to the linearized form of VWOWi) | i = TRAD_COMM\nvaluewui value of world supply of good i at user prices | i = TRAD_COMM\nvgdpr percentage change in value of GDP in region r (is identical to the linearized form of GDPr) | r = REG\nyr percentage change in regional household income (is identical to the linearized form of INCOME) | r = REG\nypr percentage change in private household expenditure (is identical to the linearized form of PRIVEXP). | r = REG\nFYr primary factor income in r net of depreciation\nFYr = SiVOMir - VDEPr | i = ENDW_COMM; r = REG\nfincomer pc change in factor income variable in r net of depreciation r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#utility-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#utility-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "Ur per capita utility from aggregate household expenditure in region r r = REG\nUPr per capita utility from private household expenditure in region r r = REG\nUGr aggregate utility from government household expenditure in region r r = REG\nUTILELASr elasticity of cost of utility wrt utility\nUTILELASr = (UELASPRIVr*XSHRPRIVr + XSHRGOVr + XSHRSAVEr)/DPARSUMr r = REG\nuelasr pc change in elasticity of cost of utility wrt utility r = REG\nueprivr pc change in utility elasticity of private cons expenditure r = REG\nuelasevr pc change in elasticity of cost of utility wrt utility, for EV calculation r = REG\nueprivevr pc change in utility elasticity of private cons expenditure, for EV calculation r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#welfare-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#welfare-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "EVr equivalent variation, in $ US million (positive figure indicates welfare improvement) r = REG\nWEV equivalent variation, in $ US million, for the world (positive figure indicates welfare improvement)\nugevr per capita utility from gov’t expend., for EV calculation r = REG\nupevr per capita utility from private expend., for EV calculation r = REG\nqsaveevr total quantity of savings demanded, for EV calculation r = REG\nypevr private consumption expenditure, in region r, for EV calculation r = REG\nygevr government consumption expenditure, in region r, for EV calculation r = REG\nqpevir private household demand for commodity i in region r, for EV calculation i = TRAD_COMM, r = REG\nyevr regional household income, in region r, for EV calculation\nysaveevr NET savings expenditure, for EV calculation\ndpavevr average distribution parameter shift, for EV calculation r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_variables.html#trade-balance-variables",
    "href": "gtap_invest/user_guide/gtapv7_v11_variables.html#trade-balance-variables",
    "title": "Variables in v11",
    "section": "",
    "text": "DTBALr change in trade balance of region r , in US$ million (positive figure indicates increases in exports exceeds increases in imports) | r = REG\nDTBALiir change in trade balance for tradable commodity i in region r , in US$ million (positive figure indicates increases in exports exceeds increases in imports) | i = TRAD_COMM; r = REG",
    "crumbs": [
      "Variables in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_regions.html",
    "href": "gtap_invest/user_guide/gtapv7_v11_regions.html",
    "title": "Regions in v11:",
    "section": "",
    "text": "Regions in v11:\nAggregate string: v11_r160_labels\n\n\n\n\n\n\n\n\n\ngtapv7_r160_id\ngtapv7_r160_label\ngtapv7_r160_name\ngtapv7_r160_description\n\n\n1\naus\nAustralia\nAustralia\n\n\n2\nnzl\nNew Zealand\nNew Zealand\n\n\n3\nxoc\nRest of Oceania\nRest of Oceania\n\n\n4\nchn\nChina\nChina\n\n\n5\nhkg\nChina, Hong Kong SAR\nChina, Hong Kong SAR\n\n\n6\njpn\nJapan\nJapan\n\n\n7\nkor\nRepublic of Korea\nRepublic of Korea\n\n\n8\nmng\nMongolia\nMongolia\n\n\n9\ntwn\nTaiwan Province of China\nTaiwan Province of China\n\n\n10\nxea\nRest of East Asia\nRest of East Asia\n\n\n11\nbrn\nBrunei Darussalam\nBrunei Darussalam\n\n\n12\nkhm\nCambodia\nCambodia\n\n\n13\nidn\nIndonesia\nIndonesia\n\n\n14\nlao\nLao People’s Democratic Republic\nLao People’s Democratic Republic\n\n\n15\nmys\nMalaysia\nMalaysia\n\n\n16\nphl\nPhilippines\nPhilippines\n\n\n17\nsgp\nSingapore\nSingapore\n\n\n18\ntha\nThailand\nThailand\n\n\n19\nvnm\nViet Nam\nViet Nam\n\n\n20\nxse\nRest of Southeast Asia\nRest of Southeast Asia\n\n\n21\nafg\nAfghanistan\nAfghanistan\n\n\n22\nbgd\nBangladesh\nBangladesh\n\n\n23\nind\nIndia\nIndia\n\n\n24\nnpl\nNepal\nNepal\n\n\n25\npak\nPakistan\nPakistan\n\n\n26\nlka\nSri Lanka\nSri Lanka\n\n\n27\nxsa\nRest of South Asia\nRest of South Asia\n\n\n28\ncan\nCanada\nCanada\n\n\n29\nusa\nUnited States of America\nUnited States of America\n\n\n30\nmex\nMexico\nMexico\n\n\n31\nxna\nRest of North America\nRest of North America\n\n\n32\narg\nArgentina\nArgentina\n\n\n33\nbol\nBolivia (Plurinational State of)\nBolivia (Plurinational State of)\n\n\n34\nbra\nBrazil\nBrazil\n\n\n35\nchl\nChile\nChile\n\n\n36\ncol\nColombia\nColombia\n\n\n37\necu\nEcuador\nEcuador\n\n\n38\npry\nParaguay\nParaguay\n\n\n39\nper\nPeru\nPeru\n\n\n40\nury\nUruguay\nUruguay\n\n\n41\nven\nVenezuela (Bolivarian Republic of)\nVenezuela (Bolivarian Republic of)\n\n\n42\nxsm\nRest of South America\nRest of South America\n\n\n43\ncri\nCosta Rica\nCosta Rica\n\n\n44\ngtm\nGuatemala\nGuatemala\n\n\n45\nhnd\nHonduras\nHonduras\n\n\n46\nnic\nNicaragua\nNicaragua\n\n\n47\npan\nPanama\nPanama\n\n\n48\nslv\nEl Salvador\nEl Salvador\n\n\n49\nxca\nRest of Central America\nRest of Central America\n\n\n50\ndom\nDominican Republic\nDominican Republic\n\n\n51\nhti\nHaiti\nHaiti\n\n\n52\njam\nJamaica\nJamaica\n\n\n53\npri\nPuerto Rico\nPuerto Rico\n\n\n54\ntto\nTrinidad and Tobago\nTrinidad and Tobago\n\n\n55\nxcb\nCaribbean\nCaribbean\n\n\n56\naut\nAustria\nAustria\n\n\n57\nbel\nBelgium\nBelgium\n\n\n58\nbgr\nBulgaria\nBulgaria\n\n\n59\nhrv\nCroatia\nCroatia\n\n\n60\ncyp\nCyprus\nCyprus\n\n\n61\ncze\nCzechia\nCzechia\n\n\n62\ndnk\nDenmark\nDenmark\n\n\n63\nest\nEstonia\nEstonia\n\n\n64\nfin\nFinland\nFinland\n\n\n65\nfra\nFrance\nFrance\n\n\n66\ndeu\nGermany\nGermany\n\n\n67\ngrc\nGreece\nGreece\n\n\n68\nhun\nHungary\nHungary\n\n\n69\nirl\nIreland\nIreland\n\n\n70\nita\nItaly\nItaly\n\n\n71\nlva\nLatvia\nLatvia\n\n\n72\nltu\nLithuania\nLithuania\n\n\n73\nlux\nLuxembourg\nLuxembourg\n\n\n74\nmlt\nMalta\nMalta\n\n\n75\nnld\nNetherlands\nNetherlands\n\n\n76\npol\nPoland\nPoland\n\n\n77\nprt\nPortugal\nPortugal\n\n\n78\nrou\nRomania\nRomania\n\n\n79\nsvk\nSlovakia\nSlovakia\n\n\n80\nsvn\nSlovenia\nSlovenia\n\n\n81\nesp\nSpain\nSpain\n\n\n82\nswe\nSweden\nSweden\n\n\n83\ngbr\nUnited Kingdom of Great Britain and Northern Ireland\nUnited Kingdom of Great Britain and Northern Ireland\n\n\n84\nche\nSwitzerland\nSwitzerland\n\n\n85\nnor\nNorway\nNorway\n\n\n86\nxef\nRest of EFTA\nRest of EFTA\n\n\n87\nalb\nAlbania\nAlbania\n\n\n88\nsrb\nSerbia\nSerbia\n\n\n89\nblr\nBelarus\nBelarus\n\n\n90\nrus\nRussian Federation\nRussian Federation\n\n\n91\nukr\nUkraine\nUkraine\n\n\n92\nxee\nRest of Eastern Europe\nRest of Eastern Europe\n\n\n93\nxer\nRest of Europe\nRest of Europe\n\n\n94\nkaz\nKazakhstan\nKazakhstan\n\n\n95\nkgz\nKyrgyzstan\nKyrgyzstan\n\n\n96\ntjk\nTajikistan\nTajikistan\n\n\n97\nuzb\nUzbekistan\nUzbekistan\n\n\n98\nxsu\nRest of Former Soviet Union\nRest of Former Soviet Union\n\n\n99\narm\nArmenia\nArmenia\n\n\n100\naze\nAzerbaijan\nAzerbaijan\n\n\n101\ngeo\nGeorgia\nGeorgia\n\n\n102\nbhr\nBahrain\nBahrain\n\n\n103\nirn\nIran (Islamic Republic of)\nIran (Islamic Republic of)\n\n\n104\nirq\nIraq\nIraq\n\n\n105\nisr\nIsrael\nIsrael\n\n\n106\njor\nJordan\nJordan\n\n\n107\nkwt\nKuwait\nKuwait\n\n\n108\nlbn\nLebanon\nLebanon\n\n\n109\nomn\nOman\nOman\n\n\n110\npse\nPalestine\nPalestine\n\n\n111\nqat\nQatar\nQatar\n\n\n112\nsau\nSaudi Arabia\nSaudi Arabia\n\n\n113\nsyr\nSyrian Arab Republic\nSyrian Arab Republic\n\n\n114\ntur\nTurkey\nTÃ¼rkiye\n\n\n115\nare\nUnited Arab Emirates\nUnited Arab Emirates\n\n\n116\nxws\nRest of Western Asia\nRest of Western Asia\n\n\n117\ndza\nAlgeria\nAlgeria\n\n\n118\negy\nEgypt\nEgypt\n\n\n119\nmar\nMorocco\nMorocco\n\n\n120\ntun\nTunisia\nTunisia\n\n\n121\nxnf\nRest of North Africa\nRest of North Africa\n\n\n122\nben\nBenin\nBenin\n\n\n123\nbfa\nBurkina Faso\nBurkina Faso\n\n\n124\ncmr\nCameroon\nCameroon\n\n\n125\nciv\nCote d’lvoire\nCÃ´te d’lvoire\n\n\n126\ngha\nGhana\nGhana\n\n\n127\ngin\nGuinea\nGuinea\n\n\n128\nmli\nMali\nMali\n\n\n129\nner\nNiger\nNiger\n\n\n130\nnga\nNigeria\nNigeria\n\n\n131\nsen\nSenegal\nSenegal\n\n\n132\ntgo\nTogo\nTogo\n\n\n133\nxwf\nRest of Western Africa\nRest of Western Africa\n\n\n134\ncaf\nCentral African Republic\nCentral African Republic\n\n\n135\ntcd\nChad\nChad\n\n\n136\ncog\nCongo\nCongo\n\n\n137\ncod\nDemocratic Republic of the Congo\nDemocratic Republic of the Congo\n\n\n138\ngnq\nEquatorial Guinea\nEquatorial Guinea\n\n\n139\ngab\nGabon\nGabon\n\n\n140\nxac\nSouth-Central Africa\nSouth-Central Africa\n\n\n141\ncom\nComoros\nComoros\n\n\n142\neth\nEthiopia\nEthiopia\n\n\n143\nken\nKenya\nKenya\n\n\n144\nmdg\nMadagascar\nMadagascar\n\n\n145\nmwi\nMalawi\nMalawi\n\n\n146\nmus\nMauritius\nMauritius\n\n\n147\nmoz\nMozambique\nMozambique\n\n\n148\nrwa\nRwanda\nRwanda\n\n\n149\nsdn\nSudan\nSudan\n\n\n150\ntza\nUnited Republic of Tanzania\nUnited Republic of Tanzania\n\n\n151\nuga\nUganda\nUganda\n\n\n152\nzmb\nZambia\nZambia\n\n\n153\nzwe\nZimbabwe\nZimbabwe\n\n\n154\nxec\nRest of Eastern Africa\nRest of Eastern Africa\n\n\n155\nbwa\nBotswana\nBotswana\n\n\n156\nswz\nEswatini\nEswatini\n\n\n157\nnam\nNamibia\nNamibia\n\n\n158\nzaf\nSouth Africa\nSouth Africa\n\n\n159\nxsc\nRest of Southern African Customs Union\nRest of Southern African Customs Union\n\n\n160\nxtw\nRest of the World\nRest of the World",
    "crumbs": [
      "Regions in v11:"
    ]
  },
  {
    "objectID": "gtap_invest/results/index.html",
    "href": "gtap_invest/results/index.html",
    "title": "GTAP-InVEST Results",
    "section": "",
    "text": "Download the full results from Johnson et al. 2023 (PNAS). This google drive directory has the results in raw Header Array format (.har) files used by GEMPACK, along with the results in CSV format for easier viewing. See the data description and readme file for explanations of the data organization."
  },
  {
    "objectID": "earth_economy_devstack/skill_sessions.html",
    "href": "earth_economy_devstack/skill_sessions.html",
    "title": "NatCap TEEMs Skill Sessions",
    "section": "",
    "text": "Matt Braaksma\nUniversity of Minnesota, March 2025\n\n\n\n\nGithub Repo"
  },
  {
    "objectID": "earth_economy_devstack/skill_sessions.html#intro-to-high-performance-computing-at-msi",
    "href": "earth_economy_devstack/skill_sessions.html#intro-to-high-performance-computing-at-msi",
    "title": "NatCap TEEMs Skill Sessions",
    "section": "",
    "text": "Matt Braaksma\nUniversity of Minnesota, March 2025\n\n\n\n\nGithub Repo"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html",
    "href": "earth_economy_devstack/seals_walkthrough.html",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "Make sure you have followed all of the steps in the installation page.\n\nIn particular, Clone the SEALS and Hazelbean repositories in the correct location, as described here\nYou will know you’ve got them installed correctly if your VS Code Explorer tab shows the repositories without an error message (Figure 1)\n\n\n\n\n\n\n\nIn the VS Code Explorer tab, navigate to your seals_dev directory (Figure 1)\n\nQuick note about file organization\n\nThe root directory of seals_dev contains more than just the seals library, such as directories for scripts, images, etc.\nThe library itself is in the seals subdirectory seals_dev/seals which may seem redundant but is necessary for the way Python imports work.\nIf you inspect the seals directory, you will see an __init__.py file. This make Python able to import the directory as a package.\n\nYou will also see a seals_main.py file. This is where most of the actual logic of seals is.\n\n\n\n\n\n\n\nOne does not simply run a main.py (Figure 1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (Figure 2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)\n\n\n\n\n\n\n\nThe run file begins with standard python imports\nThen in the if __name__ == '__main__': block, we define the project directory and initialize the project flow object\n\nThe reason for putting it in this block is so that you don’t accidentally run the code when you import the file in another script\n\nThis file then creates a ProjectFlow objected, assigned to the variable p.\n\nPython is an object-oriented programming langage\n\nThe hb.ProjectFlow() defines a class, which is like a recipe for an object\nWhen we call it, it generates on object of that class, which we assign to the variable p\n\n\n\nimport os, sys\nimport seals_utils\nimport seals_initialize_project\nimport hazelbean as hb\nimport pandas as pd\n\nmain = ''\nif __name__ == '__main__':\n    \n    # Create a ProjectFlow Object to organize directories and enable parallel processing.\n    p = hb.ProjectFlow()\n\n\n\n\nSEALS (and the EE Devstack) assumes (or softly requires) that you put all code and data somewhere relative to the user’s home directory os.path.expanduser('~')\n\nCan put it in sub-directories with extra_dirs = ['Files', 'seals', 'projects']\n\nIf you followed the EE method, you will have already created the seals directory at &lt;user_dir&gt;/Files/seals\n\nIn the seals directory, your code is in seals_dev\nIn the seals directory, you also will have a projects directory\n\nThis is created automatically if its not there\nAll data and outputs will be saved in this directory\n\nAs a best practice, you should not save data in the seals_dev directory\n\n\n\nGiven the directory structure above, p.project_name will also be use\n\n# Assign project-level attributes to the p object (such as in p.base_data_dir = ... below)\n# including where the project_dir and base_data are located.\n# The project_name is used to name the project directory below. If the directory exists, each task will not recreate\n# files that already exist. \np.user_dir = os.path.expanduser('~')        \np.extra_dirs = ['Files', 'seals', 'projects']\np.project_name = 'test_examples'\np.project_name = p.project_name + '_' + hb.pretty_time() # If don't you want to recreate everything each time, comment out this line.\n\n# Based on the paths above, set the project_dir. All files will be created in this directory.\np.project_dir = os.path.join(p.user_dir, os.sep.join(p.extra_dirs), p.project_name)\np.set_project_dir(p.project_dir) \n\n\n\n\nThe p object we created will organize input variables (called attributes when assigned to an object)\n\nLike this: p.attribute_name = 'ItsName'\n\nThe p object also has functions tied specificially to it (called methods when assigned to an object)\n\nSuch as: p.validate_name()\nMethods operate on the object that defined it\n\nSo validate_name() is specifically looking at the p object, often doing something to the p object, like fixing value of p.attribute_name",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#getting-set-up",
    "href": "earth_economy_devstack/seals_walkthrough.html#getting-set-up",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "Make sure you have followed all of the steps in the installation page.\n\nIn particular, Clone the SEALS and Hazelbean repositories in the correct location, as described here\nYou will know you’ve got them installed correctly if your VS Code Explorer tab shows the repositories without an error message (Figure 1)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#explore-the-seals-code",
    "href": "earth_economy_devstack/seals_walkthrough.html#explore-the-seals-code",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "In the VS Code Explorer tab, navigate to your seals_dev directory (Figure 1)\n\nQuick note about file organization\n\nThe root directory of seals_dev contains more than just the seals library, such as directories for scripts, images, etc.\nThe library itself is in the seals subdirectory seals_dev/seals which may seem redundant but is necessary for the way Python imports work.\nIf you inspect the seals directory, you will see an __init__.py file. This make Python able to import the directory as a package.\n\nYou will also see a seals_main.py file. This is where most of the actual logic of seals is.",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#run-files",
    "href": "earth_economy_devstack/seals_walkthrough.html#run-files",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "One does not simply run a main.py (Figure 1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (Figure 2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#setting-up-the-run-file",
    "href": "earth_economy_devstack/seals_walkthrough.html#setting-up-the-run-file",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "The run file begins with standard python imports\nThen in the if __name__ == '__main__': block, we define the project directory and initialize the project flow object\n\nThe reason for putting it in this block is so that you don’t accidentally run the code when you import the file in another script\n\nThis file then creates a ProjectFlow objected, assigned to the variable p.\n\nPython is an object-oriented programming langage\n\nThe hb.ProjectFlow() defines a class, which is like a recipe for an object\nWhen we call it, it generates on object of that class, which we assign to the variable p\n\n\n\nimport os, sys\nimport seals_utils\nimport seals_initialize_project\nimport hazelbean as hb\nimport pandas as pd\n\nmain = ''\nif __name__ == '__main__':\n    \n    # Create a ProjectFlow Object to organize directories and enable parallel processing.\n    p = hb.ProjectFlow()",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#directories-and-project-name",
    "href": "earth_economy_devstack/seals_walkthrough.html#directories-and-project-name",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "SEALS (and the EE Devstack) assumes (or softly requires) that you put all code and data somewhere relative to the user’s home directory os.path.expanduser('~')\n\nCan put it in sub-directories with extra_dirs = ['Files', 'seals', 'projects']\n\nIf you followed the EE method, you will have already created the seals directory at &lt;user_dir&gt;/Files/seals\n\nIn the seals directory, your code is in seals_dev\nIn the seals directory, you also will have a projects directory\n\nThis is created automatically if its not there\nAll data and outputs will be saved in this directory\n\nAs a best practice, you should not save data in the seals_dev directory\n\n\n\nGiven the directory structure above, p.project_name will also be use\n\n# Assign project-level attributes to the p object (such as in p.base_data_dir = ... below)\n# including where the project_dir and base_data are located.\n# The project_name is used to name the project directory below. If the directory exists, each task will not recreate\n# files that already exist. \np.user_dir = os.path.expanduser('~')        \np.extra_dirs = ['Files', 'seals', 'projects']\np.project_name = 'test_examples'\np.project_name = p.project_name + '_' + hb.pretty_time() # If don't you want to recreate everything each time, comment out this line.\n\n# Based on the paths above, set the project_dir. All files will be created in this directory.\np.project_dir = os.path.join(p.user_dir, os.sep.join(p.extra_dirs), p.project_name)\np.set_project_dir(p.project_dir)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#using-objects",
    "href": "earth_economy_devstack/seals_walkthrough.html#using-objects",
    "title": "First SEALS run walkthrough",
    "section": "",
    "text": "The p object we created will organize input variables (called attributes when assigned to an object)\n\nLike this: p.attribute_name = 'ItsName'\n\nThe p object also has functions tied specificially to it (called methods when assigned to an object)\n\nSuch as: p.validate_name()\nMethods operate on the object that defined it\n\nSo validate_name() is specifically looking at the p object, often doing something to the p object, like fixing value of p.attribute_name",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#scenario-definitions-csv",
    "href": "earth_economy_devstack/seals_walkthrough.html#scenario-definitions-csv",
    "title": "First SEALS run walkthrough",
    "section": "Scenario definitions CSV",
    "text": "Scenario definitions CSV\n\nThe scenario_definitions file specifies what defines the many different scenarios you want to run\n\nEach row will be one scenario\nEach time the model runs a new scenario, it will update its attributes based on this row\n\nIf you haven’t run SEALS yet, you won’t have a scenario_defintions file, so it will download the default one on the first run\n\n    p.scenario_definitions_filename = 'test_standard_scenarios.csv' \n    p.scenario_definitions_path = os.path.join(p.input_dir, p.scenario_definitions_filename)\n    seals_initialize_project.initialize_scenario_definitions(p)",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#scenario-types",
    "href": "earth_economy_devstack/seals_walkthrough.html#scenario-types",
    "title": "First SEALS run walkthrough",
    "section": "Scenario types",
    "text": "Scenario types\n\nScenario type determines if it is historical (baseline) or future (anything else) as well as what the scenario should be compared against. I.e., Policy minus BAU.\n\np.scenario_type = 'bau'",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#automatically-downloading-data",
    "href": "earth_economy_devstack/seals_walkthrough.html#automatically-downloading-data",
    "title": "First SEALS run walkthrough",
    "section": "Automatically downloading data",
    "text": "Automatically downloading data\n\nThis computing stack also uses hazelbean to automatically download needed data at run time.\n\nIn the code block below, we set p.base_data_dir to a location where we want to store lots of very large files.\nHazelbean will look here for certain files that are necessary and will download them from a cloud bucket if they are not present. T\nThis also lets you use the same base data across different projects.\n\nThe final directory has to be named base_data to match the naming convention on the google cloud bucket.\n\np.base_data_dir = os.path.join('Files/base_data')",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough.html#running-the-model",
    "href": "earth_economy_devstack/seals_walkthrough.html#running-the-model",
    "title": "First SEALS run walkthrough",
    "section": "Running the model",
    "text": "Running the model\nAfter doing the above steps, you should be ready to run run_test_seals.py. Upon starting, SEALS will report the “task tree” of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.\nOnce the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:\n[THIS IS NOT THE CORRECT IMAGE]\nOpen up the projected one (e.g., lulc_ssp2_rcp45_luh2-message_bau_2045.tif) in QGIS and enjoy your new, high-resolution land-use change projection!",
    "crumbs": [
      "SEALS",
      "Walkthrough"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_overview.html",
    "href": "earth_economy_devstack/seals_overview.html",
    "title": "SEALS",
    "section": "",
    "text": "SEALS, the Spatial Economic Allocation Landscape Simulator, is a land-use change model that downscales predictions of land-use change from aggregate (regional or coarse-gridded) inputs to a finer resolution (typically 10-300 meters). The primary comparative advantage of SEALS is fast computation. It is able to downscale to 300 meters globally (~8.4 billion grid-cells) in about an hour on a laptop. It is designed with parallelization in mind, and so scales well to high-performance, cloud or distributed computing. It is written in C++ for all performance-critical functions and uses Python for everything else. SEALS is a part of the Earth Economy Devstack, a software platform supported by NatCap TEEMs (The Earth-Economy Modellers).\n\n\nFull instructions for installing SEALS can be found in the Installation Steps. You will need to do the Repository Installation for and pull from the latest seals_dev repository for it to work. SEALS is under active development and is changing rapidly. We have submitted this code for publication but are waiting on reviews.",
    "crumbs": [
      "SEALS"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_overview.html#installation",
    "href": "earth_economy_devstack/seals_overview.html#installation",
    "title": "SEALS",
    "section": "",
    "text": "Full instructions for installing SEALS can be found in the Installation Steps. You will need to do the Repository Installation for and pull from the latest seals_dev repository for it to work. SEALS is under active development and is changing rapidly. We have submitted this code for publication but are waiting on reviews.",
    "crumbs": [
      "SEALS"
    ]
  },
  {
    "objectID": "earth_economy_devstack/run_file.html",
    "href": "earth_economy_devstack/run_file.html",
    "title": "Run File",
    "section": "",
    "text": "Run File"
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html",
    "href": "earth_economy_devstack/project_flow.html",
    "title": "Project Flow",
    "section": "",
    "text": "Project Flow is intended to allow a user to flow easily past the different stages of software complexity. A common situation for an academic or a research software engineer (RES) to find themselves in is that they wrote a quick script to answer a specific quesiton, but it turned out to be useful in other context. This can lead to a the script grows and grows until complexity hurts its usefulness. A software developer would then think “oops, I should really make this modular.” ProjectFlow provides several modalities useful to researchers ranging from simple drop-in solution to complex scripting framework. To do this, ProjectFlow manages folders and defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks.\nProject Flow is intended to flow easily from the situation where you have coded a script that grows and grows until you think “oops, I should really make this modular.” Thus, it has several modalities useful to researchers ranging from simple drop-in solution to complex scripting framework. Similar to the Stages of Complexity page, we will introduce progressively more complex usages, starting with the simples examples and working towards distributed parallelization on complex task-trees. If you would like to skip the stages and go straight to a full-functioning example, go to Standard ProjectFlow run file.\n\n\nWe will use the third level of complexity from the previous example as the launching point for how to use ProjectFlow. The code below adds a few additional steps we will work with.\nimport os\nimport numpy as np\nimport gdal\n\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# Load the raster\nraster_input_path = 'data/yield.tif' # TODOO[make all these actually point to data in hazelbean]\narray = raster_to_array(raster_input_path)\n\n# Set no data values equal to zero\n\n# Method 1 (creates new array)\narray_ndv_fix = np.where(array == -9999, 0, array)\n\n# Method 2 (inplace)\narray[array == -9999] = 0\n\n# Sum the raster\nsum = np.sum(array_ndv_fix)\n\n# Calculate the average value on &gt;0 cells\n\n## First create a binary map of where there is positive value\nnon_zero = np.where(array_ndv_fix &gt; 0, 1, 0) \n\n## Count those\nn_non_zero = np.sum(non_zero)\n\n## Calculate the average\nmean = sum / n_non_zero\n\n## Write the value to a file\nwith open(output_path, rw) as f: \n    print('Write here.')\n\nprint('Sums of layers: ' + str(mean))\n\n\n\nProjectFlow is a Python class that manages a project’s file structure and task tree. It is designed to be a drop-in solution for researchers who have a script that has grown too large and complex to manage. The ProjectFlow object is initialized with a directory, and it will create a file structure within that directory that is designed to be easy to navigate and understand. The ProjectFlow object also manages a task tree, which is a tree of tasks that need to be run in order to complete the project. The task tree is defined by the user, and the ProjectFlow object will manage the execution of the tasks, ensuring that they are run in the correct order and that any dependencies between tasks are satisfied.\nTo create a task tree NOT FINISHED. Just look at the example.py in docs/examples\n\n\n\n\nimport hazelbean as hb\n\nif __name__ == '__main__':\n    p = hb.ProjectFlow(r'C:\\Files\\Research\\cge\\gtap_invest\\projects\\feedback_policies_and_tipping_points')\nIn a multi-file setup, in the run.py you will need to import different scripts, such as main.py i.e.:\nimport visualizations.main\nThe script file mainpy can have whatever code, but in particular can include “task” functions. A task function, shown below, takes only p as an agrument and returns p (potentially modified). It also must have a conditional (if p.run_this:) to specify what always runs (and is assumed to run trivially fast, i.e., to specify file paths) just by nature of having it in the task tree and what is run only conditionally (based on the task.run attribute, or optionally based on satisfying a completed function.)\ndef example_task_function(p):\n    \"\"\"Fast function that creates several tiny geotiffs of gaussian-like kernels for later use in ffn_convolve.\"\"\"\n\n    if p.run_this:\n        for i in computationally_intensive_loop:\n            print(i)\nImportant Non-Obvious Note\nImporting the script will define function(s) to add “tasks”, which take the ProjectFlow object as an argument and returns it after potential modification.\ndef add_all_tasks_to_task_tree(p):\n    p.generated_kernels_task = p.add_task(example_task_function)\n\n\n\nIn many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:\n\nfor aggregation_label in p.aggregation_labels:\n     \n    for experiment_label in p.experiment_labels:\n        \n        for n_years_counter, ending_year in enumerate(p.years):\n            \n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))\nBut sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.\n\nfor index, row in p.scenarios_df.iterrows():\n    seals_utils.assign_df_row_to_object_attributes(p, row)\n    \n    if p.scenario_type != 'baseline':\n                            \n        for n_years_counter, ending_year in enumerate(p.years):\n\n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)\n            output_dir = p.get_path(current_run_dirs, str(ending_year))\n            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')\nIn this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via seals_utils.assign_df_row_to_object_attributes(p, row). This is used instead of the nested for loop.\n\n\n\nHere. Explain why it writes the scenarios_csv FROM CODE rather than downloading it (keeps it up to date as code changes quickly). However, this gets convoluted when you also have to initialize the attributes before you write?!?\n\n    # If you want to run SEALS with the run.py file in a different directory (ie in the project dir)\n    # then you need to add the path to the seals directory to the system path.\n    custom_seals_path = None\n    if custom_seals_path is not None: # G:/My Drive/Files/Research/seals/seals_dev/seals\n        sys.path.insert(0, custom_seals_path)\n\n    # SEALS will run based on the scenarios defined in a scenario_definitions.csv\n    # If you have not run SEALS before, SEALS will generate it in your project's input_dir.\n    # A useful way to get started is to to run SEALS on the test data without modification\n    # and then edit the scenario_definitions.csv to your project needs.\n    # Some of the other test files use different scenario definition csvs \n    # to illustrate the technique. If you point to one of these \n    # (or any one CSV that already exists), SEALS will not generate a new one.\n    # The avalable example files in the default_inputs include:\n    # - test_three_scenario_defininitions.csv\n    # - test_scenario_defininitions_multi_coeffs.csvs\n    \n    p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n    # Set defaults and generate the scenario_definitions.csv if it doesn't exist.\n    if not hb.path_exists(p.scenario_definitions_path):\n        # There are several possibilities for what you might want to set as the default.\n        # Choose accordingly by uncommenting your desired one. The set of\n        # supported options are\n        # - set_attributes_to_dynamic_default (primary one)\n        # - set_attributes_to_dynamic_many_year_default\n        # - set_attributes_to_default # Deprecated\n\n        gtap_invest_utils.set_attributes_to_dynamic_gtap_default(p) # Default option\n\n\n        # # Optional overrides for us in intitla scenarios\n        # p.aoi = 'RWA'\n\n        # gtap_invest_utils.set_attributes_to_dynamic_default(p)\n        # Once the attributes are set, generate the scenarios csv and put it in the input_dir.\n        gtap_invest_utils.generate_gtap_invest_scenarios_csv_and_put_in_input_dir(p)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n    else:\n        # Read in the scenarios csv and assign the first row to the attributes of this object (in order to setup additional \n        # project attributes like the resolutions of the fine scale and coarse scale data)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n\n        # Because we've only read the scenarios file, set the attributes\n        # to what is in the first row.\n        for index, row in p.scenarios_df.iterrows():\n            seals_utils.assign_df_row_to_object_attributes(p, row)\n            break # Just get first for initialization.\n\n\n\nProject flow requires a consistent format for tasks. The following is an example of a task that creates a correspondence file from gtap11 regions to gtapaez11 regions. The task itself defined as a function that takes a p object as an argument. This p object is a ProjectFlow object that contains all the project-level variables, manages folders and files, and manages tasks and parallelization. p also includes documentation, which will be written directly into the task directory.\nAlso note that any project-level attribute defined in between the function start and the if p.run_this: component are the “project level variables” that are fair-game for use in other tasks. These paths are critical for high performance because they enable quick-skipping of completed tasks and determiniation of which parts of the task tree need rerunning.\nTasks should be named as a noun (this breaks Python pep8 style) referencing what will be stored in the tasks output dir. This might feel awkward at first, but it means that the resultant file structure is easier to interpret by a non-EE outsider.\n\ndef gtap_aez_seals_correspondences(p):\n    p.current_task_documentation = \"\"\"\n    Create correspondence CSVs from ISO3 countries to GTAPv11 160\n    regions, and then to gtapaezv11 50ish regions, also put the classification\n    for seals simplification and luh.  \n    \"\"\"\n    p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')\n    p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')\n    p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')    \n\n    if p.run_this:\n        \n        \"logic here\"\n\n\n\nHazelbean automatically defines directory organization as a function of the task tree. When the ProjectFlow object is created, it takes a directory as its only required input. This directory defines the root of the project. The other directory that needs to be referenced is the base_data_dir. When you initialize the p object, it notes this:\nCreated ProjectFlow object at C:\\Users\\jajohns\\Files\\gtap_invest\\projects\\cwon     from script C:\\Users\\jajohns\\Files\\gtap_invest\\gtap_invest_dev\\gtap_invest\\run_cwon.py     with base_data set at C:\\Users\\jajohns\\Files/base_data\nIn the run file, the following line generates the task tree:\ngtap_invest_initialize_project.build_extract_and_run_aez_seals_task_tree(p)\nWhich points to a builder function in the initialize file, looking something like this:\n\nThis would generate the following task tree:\n\nTwo notations are especially useful within this task tree.\n\nWithin the function that defines a task, p.cur_dir points to the directory of that task. So for instance, the last task defined in the image above, in its code, you could reference p.cur_dir, and it would point to &lt;project_root&gt;/econ_visualization/econ_lcovercom\nOutside of a given function’s code, you can still refer to paths that were defined from within the functions code, but now (because you are outside the function) it is given a new reference. Using the example above, you could reference the same directory with p.econ_lcovercom_dir where the p attribute is named exactly as &lt;function_name&gt;_dir\n\nAll of this setup enable another useful feature: automatic management of file generation, storage and downloading. This is done via the hazelbean function:\n\nuseful_path = hb.get_path(relative_path)\nThis function will iteratively search multiple locations and return the most “useful” one. By default, the relative_path variable will first joined with the p.cur_dir. If the file exists, it returns it. If not, it checks the next location, which is p.input_dir, and then p.base_data_dir. If it doesn’t find it anywhere, it will attempt to download it from google cloud (NYI) and save it in the p.cur_dir. If it is not available to download on google cloud, then it treats the path as something we will be generating within the task, and thus, get_path returns the first option above, namely joining the relative_path with p.cur_dir.\nOne important use-case that needs explaining is for tasks that generate files that will eventually be placed in the base_data_dir. The goal is to enable easy generation of it to the intermediate directory in the appropriate task_dir, but then have the ability to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches the desired directory relative to the base data dir. So, for example, we include 'gtappy', 'aggregation_mappings' at the beginning of the relative path for in the intermediate directory in the appropriate task_dir, but then we also will want to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches that in the base_data_dir, for example &lt;base_data_dir&gt;/'gtappy/aggregation_mappings/gadm_adm0.gpkg',\n\ntemplate_path = p.get_path(os.path.join('gtappy', 'aggregation_mappings', 'gadm_adm0.gpkg')) \nIt can be hard deciding what counts as a base_data_generating task or not, but generally if it is a file that will not be used by other projects, you should not treat it as a base_data_generating task. Instead, you should just make it relative to the cur_dir (or wahtever makes sense), as below:\n\noutput_path = p.get_path(os.path.join(aggregation_label + '_' + experiment_label + '_' + header + '_stacked_time_series.csv'))\nOne additional exception to the above is if you are calling get_path outside of a task/task_tree. One common example is in the run file before you build the task tree. In this case, the default_dirs will not make sense, and so you need to specify it manually as here:\n\np.countries_iso3_path = p.get_path(os.path.join('cartographic', 'gadm', 'gadm_adm0_10sec.gpkg'), possible_dirs=[p.input_dir, p.base_data_dir])\n\n\n\nProjectFlow is designed to calculate very fast while simultaneously validating that everything is approximately correct. It does this by checking for the existence of files (often combined with hb.get_path()). For example\n\np.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'gadm_r263_gtapv7_r251_r160_r50_regions.gpkg'))     \nif not hb.path_exists(p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path):         \n    hb.log('Creating ' + p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path)         \n    \"computationally expensive thing here.\"\nProjectFlow very carefully defines whether or not you should run something based on the existence of specific paths. Usually this is just checking for each written path and only executing the code if it’s missing, but in some cases where lots of files are created, it’s possible to take the shortcut of just checking for the existence of the last-created path.\n\n\nIf you have a time consuming task that, or example, writes to\n\nbig_file_path = hb.get_path('lulc', 'esa', 'seals7', 'convolutions', '2017', 'convolution_esa_seals7_2017_cropland_gaussian_5.tif' )\nIn this example, suppose you needed to create this file via your create_convolutions() task or something. When you first do this, it obviously won’t exist yet, so get_path() will join that relative path in the p.cur_dir location. If you run the ProjectFlow again, it will see it’s there and then instantly skip recalcualting it.\nIn addition to the 5 repos plus the EE repo, there is a managed base data set stored in teh same location\n\nA ProjectFlow object must have a base_data_dir set (I think…). This is because the p.get_path() will look in this folder for it, and/or will download to it.\n\n\n\n\nFor large files that take a long time to load, use a string-&gt;dataframe/dataset substitution as below. Make a LOCAL variable to contain the loaded object, and have that be assigned to the correct project-level path string. In subsequent usages, check type and if it’s still a string, then it hasn’t been loaded yet, so do that. I’m debating making it a project level variable trick\n\ngadm = p.gadm_adm0_vector_input_path    \n\n# Just load it on first pass\nif type(gadm) is str:\n    gadm = hb.read_vector(p.gadm_adm0_vector_input_path)\n\n\n\nHere we document the case where you’ve run a project which involved generating an output you want to use in a subsequent project.\nFor this example, we will consider the project IUCN, which among other things pre-processed some key data, like kbas.tif and star_threat.tif. There are large global rasters and it would slow down our script to run them again in each new project. Thus, we want to promote them to the base_data_dir. Looking at the folders in the IUCN project, we see a pyramidal output star_threat.tif. We want to promote this to the base_data_dir and use it in other project.\n\nOur goal now is to clone the IUCN project into a new project we can modify. First, let us create the new project directory called test_iucn_30by30. In that folder, also create a new folder called input. Here, you will copy over the scenario_definitions file used in the IUCN project, which was named scenario_definitions_iucn_all.csv. After copying, rename it something sensible in the new location like scenario_definitions_iucn_30by30.csv, as shown below.\n\nNow create a new Python run file. Go back to the code directory (Files/seals/seals_dev) and copy run_iucn.py in the same directory and rename it run_test_iucn_30by30.py. Note the word test her which means it is going to be a non-global, fast running script to prove the new code works. Eventually a new run file run_iucn_30by30.py will be created when we know it works. In the new file, rename project_name = 'test_iucn_30by30'. Also in the code, edit the line below to match the name of your new scenario_definitions file.\np.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions_iucn_30by30.csv')\nBy copying the run_iucn.py file, we have ensured that we load the set of tasks we built for that project, specifically:\nseals_initialize_project.build_iucn_task_tree(p)\nBesides these two changes, the code will be identical to the standard SEALS run, except for the extra IUCN-specific tasks. You can inspect that function to see the logic of an IUCN-style SEALS run:\ndef build_iucn_task_tree(p):\n\n    # Define the project AOI\n    p.project_aoi_task = p.add_task(project_aoi)\n    \n    # Process the IUCN-specific data to be used in SEALS\n    p.biodiversity_task = p.add_task(seals_generate_base_data.biodiversity)\n    p.kbas_task = p.add_task(seals_generate_base_data.kba, parent=p.biodiversity_task)\n    p.star_task = p.add_task(seals_generate_base_data.star, parent=p.biodiversity_task)\n\n    ##### FINE PROCESSED INPUTS #####    \n\n    # Make folder for all generated data.\n    p.fine_processed_inputs_task = p.add_task(seals_generate_base_data.fine_processed_inputs)\n\n    # Generate some simple gaussian kernels for later convolutions\n    p.generated_kernels_task = p.add_task(seals_generate_base_data.generated_kernels, parent=p.fine_processed_inputs_task, creates_dir=False)\n\n    # Clip the fine LULC to the project AOI\n    p.lulc_clip_task = p.add_task(seals_generate_base_data.lulc_clip, parent=p.fine_processed_inputs_task, creates_dir=False)\nWithout any other modifications, run this new run file. It will (intentionally) fail and give an error message like this:\n\nWe’re going to use this example to learn about the debugger, the “call stack”, and ALSO what’s going on with our new project. First, look in the upper left of the image at the call stack. When you run in debug mode and it hits an exception, it will report the error message (red text) but also will freeze execution at the point it stopped and report the exact set of (potentially nested) functions that were called right before it stopped. These nested functions are the call stack. You can click on any one to see the code corresponding to different layers of what was called. The topmost function is often the most relevant one to what went wrong (like in this case get_geotransform_path() didn’t find the path it was given) but sometimes a deeper function gives you more information about what’s actually wrong.\nWith this information, we can understand what’s going on with our new project. We see in the error message that it is not finding star_threat_input.tif in the project’s input directory. This is desired because our point of this exercise is to NOT have to reprocess that input (and instead have our new project use the output of the task that would have called this). One would be tempted at this point to simply change the task tree to not call this function, but we actually do want this project to have access to this file, and moreover, we want it to work whether or not someone has generated the file. ProjectFlow is designed to solve this type of situation because any time a computationally-intensive function is called, it will check if the output has already been generated (and thus will skip the computation). To make this logic work across projects, we simply move the outputs of the task to our base data dir. Specifically, copy the file referenced, along with its containing folders relative to the intermediate directory and place those files/folders in the base data. If your tasks were defined correctly, these extra directories also define the canonical file location in the base data (and also the remote server for when we get to that!).\nTo do this, we need to figure out what file our code was actually looking to recreate. For this, we actually need to use the call stack to go one level deeper. Click on the star element in the call stack. Here we will see the following code which led to the actual error.\ndef star(p):\n    star_threat_input_path = p.get_path('biodiversity', 'star', 'star_threat_input.tif')\n    star_threat_path = p.get_path('star_threat.tif') \n    \n    if not hb.path_exists(star_threat_path):\n        # WTF apparently this was all shifted by a random amount.?\n        shift = 90.0 - 83.6361111111106226\n        \n        \n        geotransform_input = hb.get_geotransform_path(star_threat_input_path)\n        geotransform = list(geotransform_input)\n        \n        # Write a new file that corrects the weird shift without modifying the original file.\n        star_threat_shifted_path = p.get_path('star_threat_shifted.tif')\n        \n        if geotransform[3] == 90.0:\n            geotransform[3] -= shift        \n            if not hb.path_exists(star_threat_shifted_path):\n                hb.set_geotransform_to_tuple(star_threat_input_path, geotransform, output_path=star_threat_shifted_path)\n        \n        # INTERSTING IMPLICATION: Because the cur_dir is already representing the base_data nesting location, adding it again doesn't work makes it be repeated 2x\n        star_threat_padded_path = p.get_path('star_threat_padded.tif')\n        if not hb.path_exists(star_threat_padded_path):\n            hb.fill_to_match_extent(star_threat_shifted_path, p.aoi_ha_per_cell_fine_path, output_path=star_threat_padded_path, fill_value=-9999., remove_temporary_files=False)\n        \n        star_threat_path = p.get_path('star_threat.tif') \n        if not hb.path_exists(star_threat_path):\n            hb.make_path_global_pyramid(star_threat_padded_path, output_path=star_threat_path, verbose=True)\nIn the above, we see several things. First, the p.get_path() function defines the same set of “indexed data” directories you saw above pointing to the input raster. We also see the expected output file, defined in:\nstar_threat_path = p.get_path('star_threat.tif')\nThis get_path() function doesn’t define any indexed data directories because it is defined by the task tree! Specifically, p.cur_dir will already be defined to be in the ‘biodiversity/star’ subdirectories. We can see this in the task_tree printout.\n\nIn ProjectFlow, all computationally intensive tasks must be inside of a check to see if it’s already been run (and sometimes, that the output validates). This happens in this task because everything besides just defining file paths is inside of following conditional:\nif not hb.path_exists(star_threat_path):\nRegardless, we can see that this file, star_threat.tif will be (one of) the outputs of this task. This is the one that we want to move into the base data so future functions can find it. Go into the project’s intermediate directory and navigate to the right task. Here we see the file we want to promote. Copy this file, along with all of the indexed folders (in this case, ‘biodiversity/star’) into the root of the base_data_dir. This means relative to the two different file roots of base_data_dir and intermediate_dir will have the same relative file structure thereafter. This can be seen in the image below which shows the file copied into base_data. Notice that I didn’t copy any of the intermediate files produced by the task.\n\nNow, with this file in place, you can rerun the run_test_iucn_30by30.py file and the p.get_path() functions will find the desired file when it is checking in the base_data! There will be a new directory created for the task at intermediate/biodiversity/star but that will be empty. The project should run much faster now by not having to create the file while still ensuring the user has access to it.\nNow that you have a new working project. Your likely next step is to create a new task tree builder function, perhaps named build_test_iucn_30by30_task_tree(p). This function will define the tasks that are unique to this project. You can copy the tasks from the IUCN project and modify them as needed. You can also add new tasks that are unique to this project.",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#starter-code",
    "href": "earth_economy_devstack/project_flow.html#starter-code",
    "title": "Project Flow",
    "section": "",
    "text": "We will use the third level of complexity from the previous example as the launching point for how to use ProjectFlow. The code below adds a few additional steps we will work with.\nimport os\nimport numpy as np\nimport gdal\n\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# Load the raster\nraster_input_path = 'data/yield.tif' # TODOO[make all these actually point to data in hazelbean]\narray = raster_to_array(raster_input_path)\n\n# Set no data values equal to zero\n\n# Method 1 (creates new array)\narray_ndv_fix = np.where(array == -9999, 0, array)\n\n# Method 2 (inplace)\narray[array == -9999] = 0\n\n# Sum the raster\nsum = np.sum(array_ndv_fix)\n\n# Calculate the average value on &gt;0 cells\n\n## First create a binary map of where there is positive value\nnon_zero = np.where(array_ndv_fix &gt; 0, 1, 0) \n\n## Count those\nn_non_zero = np.sum(non_zero)\n\n## Calculate the average\nmean = sum / n_non_zero\n\n## Write the value to a file\nwith open(output_path, rw) as f: \n    print('Write here.')\n\nprint('Sums of layers: ' + str(mean))",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#what-is-the-projectflow-and-the-task-tree",
    "href": "earth_economy_devstack/project_flow.html#what-is-the-projectflow-and-the-task-tree",
    "title": "Project Flow",
    "section": "",
    "text": "ProjectFlow is a Python class that manages a project’s file structure and task tree. It is designed to be a drop-in solution for researchers who have a script that has grown too large and complex to manage. The ProjectFlow object is initialized with a directory, and it will create a file structure within that directory that is designed to be easy to navigate and understand. The ProjectFlow object also manages a task tree, which is a tree of tasks that need to be run in order to complete the project. The task tree is defined by the user, and the ProjectFlow object will manage the execution of the tasks, ensuring that they are run in the correct order and that any dependencies between tasks are satisfied.\nTo create a task tree NOT FINISHED. Just look at the example.py in docs/examples",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#step-1-substitute-in-a-few-pre-built-hazelbean-functions",
    "href": "earth_economy_devstack/project_flow.html#step-1-substitute-in-a-few-pre-built-hazelbean-functions",
    "title": "Project Flow",
    "section": "",
    "text": "import hazelbean as hb\n\nif __name__ == '__main__':\n    p = hb.ProjectFlow(r'C:\\Files\\Research\\cge\\gtap_invest\\projects\\feedback_policies_and_tipping_points')\nIn a multi-file setup, in the run.py you will need to import different scripts, such as main.py i.e.:\nimport visualizations.main\nThe script file mainpy can have whatever code, but in particular can include “task” functions. A task function, shown below, takes only p as an agrument and returns p (potentially modified). It also must have a conditional (if p.run_this:) to specify what always runs (and is assumed to run trivially fast, i.e., to specify file paths) just by nature of having it in the task tree and what is run only conditionally (based on the task.run attribute, or optionally based on satisfying a completed function.)\ndef example_task_function(p):\n    \"\"\"Fast function that creates several tiny geotiffs of gaussian-like kernels for later use in ffn_convolve.\"\"\"\n\n    if p.run_this:\n        for i in computationally_intensive_loop:\n            print(i)\nImportant Non-Obvious Note\nImporting the script will define function(s) to add “tasks”, which take the ProjectFlow object as an argument and returns it after potential modification.\ndef add_all_tasks_to_task_tree(p):\n    p.generated_kernels_task = p.add_task(example_task_function)",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#iterating-over-many-model-assumptions",
    "href": "earth_economy_devstack/project_flow.html#iterating-over-many-model-assumptions",
    "title": "Project Flow",
    "section": "",
    "text": "In many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:\n\nfor aggregation_label in p.aggregation_labels:\n     \n    for experiment_label in p.experiment_labels:\n        \n        for n_years_counter, ending_year in enumerate(p.years):\n            \n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))\nBut sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.\n\nfor index, row in p.scenarios_df.iterrows():\n    seals_utils.assign_df_row_to_object_attributes(p, row)\n    \n    if p.scenario_type != 'baseline':\n                            \n        for n_years_counter, ending_year in enumerate(p.years):\n\n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)\n            output_dir = p.get_path(current_run_dirs, str(ending_year))\n            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')\nIn this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via seals_utils.assign_df_row_to_object_attributes(p, row). This is used instead of the nested for loop.",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#creating-scenarios-spreadsheets",
    "href": "earth_economy_devstack/project_flow.html#creating-scenarios-spreadsheets",
    "title": "Project Flow",
    "section": "",
    "text": "Here. Explain why it writes the scenarios_csv FROM CODE rather than downloading it (keeps it up to date as code changes quickly). However, this gets convoluted when you also have to initialize the attributes before you write?!?\n\n    # If you want to run SEALS with the run.py file in a different directory (ie in the project dir)\n    # then you need to add the path to the seals directory to the system path.\n    custom_seals_path = None\n    if custom_seals_path is not None: # G:/My Drive/Files/Research/seals/seals_dev/seals\n        sys.path.insert(0, custom_seals_path)\n\n    # SEALS will run based on the scenarios defined in a scenario_definitions.csv\n    # If you have not run SEALS before, SEALS will generate it in your project's input_dir.\n    # A useful way to get started is to to run SEALS on the test data without modification\n    # and then edit the scenario_definitions.csv to your project needs.\n    # Some of the other test files use different scenario definition csvs \n    # to illustrate the technique. If you point to one of these \n    # (or any one CSV that already exists), SEALS will not generate a new one.\n    # The avalable example files in the default_inputs include:\n    # - test_three_scenario_defininitions.csv\n    # - test_scenario_defininitions_multi_coeffs.csvs\n    \n    p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n    # Set defaults and generate the scenario_definitions.csv if it doesn't exist.\n    if not hb.path_exists(p.scenario_definitions_path):\n        # There are several possibilities for what you might want to set as the default.\n        # Choose accordingly by uncommenting your desired one. The set of\n        # supported options are\n        # - set_attributes_to_dynamic_default (primary one)\n        # - set_attributes_to_dynamic_many_year_default\n        # - set_attributes_to_default # Deprecated\n\n        gtap_invest_utils.set_attributes_to_dynamic_gtap_default(p) # Default option\n\n\n        # # Optional overrides for us in intitla scenarios\n        # p.aoi = 'RWA'\n\n        # gtap_invest_utils.set_attributes_to_dynamic_default(p)\n        # Once the attributes are set, generate the scenarios csv and put it in the input_dir.\n        gtap_invest_utils.generate_gtap_invest_scenarios_csv_and_put_in_input_dir(p)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n    else:\n        # Read in the scenarios csv and assign the first row to the attributes of this object (in order to setup additional \n        # project attributes like the resolutions of the fine scale and coarse scale data)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n\n        # Because we've only read the scenarios file, set the attributes\n        # to what is in the first row.\n        for index, row in p.scenarios_df.iterrows():\n            seals_utils.assign_df_row_to_object_attributes(p, row)\n            break # Just get first for initialization.",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#task-format",
    "href": "earth_economy_devstack/project_flow.html#task-format",
    "title": "Project Flow",
    "section": "",
    "text": "Project flow requires a consistent format for tasks. The following is an example of a task that creates a correspondence file from gtap11 regions to gtapaez11 regions. The task itself defined as a function that takes a p object as an argument. This p object is a ProjectFlow object that contains all the project-level variables, manages folders and files, and manages tasks and parallelization. p also includes documentation, which will be written directly into the task directory.\nAlso note that any project-level attribute defined in between the function start and the if p.run_this: component are the “project level variables” that are fair-game for use in other tasks. These paths are critical for high performance because they enable quick-skipping of completed tasks and determiniation of which parts of the task tree need rerunning.\nTasks should be named as a noun (this breaks Python pep8 style) referencing what will be stored in the tasks output dir. This might feel awkward at first, but it means that the resultant file structure is easier to interpret by a non-EE outsider.\n\ndef gtap_aez_seals_correspondences(p):\n    p.current_task_documentation = \"\"\"\n    Create correspondence CSVs from ISO3 countries to GTAPv11 160\n    regions, and then to gtapaezv11 50ish regions, also put the classification\n    for seals simplification and luh.  \n    \"\"\"\n    p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')\n    p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')\n    p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')    \n\n    if p.run_this:\n        \n        \"logic here\"",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#automatic-directory-organization-via-tasks",
    "href": "earth_economy_devstack/project_flow.html#automatic-directory-organization-via-tasks",
    "title": "Project Flow",
    "section": "",
    "text": "Hazelbean automatically defines directory organization as a function of the task tree. When the ProjectFlow object is created, it takes a directory as its only required input. This directory defines the root of the project. The other directory that needs to be referenced is the base_data_dir. When you initialize the p object, it notes this:\nCreated ProjectFlow object at C:\\Users\\jajohns\\Files\\gtap_invest\\projects\\cwon     from script C:\\Users\\jajohns\\Files\\gtap_invest\\gtap_invest_dev\\gtap_invest\\run_cwon.py     with base_data set at C:\\Users\\jajohns\\Files/base_data\nIn the run file, the following line generates the task tree:\ngtap_invest_initialize_project.build_extract_and_run_aez_seals_task_tree(p)\nWhich points to a builder function in the initialize file, looking something like this:\n\nThis would generate the following task tree:\n\nTwo notations are especially useful within this task tree.\n\nWithin the function that defines a task, p.cur_dir points to the directory of that task. So for instance, the last task defined in the image above, in its code, you could reference p.cur_dir, and it would point to &lt;project_root&gt;/econ_visualization/econ_lcovercom\nOutside of a given function’s code, you can still refer to paths that were defined from within the functions code, but now (because you are outside the function) it is given a new reference. Using the example above, you could reference the same directory with p.econ_lcovercom_dir where the p attribute is named exactly as &lt;function_name&gt;_dir\n\nAll of this setup enable another useful feature: automatic management of file generation, storage and downloading. This is done via the hazelbean function:\n\nuseful_path = hb.get_path(relative_path)\nThis function will iteratively search multiple locations and return the most “useful” one. By default, the relative_path variable will first joined with the p.cur_dir. If the file exists, it returns it. If not, it checks the next location, which is p.input_dir, and then p.base_data_dir. If it doesn’t find it anywhere, it will attempt to download it from google cloud (NYI) and save it in the p.cur_dir. If it is not available to download on google cloud, then it treats the path as something we will be generating within the task, and thus, get_path returns the first option above, namely joining the relative_path with p.cur_dir.\nOne important use-case that needs explaining is for tasks that generate files that will eventually be placed in the base_data_dir. The goal is to enable easy generation of it to the intermediate directory in the appropriate task_dir, but then have the ability to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches the desired directory relative to the base data dir. So, for example, we include 'gtappy', 'aggregation_mappings' at the beginning of the relative path for in the intermediate directory in the appropriate task_dir, but then we also will want to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches that in the base_data_dir, for example &lt;base_data_dir&gt;/'gtappy/aggregation_mappings/gadm_adm0.gpkg',\n\ntemplate_path = p.get_path(os.path.join('gtappy', 'aggregation_mappings', 'gadm_adm0.gpkg')) \nIt can be hard deciding what counts as a base_data_generating task or not, but generally if it is a file that will not be used by other projects, you should not treat it as a base_data_generating task. Instead, you should just make it relative to the cur_dir (or wahtever makes sense), as below:\n\noutput_path = p.get_path(os.path.join(aggregation_label + '_' + experiment_label + '_' + header + '_stacked_time_series.csv'))\nOne additional exception to the above is if you are calling get_path outside of a task/task_tree. One common example is in the run file before you build the task tree. In this case, the default_dirs will not make sense, and so you need to specify it manually as here:\n\np.countries_iso3_path = p.get_path(os.path.join('cartographic', 'gadm', 'gadm_adm0_10sec.gpkg'), possible_dirs=[p.input_dir, p.base_data_dir])",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_flow.html#validation-of-files",
    "href": "earth_economy_devstack/project_flow.html#validation-of-files",
    "title": "Project Flow",
    "section": "",
    "text": "ProjectFlow is designed to calculate very fast while simultaneously validating that everything is approximately correct. It does this by checking for the existence of files (often combined with hb.get_path()). For example\n\np.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'gadm_r263_gtapv7_r251_r160_r50_regions.gpkg'))     \nif not hb.path_exists(p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path):         \n    hb.log('Creating ' + p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path)         \n    \"computationally expensive thing here.\"\nProjectFlow very carefully defines whether or not you should run something based on the existence of specific paths. Usually this is just checking for each written path and only executing the code if it’s missing, but in some cases where lots of files are created, it’s possible to take the shortcut of just checking for the existence of the last-created path.\n\n\nIf you have a time consuming task that, or example, writes to\n\nbig_file_path = hb.get_path('lulc', 'esa', 'seals7', 'convolutions', '2017', 'convolution_esa_seals7_2017_cropland_gaussian_5.tif' )\nIn this example, suppose you needed to create this file via your create_convolutions() task or something. When you first do this, it obviously won’t exist yet, so get_path() will join that relative path in the p.cur_dir location. If you run the ProjectFlow again, it will see it’s there and then instantly skip recalcualting it.\nIn addition to the 5 repos plus the EE repo, there is a managed base data set stored in teh same location\n\nA ProjectFlow object must have a base_data_dir set (I think…). This is because the p.get_path() will look in this folder for it, and/or will download to it.\n\n\n\n\nFor large files that take a long time to load, use a string-&gt;dataframe/dataset substitution as below. Make a LOCAL variable to contain the loaded object, and have that be assigned to the correct project-level path string. In subsequent usages, check type and if it’s still a string, then it hasn’t been loaded yet, so do that. I’m debating making it a project level variable trick\n\ngadm = p.gadm_adm0_vector_input_path    \n\n# Just load it on first pass\nif type(gadm) is str:\n    gadm = hb.read_vector(p.gadm_adm0_vector_input_path)\n\n\n\nHere we document the case where you’ve run a project which involved generating an output you want to use in a subsequent project.\nFor this example, we will consider the project IUCN, which among other things pre-processed some key data, like kbas.tif and star_threat.tif. There are large global rasters and it would slow down our script to run them again in each new project. Thus, we want to promote them to the base_data_dir. Looking at the folders in the IUCN project, we see a pyramidal output star_threat.tif. We want to promote this to the base_data_dir and use it in other project.\n\nOur goal now is to clone the IUCN project into a new project we can modify. First, let us create the new project directory called test_iucn_30by30. In that folder, also create a new folder called input. Here, you will copy over the scenario_definitions file used in the IUCN project, which was named scenario_definitions_iucn_all.csv. After copying, rename it something sensible in the new location like scenario_definitions_iucn_30by30.csv, as shown below.\n\nNow create a new Python run file. Go back to the code directory (Files/seals/seals_dev) and copy run_iucn.py in the same directory and rename it run_test_iucn_30by30.py. Note the word test her which means it is going to be a non-global, fast running script to prove the new code works. Eventually a new run file run_iucn_30by30.py will be created when we know it works. In the new file, rename project_name = 'test_iucn_30by30'. Also in the code, edit the line below to match the name of your new scenario_definitions file.\np.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions_iucn_30by30.csv')\nBy copying the run_iucn.py file, we have ensured that we load the set of tasks we built for that project, specifically:\nseals_initialize_project.build_iucn_task_tree(p)\nBesides these two changes, the code will be identical to the standard SEALS run, except for the extra IUCN-specific tasks. You can inspect that function to see the logic of an IUCN-style SEALS run:\ndef build_iucn_task_tree(p):\n\n    # Define the project AOI\n    p.project_aoi_task = p.add_task(project_aoi)\n    \n    # Process the IUCN-specific data to be used in SEALS\n    p.biodiversity_task = p.add_task(seals_generate_base_data.biodiversity)\n    p.kbas_task = p.add_task(seals_generate_base_data.kba, parent=p.biodiversity_task)\n    p.star_task = p.add_task(seals_generate_base_data.star, parent=p.biodiversity_task)\n\n    ##### FINE PROCESSED INPUTS #####    \n\n    # Make folder for all generated data.\n    p.fine_processed_inputs_task = p.add_task(seals_generate_base_data.fine_processed_inputs)\n\n    # Generate some simple gaussian kernels for later convolutions\n    p.generated_kernels_task = p.add_task(seals_generate_base_data.generated_kernels, parent=p.fine_processed_inputs_task, creates_dir=False)\n\n    # Clip the fine LULC to the project AOI\n    p.lulc_clip_task = p.add_task(seals_generate_base_data.lulc_clip, parent=p.fine_processed_inputs_task, creates_dir=False)\nWithout any other modifications, run this new run file. It will (intentionally) fail and give an error message like this:\n\nWe’re going to use this example to learn about the debugger, the “call stack”, and ALSO what’s going on with our new project. First, look in the upper left of the image at the call stack. When you run in debug mode and it hits an exception, it will report the error message (red text) but also will freeze execution at the point it stopped and report the exact set of (potentially nested) functions that were called right before it stopped. These nested functions are the call stack. You can click on any one to see the code corresponding to different layers of what was called. The topmost function is often the most relevant one to what went wrong (like in this case get_geotransform_path() didn’t find the path it was given) but sometimes a deeper function gives you more information about what’s actually wrong.\nWith this information, we can understand what’s going on with our new project. We see in the error message that it is not finding star_threat_input.tif in the project’s input directory. This is desired because our point of this exercise is to NOT have to reprocess that input (and instead have our new project use the output of the task that would have called this). One would be tempted at this point to simply change the task tree to not call this function, but we actually do want this project to have access to this file, and moreover, we want it to work whether or not someone has generated the file. ProjectFlow is designed to solve this type of situation because any time a computationally-intensive function is called, it will check if the output has already been generated (and thus will skip the computation). To make this logic work across projects, we simply move the outputs of the task to our base data dir. Specifically, copy the file referenced, along with its containing folders relative to the intermediate directory and place those files/folders in the base data. If your tasks were defined correctly, these extra directories also define the canonical file location in the base data (and also the remote server for when we get to that!).\nTo do this, we need to figure out what file our code was actually looking to recreate. For this, we actually need to use the call stack to go one level deeper. Click on the star element in the call stack. Here we will see the following code which led to the actual error.\ndef star(p):\n    star_threat_input_path = p.get_path('biodiversity', 'star', 'star_threat_input.tif')\n    star_threat_path = p.get_path('star_threat.tif') \n    \n    if not hb.path_exists(star_threat_path):\n        # WTF apparently this was all shifted by a random amount.?\n        shift = 90.0 - 83.6361111111106226\n        \n        \n        geotransform_input = hb.get_geotransform_path(star_threat_input_path)\n        geotransform = list(geotransform_input)\n        \n        # Write a new file that corrects the weird shift without modifying the original file.\n        star_threat_shifted_path = p.get_path('star_threat_shifted.tif')\n        \n        if geotransform[3] == 90.0:\n            geotransform[3] -= shift        \n            if not hb.path_exists(star_threat_shifted_path):\n                hb.set_geotransform_to_tuple(star_threat_input_path, geotransform, output_path=star_threat_shifted_path)\n        \n        # INTERSTING IMPLICATION: Because the cur_dir is already representing the base_data nesting location, adding it again doesn't work makes it be repeated 2x\n        star_threat_padded_path = p.get_path('star_threat_padded.tif')\n        if not hb.path_exists(star_threat_padded_path):\n            hb.fill_to_match_extent(star_threat_shifted_path, p.aoi_ha_per_cell_fine_path, output_path=star_threat_padded_path, fill_value=-9999., remove_temporary_files=False)\n        \n        star_threat_path = p.get_path('star_threat.tif') \n        if not hb.path_exists(star_threat_path):\n            hb.make_path_global_pyramid(star_threat_padded_path, output_path=star_threat_path, verbose=True)\nIn the above, we see several things. First, the p.get_path() function defines the same set of “indexed data” directories you saw above pointing to the input raster. We also see the expected output file, defined in:\nstar_threat_path = p.get_path('star_threat.tif')\nThis get_path() function doesn’t define any indexed data directories because it is defined by the task tree! Specifically, p.cur_dir will already be defined to be in the ‘biodiversity/star’ subdirectories. We can see this in the task_tree printout.\n\nIn ProjectFlow, all computationally intensive tasks must be inside of a check to see if it’s already been run (and sometimes, that the output validates). This happens in this task because everything besides just defining file paths is inside of following conditional:\nif not hb.path_exists(star_threat_path):\nRegardless, we can see that this file, star_threat.tif will be (one of) the outputs of this task. This is the one that we want to move into the base data so future functions can find it. Go into the project’s intermediate directory and navigate to the right task. Here we see the file we want to promote. Copy this file, along with all of the indexed folders (in this case, ‘biodiversity/star’) into the root of the base_data_dir. This means relative to the two different file roots of base_data_dir and intermediate_dir will have the same relative file structure thereafter. This can be seen in the image below which shows the file copied into base_data. Notice that I didn’t copy any of the intermediate files produced by the task.\n\nNow, with this file in place, you can rerun the run_test_iucn_30by30.py file and the p.get_path() functions will find the desired file when it is checking in the base_data! There will be a new directory created for the task at intermediate/biodiversity/star but that will be empty. The project should run much faster now by not having to create the file while still ensuring the user has access to it.\nNow that you have a new working project. Your likely next step is to create a new task tree builder function, perhaps named build_test_iucn_30by30_task_tree(p). This function will define the tasks that are unique to this project. You can copy the tasks from the IUCN project and modify them as needed. You can also add new tasks that are unique to this project.",
    "crumbs": [
      "Hazelbean",
      "ProjectFlow"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html",
    "href": "earth_economy_devstack/organization.html",
    "title": "Organization",
    "section": "",
    "text": "Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github.\n\n\n\nNext, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section).\n\n\n\nNavigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories.\n\n\n\n\nThe earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#getting-the-earth_economy_devstack-repository",
    "href": "earth_economy_devstack/organization.html#getting-the-earth_economy_devstack-repository",
    "title": "Organization",
    "section": "",
    "text": "Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#get-the-other-repositories",
    "href": "earth_economy_devstack/organization.html#get-the-other-repositories",
    "title": "Organization",
    "section": "",
    "text": "Next, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section).",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#launching-the-devstack-in-vs-code",
    "href": "earth_economy_devstack/organization.html#launching-the-devstack-in-vs-code",
    "title": "Organization",
    "section": "",
    "text": "Navigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/organization.html#launch-and-vs-code-configurations",
    "href": "earth_economy_devstack/organization.html#launch-and-vs-code-configurations",
    "title": "Organization",
    "section": "",
    "text": "The earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack.",
    "crumbs": [
      "Methods",
      "Organization"
    ]
  },
  {
    "objectID": "earth_economy_devstack/mergine_a_pr_into_main.html",
    "href": "earth_economy_devstack/mergine_a_pr_into_main.html",
    "title": "Steps",
    "section": "",
    "text": "Steps\n\nInstall Github Pull Requests VS Code extension\nIn the left-hand sidebar, click on the Github Pull Requests icon\n\n\n\nRight-click and select “Checkout Pull Request”\n\nNow your local files will be those of the PR.\nRun all relevant tests in the test suite."
  },
  {
    "objectID": "earth_economy_devstack/installation_walkthrough.html",
    "href": "earth_economy_devstack/installation_walkthrough.html",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "Note, this is slightly out of date. Please use the standard install if you get confused.\nThis installation is like above but organizes your repositories so that you can edit/contribute to them via a VS Code Workspace.\n\n\n\nInstall and run the installer for your operating system at https://Git-scm.com/downloads\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\nInstall just for your user account (rather than “all users”).\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nConda can be very slow at solving complex environments. The previous solution was to install mambaforge, which was much faster. Now, mamba is included in miniforge, so you can use mamba to install packages instead of conda. However, if the conda command is still unmodified, so you still have to explicitly call mamba.\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\n\n\n\n\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt;\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n\nInstall libraries using the following mamba command:\n\nmamba install hazelbean cython libgdal-hdf5\n\nInstall more with pip using the following command:\n\npip install winshell\n\n\n\n\n\n\nClone the Earth Economy devstack into the correct location\n\nCreate a directory for called “Files” at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\n\nOpen a terminal or command prompt and navigate to the Files directory you created\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\nThis will add a new folder C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the repositories in your workspace\n\n\n\n\n\nInstall required extensions\n\nInstall the Python extension in VS Code\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\nInstall Quarto extension in VS Code\n\nFor the documentation, I recommend installing the Quarto CLI https://quarto.org/docs/get-started\n\nInstall GitGraph extension in VS Code\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\n\n\n\n\n\nThe Earth Economy Devstack organizes repositories in a specific way, based in the Files directory discussed above\n\nThis ensures, among other things, that the Debug Launch configurations in the .vscode directory will use locally cloned repositories\n\nFor each new repository you want to add, for example Hazelbean, create a new directory in Files called hazelbean\n\nIn this new directory, Git clone the hazelbean_dev repo, which will create a directory hazelbean_dev inside hazelbean\n\nRepeat the previous step for all the repositories you want (assuming you have permissions, though most are public):\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\n\nThese repos do NOT need to be installed via pip because they are referenced in the debug launch.json\n\nHowever, this approach assumes you can compile the C/Cython files, so you must do the following\n\n\n\n\n\n\nInstall C/C++ compiler. This is necessary to edit the underlying C/Cython/.pyx files.\n\nIf you run any part of the code that uses compiled C++ code (mostly SEALS and the optimized raster functions in Hazelbean), then you will need to have a C++ compiler on your system.\n\nWindows:\n\nOption 1: You could go to https://visualstudio.microsoft.com/visual-cpp-build-tools/ and select download build tools.\nOption 2: Enter the following command in the Terminal: winget install Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;includeRecommended\" This will launch the build-tools installer (you could do this manually via the MS website if you want, but this ensures you get the right tools).\n\nMac can use Xcode to compile the cython files. Most users will already have this installed but if not, follow the directions below.\n\nXcode Command Line Tools: macOS users can install these tools by running xcode-select –install in the Terminal. This command downloads and installs the Xcode Command Line Tools, which includes gcc and clang, the compilers needed to compile C/C++ code on macOS. This is somewhat analogous to the Visual Studio Build Tools on Windows.\n\n\n\nRun the appropriate setup.py to trigger the Cython compilation\n\nMost libraries will auto-compile when you first import them (or change the underlying .pyx file)\nAlternatively, you can manually compile them by running the corresponding compile_cython_files.py in the repo’s\n\nNavigate to the scripts directory and use the terminal to run python compile_cython_files.py build_ext --inplace"
  },
  {
    "objectID": "earth_economy_devstack/installation_walkthrough.html#workspace-installation",
    "href": "earth_economy_devstack/installation_walkthrough.html#workspace-installation",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "Note, this is slightly out of date. Please use the standard install if you get confused.\nThis installation is like above but organizes your repositories so that you can edit/contribute to them via a VS Code Workspace.\n\n\n\nInstall and run the installer for your operating system at https://Git-scm.com/downloads\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\nInstall just for your user account (rather than “all users”).\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nConda can be very slow at solving complex environments. The previous solution was to install mambaforge, which was much faster. Now, mamba is included in miniforge, so you can use mamba to install packages instead of conda. However, if the conda command is still unmodified, so you still have to explicitly call mamba.\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\n\n\n\n\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt;\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n\nInstall libraries using the following mamba command:\n\nmamba install hazelbean cython libgdal-hdf5\n\nInstall more with pip using the following command:\n\npip install winshell\n\n\n\n\n\n\nClone the Earth Economy devstack into the correct location\n\nCreate a directory for called “Files” at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\n\nOpen a terminal or command prompt and navigate to the Files directory you created\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\nThis will add a new folder C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the repositories in your workspace\n\n\n\n\n\nInstall required extensions\n\nInstall the Python extension in VS Code\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\nInstall Quarto extension in VS Code\n\nFor the documentation, I recommend installing the Quarto CLI https://quarto.org/docs/get-started\n\nInstall GitGraph extension in VS Code\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\n\n\n\n\n\nThe Earth Economy Devstack organizes repositories in a specific way, based in the Files directory discussed above\n\nThis ensures, among other things, that the Debug Launch configurations in the .vscode directory will use locally cloned repositories\n\nFor each new repository you want to add, for example Hazelbean, create a new directory in Files called hazelbean\n\nIn this new directory, Git clone the hazelbean_dev repo, which will create a directory hazelbean_dev inside hazelbean\n\nRepeat the previous step for all the repositories you want (assuming you have permissions, though most are public):\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\n\nThese repos do NOT need to be installed via pip because they are referenced in the debug launch.json\n\nHowever, this approach assumes you can compile the C/Cython files, so you must do the following\n\n\n\n\n\n\nInstall C/C++ compiler. This is necessary to edit the underlying C/Cython/.pyx files.\n\nIf you run any part of the code that uses compiled C++ code (mostly SEALS and the optimized raster functions in Hazelbean), then you will need to have a C++ compiler on your system.\n\nWindows:\n\nOption 1: You could go to https://visualstudio.microsoft.com/visual-cpp-build-tools/ and select download build tools.\nOption 2: Enter the following command in the Terminal: winget install Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;includeRecommended\" This will launch the build-tools installer (you could do this manually via the MS website if you want, but this ensures you get the right tools).\n\nMac can use Xcode to compile the cython files. Most users will already have this installed but if not, follow the directions below.\n\nXcode Command Line Tools: macOS users can install these tools by running xcode-select –install in the Terminal. This command downloads and installs the Xcode Command Line Tools, which includes gcc and clang, the compilers needed to compile C/C++ code on macOS. This is somewhat analogous to the Visual Studio Build Tools on Windows.\n\n\n\nRun the appropriate setup.py to trigger the Cython compilation\n\nMost libraries will auto-compile when you first import them (or change the underlying .pyx file)\nAlternatively, you can manually compile them by running the corresponding compile_cython_files.py in the repo’s\n\nNavigate to the scripts directory and use the terminal to run python compile_cython_files.py build_ext --inplace"
  },
  {
    "objectID": "earth_economy_devstack/installation_walkthrough.html#workspace-step-by-step-installation",
    "href": "earth_economy_devstack/installation_walkthrough.html#workspace-step-by-step-installation",
    "title": "Justin Andrew Johnson",
    "section": "Workspace Step by Step Installation",
    "text": "Workspace Step by Step Installation\n\nInstall Git\n\nGet the git software\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\nGit vs. GitHub\n\nGithub is a website that hosts code and connects a community of coders.\nGit is a “version control” software tool that records the history of files in a Git repository.\nNearly every coder uses Git to push their repository of code to GitHub.\n\nUse the default options for everything\n\nUnless you REALLY know what you’re doing.\n\nConfigure your user name via the command prompt with the following commands\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email johndoe@example.com\n\n\n\nVerify Git Installation\n\nOpen up the command prompt and type git to test that it’s installed\n\n(PC) Search for cmd in the start menu\n\nThis is the OG way of working on computers\n\nAll version control tasks can be done via git commands here, but we will be using VS Code instead\n\n\n\n\nConfigure Windows Explorer (optional)\n\nMake hidden folders visible\nMake sure file extensions are visible\n\n\n\n\n\n\nCreate a python environment\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\n\n\n\nYou’ll know it worked if you see (base) in front of your path\n\nBase is the default “environment” that you will use.\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n- When you have lots of projects, most people create multiple environments specific to each project.  - For now, we’re going to install everything to the base environment\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n- You’ll know it worked if you see (env_name) in front of your path - You can deactivate the environment with `conda deactivate` - ![](images/2024-02-02-10-39-21.png) - You can list all environments with `conda env list`\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm sympy gekko\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\nThis step may take a long time because you are downloading and installing hundreds of libraries\n\n\n\nWhen you’re done, it should look like the image here.\n\n\n\nSuccess! You now have a modern scientific computing environment (sometimes called a scientific computing stack) on your computer!\n\n\n\nClone the Earth Economy Devstack\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\n\nBy default, your terminal will open in your user directory, so C:\\Users\\&lt;YOUR_USERNAME&gt;\nYou can navigate to the directory you created with the command cd Files\n\nSee image below.\n\n\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\nInstall Visual Studio Code (VS Code)\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\n\n\n\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\nYou could just open VS Code now, but we’re going to open it up with a specific Worspace Configuration file below\n\n\n\nOpen the Earth Economy Devstack in VS Code workspace\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\n\n\n\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\nWe’ll describe debugging more in future sections\n\nFor now, just know that these launch configurations make sure you’re using the repositories that we’ve added to your workspace\n\n\nAlso note that the other repositories shown in the VS Code file explorer will be empty until you git clone them (described below)\n\n\n\nConfigure VS CODE\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\nInstall required extensions\n\nInstall the Python extension\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\n\nInstall Quarto extension in VS Code\n\nWe use Quarto to create reports and documents on .qmd, .md and .ipynb files\n\nInstall the Quarto CLI https://quarto.org/docs/get-started\nYou can edit in source mode:\n\n\n\nOr you can press ctrl+shift+f4 to use the visual editor\n\n\n\n\n\nInstall Git Graph extension in VS Code\n\nOnce installed, click the Git Graph button on the bottom-left status bar to see a visual representation of your git history\n\n\n\n\n\n\nOrganize Directories for the devstack\n\nThe Earth Economy Devstack organizes repositories in a specific way, described here.\n\nRecall that our workspace links to e.g. the hazelbean_dev repository, but it currently points to a directory that doesn’t yet exist.\n\nIn our Files directory, create a directory named hazelbean (not hazelbean_dev as that will be the repository’s name)\n\n\n\nBelow, we will use git to clone the hazelbean_dev into this directory we just created\n\n\n\n\n\n\n\n\nClone the required repositories\n\nInstead of using the command line, we will use Git via VS Code’s “Command Pallate”\n\nThe Command Pallate is accessed via \nIt is a search bar that you can use to run commands in VS Code\n\nOnce you’ve opened the Command Pallate, type “git clone” and it will search for the command\n\n\n\nOnce you select the command, it will prompt you if you want to write in your Repo’s GitHub URL manually, or you can use VS Code to search the different repositories you have access to\n\n\n\nSearching via GitHub found, for instance, NatCapTEEMs/gep repo\n\n\n\nOnce you select that, it will ask you what local directory you want to clone the repository to.\n\nBy default it assumes you want to clone it to your user directory as below\n\n\n\nWe instead want to clone it to the hazelbean directory we created earlier, which will put a new folder hazelbean_dev in that directory\nTo do this, navigate to the hazelbean directory and select it\n\n\n\n\nAfter you’ve cloned a repository, you can now access it in the file explorer\n\n\n\n\n\n\nRepeat the cloning approach with all the repos you need\n\nIf you are a member of NatCap TEEMs or the JohnsonPolaskyLab, you should have access to these repositories via our GitHub organization\nIf you’re not a member, you will still be able to clone all of the public repositories (which are all documented in various journal articles)\n\n\n\nOptional Cython Installation steps\nIf you run any part of the code that uses compiled C++ code (mostly SEALS and the optimized raster functions in Hazelbean), then you will need to have a C++ compiler on your system.\n\nWindows Cython Installation\nYou could go to https://visualstudio.microsoft.com/visual-cpp-build-tools/ and select download build tools.\n\nHowever, the following command line in the preferred way. Enter the following command in the Terminal:\nwinget install Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;includeRecommended\"\nThis will launch the build-tools installer (you could do this manually via the MS website if you want, but this ensures you get the right tools).\n\n\n\nMac Cython Installation\nXcode Command Line Tools: macOS users can install these tools by running xcode-select –install in the Terminal. This command downloads and installs the Xcode Command Line Tools, which includes gcc and clang, the compilers needed to compile C/C++ code on macOS. This is somewhat analogous to the Visual Studio Build Tools on Windows."
  },
  {
    "objectID": "earth_economy_devstack/index.html",
    "href": "earth_economy_devstack/index.html",
    "title": "Earth-Economy Devstack",
    "section": "",
    "text": "Earth-Economy Devstack\nThis is the documentation for the Earth-Economy Devstack, which is the set of repositories and code tools used by NatCap TEEMs. This documentation starts with installation instructions, overall organization of the Earth-Economy Devstack and discusses common coding practices used among the multiple related repositories. See my YouTube channel or specifically the most up-to-date Earth-Economy Devstack Training playlist.\n\nMost of my Youtube playlists are publicly available but unlisted, so to access the rest of my videos, see the following links:\n\nPrevious version of my Big Data class. APEC 8222, Spring 2023\nA comprehensivelist of videos without any editing/cutting. Only useful if you want to hear inane babbling from me when I’m waiting for something to download or are really interested in what I do after class when I forget to turn off the zoom recording…",
    "crumbs": [
      "Earth-Economy Devstack"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html",
    "href": "earth_economy_devstack/hazelbean_overview.html",
    "title": "Hazelbean",
    "section": "",
    "text": "Hazelbean is a collection of geospatial processing tools based on gdal, numpy, scipy, cython, pygeoprocessing, taskgraph, natcap.invest, geopandas and many others to assist in common spatial analysis tasks in sustainability science, ecosystem service assessment, global integrated modelling assessment, natural capital accounting, and/or calculable general equilibrium modelling.\nNote that for all of the features of hazelbean to work, your computer will need to be configured to compile Cython files to C code. This workflow is tested in a Python 3.10, 64 bit Windows environment. It should work on other system environments, but this is not yet tested.\n\n\nFollow the instructions in the Earth-Economy Devstack repository.\n\n\n\nTest that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.\nimport hazelbean as hb\nFrom here, explore examples of the useful spatial, statistical and economic functions in the examples section of this documentation. A good starting example would be the zonal_statistics function. These functions are documented in their code declarations and in the Hazelbean Spatial Algorithms section. #TODOO[Make Link]. Anothr good starting point would be to understand the ProjectFlow() object, which is described in the project_flow page #TODOO[make link]",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#installation",
    "href": "earth_economy_devstack/hazelbean_overview.html#installation",
    "title": "Hazelbean",
    "section": "",
    "text": "Follow the instructions in the Earth-Economy Devstack repository.",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/hazelbean_overview.html#quickstart",
    "href": "earth_economy_devstack/hazelbean_overview.html#quickstart",
    "title": "Hazelbean",
    "section": "",
    "text": "Test that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.\nimport hazelbean as hb\nFrom here, explore examples of the useful spatial, statistical and economic functions in the examples section of this documentation. A good starting example would be the zonal_statistics function. These functions are documented in their code declarations and in the Hazelbean Spatial Algorithms section. #TODOO[Make Link]. Anothr good starting point would be to understand the ProjectFlow() object, which is described in the project_flow page #TODOO[make link]",
    "crumbs": [
      "Hazelbean"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy_overview.html",
    "href": "earth_economy_devstack/gtappy_overview.html",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "GTAPpy\n\n\nOverview\nGTAPPy is a Python package designed to facilitate the use of the Global Trade Analysis Project (GTAP) model. GTAP is a computable general equilibrium (CGE) model, widely used for economic policy analysis. GTAPpy allows for efficient, streamlined, parallelized GTAP simulations.\nUse of the model proceeds in the following 6 steps.\n1. User Creates A Run Script There will be a run test standard file that initializes GTAPPy, ensuring everything is downloaded. After this is run once, a run script can be created that is specific to the project.\nA run script (e.g. “run_process_aez_results.py”) is where the user configures run variables, including paths (e.g., the project and base data directories), as attributes to a ProjectFlow object.\nThe ProjectFlow object (imported from the Hazelbean library) then serves as the backbone for organizing directories and enabling parallel processing of tasks. It acts as a centralized structure where all the project-level features can be accessed consistently and efficiently throughout, and it manages dependencies between tasks to enable parallelization.\nInstead of modifying the run script, GTAPPy is in the process of having an API (a file, ‘scenario_definitions.csv’) that automatically gets saved out from the run test standard file. Once this is generated, a new simulation can be configured.\nTo build the task tree (i.e., ProjectFlow object), a function in gtappy_initialize_project.py is called.\n2. User Runs the Script\nThe user executes the run script (e.g., ‘run_process_aez_results.py’), preferentially in Debug Mode (in VS Code), with the Earth Economy Devstack workspace configured.\n3. GTAPpy Initializes the Project and Creates a Task Tree\nThe program initializes a ProjectFlow object with the configurations specified by the user in the run script. Then it creates a task tree (calling the functions in gtappy_initialize_project.py).\n4. GTAPpy Generates Instructions (CMF File)\nGTAP is run using GEMPACK, which requires CMF files to be prepared. GTAPpy generates these CMF files based on user input, using functions in the gtappy_runner.py script.\n5. Executing GEMPACK\nThe ‘run_gtap_cmf’ function in ‘gtappy_runner.py’ is called, which uses the CMF file to run the GTAP model through GEMPACK.\n6. Processing Model Outputs GEMPACK generates detailed outputs, which GTAPpy then processes to generate what the user requires. Often, the user wants filtered or aggregated data for specific sectors, regions, and years. GTAPpy typically generates CSV files summarizing the data, e.g. trade data (TRAD.csv).\nGTAP also generates log files to show details of the simulation, including errors.\nThe outputs are stored in the directories that were assigned by the user in the run script."
  },
  {
    "objectID": "earth_economy_devstack/global_invest.html",
    "href": "earth_economy_devstack/global_invest.html",
    "title": "Global InVEST",
    "section": "",
    "text": "The standard version of InVEST, provided by the Natural Capital Project, is a set of tools for quantifying the values of natural capital in clear, credible, and practical ways. InVEST enables decision-makers to assess the trade-offs associated with alternative management choices and to identify the policies and practices that can best balance human needs with environmental sustainability. InVEST is designed to be accessible to a broad audience, including conservation organizations, governments, development banks, academics, and the private sector.\nThe InVEST software comprises an easy-to-use graphical interface version as well as direct access to the Python library for advanced users. Typical InVEST applications looked at individual watersheds or small administrative regions. Starting withChaplin-Kramer et al. (2019), however, a series of global applications of InVEST have developed. These applications have sometimes used the standard InVEST Python library, but in most cases, custom versions were created to enable calculation of the models globally (which was challenging for both computation and data reasons). The term Global InVEST refers informally to the multiple code repositories behind these applications. Although the code-base for Global InVEST is still fragmented, we are working to organize and standardize these models.\nThe three main sources for Global InVEST is\n\nNature’s Contributions to People: Chaplin-Kramer et al. (2019)\nGTAP-InVEST: Johnson et al. (2023)\nNature’s Frontiers: Damania et al. (2023)\n\n\n\n\nTo run the Global InVEST models within the Earth Economy Dev Stack standard, you first need to follow the installation steps in the earth_economy_devstack guide to include global_invest and hazelbean in your workspace.\n\n\n\nWithin the global_invest_dev/global_invest directory, you will find the following structure:\nglobal_invest\n│-- ecosystem_services_subfolders\n│-- global_invest_main.py\n│-- ecosystem_services_functions.py\n│-- ecosystem_services_tasks.py\n│-- example_run_file.py\n│-- example_execute_invest_ES.py\n│-- run_global_ES.py\n\necosystem_services_subfolders contains the scripts for the specific ecosystem service models. Currently, it includes folders like pollination_sufficiency and cnc_global_cv.\nglobal_invest_main.py is the primary script for running the global InVEST models.\necosystem_services_functions.py includes general functions used by the global InVEST models. Files ending in _functions.py are not project-specific and contain general utility functions.\necosystem_services_tasks.py includes tasks specific to the global InVEST models. Files ending in _tasks.py are project-specific and include the project object p, which is handled by Project Flow in hazelbean.\nexample_run_file.py is a script to run an example at the global level. This script demonstrates how to run a carbon storage model for a specific area of interest (AOI). The default AOI is RWA.\nexample_execute_invest_ES.py is similar to the run script created by InVEST but uses different paths.\nrun_global_ES.py is the script for running the global InVEST models for a specific Ecosystem Service. Files ending in _ES.py are provided as examples, such as the Global-InVEST-Carbon Storage model (run_global_carbon.py).\n\n\n\n\nTo have a testing run of Global InVEST Model, you can run the example_run_file.py script. This script runs the carbon storage model for a specific AOI (Rwanda). The script will download the necessary data, run the model, and save the results in the workspace directory. It is well-documented script with the following major steps:\n\nCreate a Project object p, and set up corresponding project-directories.\n\n# Create the project flow object\np = hb.ProjectFlow()\n\n# Set project-directories\np.user_dir = os.path.expanduser('~') # EE Devstack is defined relative to the user's directory, but could be overwritten if running not for the Devstack.    \np.extra_dirs = ['Files', 'global_invest', 'projects'] # Extra directories used inside the user_dir\np.project_name = 'test_global_invest' # Name of the project, which will be used to create the project_dir\np.project_name = p.project_name + '_' + hb.pretty_time() # Comment this line out if you want it to use an existing project. Will skip recreation of files that already exist.\np.project_dir = os.path.join(p.user_dir, os.sep.join(p.extra_dirs), p.project_name) # Combines above to set the user_dir. \np.set_project_dir(p.project_dir) # Based on the project_dir, create all other relevant dirs, like input, intermediate, and output.\n\n# Set basa_data_dir. Will download required files here.\np.base_data_dir = os.path.join(p.user_dir, 'Files', 'base_data') # Could be anywhere, including external storage. But, should not be cloud-controlled (like Google Drive). \n\nDefine the test and paths for the LULC, region_ids, and global_regions_vector. By default, the AOI is set to Rwanda. The paths are set to the catographic/ee folder in the base data directory.\n\n# Set model-paths and details\np.aoi = 'RWA'\np.base_year_lulc_path = p.get_path('lulc/esa/lulc_esa_2017.tif') # Defines the fine_resolution\np.region_ids_coarse_path = p.get_path('cartographic/ee/id_rasters/eemarine_r566_ids_900sec.tif') # Defines the coarse_resolution\np.global_regions_vector_path = p.get_path('cartographic/ee/eemarine_r566_correspondence.gpkg') # Will be used to create the aoi vector\n\nCreate your own tasks trees and execute the task tree.\n\nFor example, in the following task tree, the project object p has 4 tasks:\n\nproject_aoi_task: Clips the global regions vector to the AOI selected.\naoi_inputs_task: Clips global inputs based on the AOI.\necosystem_services_task: Empty task just to contain all the other ES tasks (by being set as their parent task).\ncarbon_storage_biophysical_task: Actually implements the model logic.\n\nNote: if parent=p.ecosystem_services_task is not set for carbon_storage_biophysical_task, the task will not be set as the child-task of the ecosystem_services_task.\ndef build_task_tree(p):\n    p.project_aoi_task = p.add_task(ecosystem_services_tasks.project_aoi) # Clips the global_regions_vector to the aoi selected   \n    p.aoi_inputs_task = p.add_task(ecosystem_services_tasks.aoi_inputs) # Clips global inputs based on the aoi\n    p.ecosystem_services_task = p.add_task(ecosystem_services_tasks.ecosystem_services) # Empty task just to contain all the other ES tasks (by being set as their parent task)\n    p.carbon_storage_biophysical_task = p.add_task(ecosystem_services_tasks.example_ecosystem_services_invest_task, parent=p.ecosystem_services_task) # Actually implements the model logic\n\n# Build the task tree and excute it!\nbuild_task_tree(p)\np.execute()\n\n\n\nIf you are a new user who has forked the global_invest_dev repository and want to add a new ecosystem service model, you can follow these steps:\n\nCopy and paste the example_run_file.py, then rename it to run_global_&lt;new_es&gt;.py. In the newly created file, ensure you define your own p.&lt;new_es&gt;_biophysical_task in the ecosystem_services_tasks.py file. For instance, if your new ecosystem service model is sediment_retention, in your run_global_sdr.py, you should define your own task tree as follows:\n\ndef build_task_tree(p):\n    p.project_aoi_task = p.add_task(ecosystem_services_tasks.project_aoi) # Clips the global_regions_vector to the selected AOI\n    p.aoi_inputs_task = p.add_task(ecosystem_services_tasks.aoi_inputs) # Clips global inputs based on the AOI\n    p.ecosystem_services_task = p.add_task(ecosystem_services_tasks.ecosystem_services) # Empty task to contain all other ES tasks (set as their parent task)\n    p.sediment_retention_biophysical_task = p.add_task(ecosystem_services_tasks.sediment_retention_invest_task, parent=p.ecosystem_services_task) # CHANGED FROM CARBON STORAGE TO SEDIMENT RETENTION\n\nIn the ecosystem_services_tasks.py file, add a new task for the new ecosystem service model. For example:\n\ndef sediment_retention_invest_task(p):\n    \"\"\"Iterate over a scenarios file to calculate SDR from LULC maps.\"\"\"\n\n    if p.run_this:\n        ecosystem_services_functions.sdr_biophysical(current_lulc_path, current_sdr_path, p.exhaustive_sdr_path, sdr_output_path)\n\nAny functions used in the sediment_retention_invest_task in the ecosystem_services_tasks.py file should be called from the ecosystem_services_functions.py file. For example, the sdr_biophysical function called in the def sediment_retention_invest_task(p): function should be defined in ecosystem_services_functions.py. Alternatively, you can save this function in a separate Python or C script compiled by Cython for faster calculation, similar to carbon_storage_ipcc_tier_1_cython.pyx. However, keep the main calculation core in a single Cython file and retain the main body of the function in the ecosystem_services_functions.py file.",
    "crumbs": [
      "Global InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/global_invest.html#setting-up-global-invest-models",
    "href": "earth_economy_devstack/global_invest.html#setting-up-global-invest-models",
    "title": "Global InVEST",
    "section": "",
    "text": "To run the Global InVEST models within the Earth Economy Dev Stack standard, you first need to follow the installation steps in the earth_economy_devstack guide to include global_invest and hazelbean in your workspace.",
    "crumbs": [
      "Global InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/global_invest.html#folder-structure-of-global-invest",
    "href": "earth_economy_devstack/global_invest.html#folder-structure-of-global-invest",
    "title": "Global InVEST",
    "section": "",
    "text": "Within the global_invest_dev/global_invest directory, you will find the following structure:\nglobal_invest\n│-- ecosystem_services_subfolders\n│-- global_invest_main.py\n│-- ecosystem_services_functions.py\n│-- ecosystem_services_tasks.py\n│-- example_run_file.py\n│-- example_execute_invest_ES.py\n│-- run_global_ES.py\n\necosystem_services_subfolders contains the scripts for the specific ecosystem service models. Currently, it includes folders like pollination_sufficiency and cnc_global_cv.\nglobal_invest_main.py is the primary script for running the global InVEST models.\necosystem_services_functions.py includes general functions used by the global InVEST models. Files ending in _functions.py are not project-specific and contain general utility functions.\necosystem_services_tasks.py includes tasks specific to the global InVEST models. Files ending in _tasks.py are project-specific and include the project object p, which is handled by Project Flow in hazelbean.\nexample_run_file.py is a script to run an example at the global level. This script demonstrates how to run a carbon storage model for a specific area of interest (AOI). The default AOI is RWA.\nexample_execute_invest_ES.py is similar to the run script created by InVEST but uses different paths.\nrun_global_ES.py is the script for running the global InVEST models for a specific Ecosystem Service. Files ending in _ES.py are provided as examples, such as the Global-InVEST-Carbon Storage model (run_global_carbon.py).",
    "crumbs": [
      "Global InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/global_invest.html#run-global-invest-models",
    "href": "earth_economy_devstack/global_invest.html#run-global-invest-models",
    "title": "Global InVEST",
    "section": "",
    "text": "To have a testing run of Global InVEST Model, you can run the example_run_file.py script. This script runs the carbon storage model for a specific AOI (Rwanda). The script will download the necessary data, run the model, and save the results in the workspace directory. It is well-documented script with the following major steps:\n\nCreate a Project object p, and set up corresponding project-directories.\n\n# Create the project flow object\np = hb.ProjectFlow()\n\n# Set project-directories\np.user_dir = os.path.expanduser('~') # EE Devstack is defined relative to the user's directory, but could be overwritten if running not for the Devstack.    \np.extra_dirs = ['Files', 'global_invest', 'projects'] # Extra directories used inside the user_dir\np.project_name = 'test_global_invest' # Name of the project, which will be used to create the project_dir\np.project_name = p.project_name + '_' + hb.pretty_time() # Comment this line out if you want it to use an existing project. Will skip recreation of files that already exist.\np.project_dir = os.path.join(p.user_dir, os.sep.join(p.extra_dirs), p.project_name) # Combines above to set the user_dir. \np.set_project_dir(p.project_dir) # Based on the project_dir, create all other relevant dirs, like input, intermediate, and output.\n\n# Set basa_data_dir. Will download required files here.\np.base_data_dir = os.path.join(p.user_dir, 'Files', 'base_data') # Could be anywhere, including external storage. But, should not be cloud-controlled (like Google Drive). \n\nDefine the test and paths for the LULC, region_ids, and global_regions_vector. By default, the AOI is set to Rwanda. The paths are set to the catographic/ee folder in the base data directory.\n\n# Set model-paths and details\np.aoi = 'RWA'\np.base_year_lulc_path = p.get_path('lulc/esa/lulc_esa_2017.tif') # Defines the fine_resolution\np.region_ids_coarse_path = p.get_path('cartographic/ee/id_rasters/eemarine_r566_ids_900sec.tif') # Defines the coarse_resolution\np.global_regions_vector_path = p.get_path('cartographic/ee/eemarine_r566_correspondence.gpkg') # Will be used to create the aoi vector\n\nCreate your own tasks trees and execute the task tree.\n\nFor example, in the following task tree, the project object p has 4 tasks:\n\nproject_aoi_task: Clips the global regions vector to the AOI selected.\naoi_inputs_task: Clips global inputs based on the AOI.\necosystem_services_task: Empty task just to contain all the other ES tasks (by being set as their parent task).\ncarbon_storage_biophysical_task: Actually implements the model logic.\n\nNote: if parent=p.ecosystem_services_task is not set for carbon_storage_biophysical_task, the task will not be set as the child-task of the ecosystem_services_task.\ndef build_task_tree(p):\n    p.project_aoi_task = p.add_task(ecosystem_services_tasks.project_aoi) # Clips the global_regions_vector to the aoi selected   \n    p.aoi_inputs_task = p.add_task(ecosystem_services_tasks.aoi_inputs) # Clips global inputs based on the aoi\n    p.ecosystem_services_task = p.add_task(ecosystem_services_tasks.ecosystem_services) # Empty task just to contain all the other ES tasks (by being set as their parent task)\n    p.carbon_storage_biophysical_task = p.add_task(ecosystem_services_tasks.example_ecosystem_services_invest_task, parent=p.ecosystem_services_task) # Actually implements the model logic\n\n# Build the task tree and excute it!\nbuild_task_tree(p)\np.execute()",
    "crumbs": [
      "Global InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/global_invest.html#customizing-_tasks-_functions-and-run_-files-in-global-invest",
    "href": "earth_economy_devstack/global_invest.html#customizing-_tasks-_functions-and-run_-files-in-global-invest",
    "title": "Global InVEST",
    "section": "",
    "text": "If you are a new user who has forked the global_invest_dev repository and want to add a new ecosystem service model, you can follow these steps:\n\nCopy and paste the example_run_file.py, then rename it to run_global_&lt;new_es&gt;.py. In the newly created file, ensure you define your own p.&lt;new_es&gt;_biophysical_task in the ecosystem_services_tasks.py file. For instance, if your new ecosystem service model is sediment_retention, in your run_global_sdr.py, you should define your own task tree as follows:\n\ndef build_task_tree(p):\n    p.project_aoi_task = p.add_task(ecosystem_services_tasks.project_aoi) # Clips the global_regions_vector to the selected AOI\n    p.aoi_inputs_task = p.add_task(ecosystem_services_tasks.aoi_inputs) # Clips global inputs based on the AOI\n    p.ecosystem_services_task = p.add_task(ecosystem_services_tasks.ecosystem_services) # Empty task to contain all other ES tasks (set as their parent task)\n    p.sediment_retention_biophysical_task = p.add_task(ecosystem_services_tasks.sediment_retention_invest_task, parent=p.ecosystem_services_task) # CHANGED FROM CARBON STORAGE TO SEDIMENT RETENTION\n\nIn the ecosystem_services_tasks.py file, add a new task for the new ecosystem service model. For example:\n\ndef sediment_retention_invest_task(p):\n    \"\"\"Iterate over a scenarios file to calculate SDR from LULC maps.\"\"\"\n\n    if p.run_this:\n        ecosystem_services_functions.sdr_biophysical(current_lulc_path, current_sdr_path, p.exhaustive_sdr_path, sdr_output_path)\n\nAny functions used in the sediment_retention_invest_task in the ecosystem_services_tasks.py file should be called from the ecosystem_services_functions.py file. For example, the sdr_biophysical function called in the def sediment_retention_invest_task(p): function should be defined in ecosystem_services_functions.py. Alternatively, you can save this function in a separate Python or C script compiled by Cython for faster calculation, similar to carbon_storage_ipcc_tier_1_cython.pyx. However, keep the main calculation core in a single Cython file and retain the main body of the function in the ecosystem_services_functions.py file.",
    "crumbs": [
      "Global InVEST"
    ]
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Data",
    "section": "",
    "text": "Data used in my work is free to use (though of course I appreciate citation and/or collaboration!). The following are links to several frequently frequently requested data sets. Feel free to contact me for other supporting documents from publications."
  },
  {
    "objectID": "data/index.html#globally-harmonized-carbon-storage-data-using-a-simple-decision-tree-approach",
    "href": "data/index.html#globally-harmonized-carbon-storage-data-using-a-simple-decision-tree-approach",
    "title": "Data",
    "section": "Globally Harmonized Carbon Storage Data using a Simple Decision Tree Approach",
    "text": "Globally Harmonized Carbon Storage Data using a Simple Decision Tree Approach\nResults can be downloaded here: data download."
  },
  {
    "objectID": "data/index.html#global-food-demand-and-carbon-preserving-cropland-expansion-under-varying-levels-of-intensification",
    "href": "data/index.html#global-food-demand-and-carbon-preserving-cropland-expansion-under-varying-levels-of-intensification",
    "title": "Data",
    "section": "Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification",
    "text": "Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification\nAll new data from this paper can be downloaded at https://drive.google.com/drive/folders/0BxVzEBY6ApU5Yk9WX0p0LUdNM28 If a map from the paper is not available it is publicly available on the data creator's site."
  },
  {
    "objectID": "data/index.html#global-agriculture-and-carbon-trade-offs",
    "href": "data/index.html#global-agriculture-and-carbon-trade-offs",
    "title": "Data",
    "section": "Global agriculture and carbon trade-offs",
    "text": "Global agriculture and carbon trade-offs\nAll new data from this paper can be downloaded at https://drive.google.com/folderview?id=0BxVzEBY6ApU5dkNLcUhMbEVkdU0  If a map from the paper is not available it is publicly available on the data creator's site."
  },
  {
    "objectID": "data/index.html#data-for-mesh-mapping-ecosystem-services-to-human-well-being",
    "href": "data/index.html#data-for-mesh-mapping-ecosystem-services-to-human-well-being",
    "title": "Data",
    "section": "Data for MESH (Mapping Ecosystem Services to Human well-being)",
    "text": "Data for MESH (Mapping Ecosystem Services to Human well-being)\nSee the MESH page for more details. Data is now hosted by The Natural Capital Project at https://naturalcapitalproject.stanford.edu/software/mesh and is distributed as a part of the software installation."
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "I have two books currently in production. Both are open-access and open-source. The first is a principles of microeconomics book and the second is a book on the economics of the environment. Both are written in markdown and converted to html, pdf, epubs, etc. using quarto. The html files are then hosted on github pages.\n\n\nCan be accessed at Open Principles of Microeconomics\n\n\n\nNot yet public, but soon will be hosted here. This is a book about the GTAP-InVEST model and the related set of similar models and tools."
  },
  {
    "objectID": "books/index.html#open-principles-of-microeconomics",
    "href": "books/index.html#open-principles-of-microeconomics",
    "title": "Books",
    "section": "",
    "text": "Can be accessed at Open Principles of Microeconomics"
  },
  {
    "objectID": "books/index.html#earth-economy-modeling",
    "href": "books/index.html#earth-economy-modeling",
    "title": "Books",
    "section": "",
    "text": "Not yet public, but soon will be hosted here. This is a book about the GTAP-InVEST model and the related set of similar models and tools."
  },
  {
    "objectID": "bio/index.html",
    "href": "bio/index.html",
    "title": "Bio",
    "section": "",
    "text": "Justin Andrew Johnson is an Assistant Professor of Applied Economics at the University of Minnesota. He received his Ph.D. from the department in 2014. Justin works closely with the Natural Capital Project at the University of Minnesota and Stanford University. Justin’s research focuses on how the economy affects the environment, and vice versa, on global to local scales. Currently, Justin leads a project that links the Global Trade Analysis Project (GTAP) out of Purdue University with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model from the Natural Capital Project, aiming to build strong quantitative evidence on how changes in ecosystem services affect economic performance at the macroeconomic level and how global policies can be designed to sustainably manage our natural capital."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "An abbreviated version of my CV can be downloaded here. For the full and most recent CV, please contact me.\nCV – Justin Andrew Johnson"
  },
  {
    "objectID": "earth_economy_devstack/conventions.html",
    "href": "earth_economy_devstack/conventions.html",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "On windows, for historical sillyness reasons, paths at the most bare-metal level separate levels with backslash \\. However, nearly everything else within windows correctly interprets a forward slash / as a backslash at the os level. Thus, although we can’t control how OTHER programs process or report out on backslashes, we follow the basic rule that if we are inputting it as text into our repos, always use forward slash, no matter what ($5 bounty: argue a case where this needs an exception within our devstack). Linux/mac always uses forward slash (henceforth, aka slash), so this should work well.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#variable-and-scenario-naming-conventions",
    "href": "earth_economy_devstack/conventions.html#variable-and-scenario-naming-conventions",
    "title": "Justin Andrew Johnson",
    "section": "Variable and Scenario Naming Conventions",
    "text": "Variable and Scenario Naming Conventions\nTODO: This one section is possibly redundant with above and some practices are out of date.\nTo keep track of the MANY different filetypes, data processes, variables, scenarios, policies etc, please follow exactly the specifications below.\n\nThe word label refers to a relatively short string (preferably 8 characters long or less) with no spaces, underscores or punctuation (but may have hyphens). This is case-sensitive, but try to avoid capitalization.\nThe word short_label refers to a label that is strictly less or equal to 8 characters to ensure compatibility with HAR files.\nThe word name refers to a longer string that describes a specific label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary.\nThe words index, indices or id refers to numerical data that describes a label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary. If both are used, index/indices refer to a unique, ordered list of ints while an id/ids refer are unique but not necessarily ordered. It’s best to use index/indices and never id/ids.\nThe word class refers to LULC class. Consider renaming this to lc-class?\nScenarios are defined in the following nested structure:\n\nLabel (with no hyphens) for Exogenous Assumptions (e.g., which SSP, which GDP, which population). Typically this will be fully defined by the SSP.\nLabel (with no hyphens) for Climate Assumption (which RCP)\nLabel (can have hyphens) for which model is used (e.g., magpie, luh2-message). Only model is allowed to have hyphens (because they are used for multistep scenario processing of counterfactuals)\nLabel for Counterfactual. This often represent policy Assumptions/Definition and can include BAU, which is a special counterfactual against which other policies are compared. Different counterfactuals correspond to different shockfiles in the econ model or different LUC projection priorities, etc.\n\nCounterfactuals may have multiple processing steps, which will be denoted by appending a hyphen and exactly 4 chars to the end of the base counterfactual label.\n\nIFor example, a run excludes consideration of ES, insert “-noes”, at the end of the policy_name if it does include ES, postpend nothing (as this will be the one that is referenced by default)\n\n\nYear\n\nWhen the variable is singular, it must be an int. If it is plural, as is ints in a list. However, when either is stored in a dataframe, always type always type check as follows:\n\nIf singular, do str(value), int(value) or float(value) as appropriate when reading from the df into a python variable.\nIf plural, assume the df value is a space-delimited string that needs to be split, e.g. as [int(i) for i in value.split(’ ‘)], or’ ’.join(values) if going into the DF.\n\nTODO: This needs to be updated with the json-style parsing added to scenarios.csv files.\n\n\nThree types of years exist, including\n\np.base_years, (which recall will always be a list even if there is a single entry because the variable name is plural)\n\n\n\nTogether, the labels above mean that the scenarios can be represented by directories as follows:\n\nssp2/rcp45/luh2-message/bau/filename_2050.tif\n\nNote, the last layer of the hierarchy will be included as a the suffix of the filename start rather than as a directory (also see below for filename conventions)\n\n\nFor filenames, there are two possible conventions:\n\nImplied: This means that the directory structure above defines all the labels with the exception of year (which is postpended to the filename label) and the current variable name (such as lulc) which appears at the front of the filename.\n\ne.g., project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif\n\nExplicit: Even if a file is in a directory which implies its labels, explicit file naming will always include each label (and the variable label stays in front of the filename), so the above example is:\n\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\nAnd if there are no ES considered, it would be project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\n\nnotice that in both examples, the variable still starts with the variable name “lulc” before the scenario information (if it’s included).\n\nFor defining variable names (like “lulc” above), you might have multiple nested layers of the variable definition. These can be (or ARE) distinct from the nesting implied by scenarios. So for example, consider the base data path:\n\nC:\\Users\\jajohns\\Files\\base_data\\lulc\\esa\\seals7\\binaries\\2014\\lulc_esa_seals7_binary_2014_cropland.tif\nHere, lulc is the variable label, but it then has sub nests for esa to denote its origin. seals7 to denote it has been recategorized, binaries to denote the seals7 has been processed into binary format (is-class vs is-not-class), and the year comes last. I haven’t figured out a standard that sets the order of the subnests, but it should be consistent. It probably depends on which element you’re going to iterate over, so years should probably be last? Actually no, once you’re in a year, you’d then iterate over class labels.\n\nBecause labels have no spaces or underscores, it is possible to convert the nested structure above to a single string, as in filename_ssp2_rcp45_policyname_year.tif.\nFiletypes that supported in this computation environment include\n\nNetcdf with the above listed dimensions in the same order\nA set of geotiffs embedded in directories. Each label gets a directory level expect for year, which by convention, will ALWAYS be the last 4 characters of a filename before the extension (with an underscore before it).\nA spreadsheet linkable to a geographic representation (e.g., a shapefile or a geopackage) in vertical format\n\nAlso we will create a set of tables to analyze results\n\nThese will define Regions (shp) for quick result plotting\nSpecifically, we will have a full vertically stacked CSV of results, then for each Report Archetype we would output 1 minimal info CSV and the corresponding Figure.\n\nMiscellaneous:\n\nbase_years is correct, never baseline_years (due to confusion between baseline and bau)\n\nScenario types\n\nThree scenario_types are supported: baseline, bau and policy\n\nBaseline assumes the year has data existing from observations (rather than modelled) and that these years are defined in p.years (and identically defined in p.base_years).\n\nOne exception is when eg GTAP is used to update the base year from 2017 to 2023, and then policies are applied on 2023.\n\nBAU and policy scenarios assume the results are modelled and that their years are defined in p.years (but not p.base_years)\n\nClarify what is the naming difference between src dst versus input output. Is the former only for file paths or can it also be e.g. array. OR does this have to do with if it is a function return.\n\nProposed answer: src/dst is a pointer/reference to a thing and Input/Output is the thing itself. Esp useful for paths.\nSimilarly, _path and _dir imply the string is a reference, so src_path and src_dir are common.\nYou might often see e.g. input_array = hb.as_array(src_path), illustrating this difference.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#get_path-and-ref_path",
    "href": "earth_economy_devstack/conventions.html#get_path-and-ref_path",
    "title": "Justin Andrew Johnson",
    "section": "get_path and ref_path",
    "text": "get_path and ref_path\nPaths that are ready to use are denoted with ‘_path’ as the last 5 characters.\np.ha_per_cell_10sec_ref_path = os.path.join(p.base_data_dir, 'pyramids', \"ha_per_cell_10sec.tif\")\nHowever prior to calling get_path, the root directory of the path is not clear. Get path will search all the options given an inputted reference path. These follow the convention that the last 8 characters are ‘ref_path’, it is a relative path, and specifically, it is relative to one of the many possible root directories, which include by default\n\ncur_dir (i.e., for when something will be generated by the task and if it exists the task should be skipped)\ninput_dir (for project specific inputs)\nbase_data_dir (for cross-project dirs, is the default download location from the next step)\nthe cloud storage location.\n\np.ha_per_cell_10sec_ref_path = os.path.join(‘pyramids’, “ha_per_cell_10sec.tif”)",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#setting-the-environment",
    "href": "earth_economy_devstack/conventions.html#setting-the-environment",
    "title": "Justin Andrew Johnson",
    "section": "Setting the environment",
    "text": "Setting the environment\n\nWhat does this mean?\n\nLet’s find the executable we installed with Mamba\n\n\nWhen we set the interpretter/environment in VS Code, we are essentially telling it to look for this python.exe file in this exact folder.\n\nTo get the path, we can right click on the file and select “Copy Path”\n\n\n\n\nWhy might you need the actual path?\n\nOn the command line, when we call python this_script.py sometimes it will fail to find (or find the right) python executable to call. We could have been explicit by calling C:\\Users\\jajohns\\mambaforge\\envs\\env2023a\\python.exe this_script.py instead.\nAdditionally, when you are publishing Quarto Books or Jupyter Notebooks, you might want to set the path explicitly\n\nFor example, to publish the EE Devstack, I have a line in the quarto.yml file like this:\nyaml     execute:         engine: python         python: C:\\Users\\jajohns\\mambaforge\\envs\\env2023a\\python.exe",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#dealing-with-utf-8-encoding-issues-in-data-analysis-a-practical-guide",
    "href": "earth_economy_devstack/conventions.html#dealing-with-utf-8-encoding-issues-in-data-analysis-a-practical-guide",
    "title": "Justin Andrew Johnson",
    "section": "Dealing with UTF-8 Encoding Issues in Data Analysis: A Practical Guide",
    "text": "Dealing with UTF-8 Encoding Issues in Data Analysis: A Practical Guide\nIf you’ve ever loaded a CSV file in pandas and found your first column mysteriously renamed to something like Ã¯Â»Â¿fao_country_id, or noticed that place names like “Åland” turn into garbled text like “Ã…land”, you’ve encountered UTF-8 encoding problems. These issues are incredibly common when working with international datasets and can be frustrating to debug. Understanding what’s happening and how to fix it will save you hours of headaches.\nThe weird characters you’re seeing are typically caused by Byte Order Mark (BOM) confusion or encoding mismatches. When you see Ã¯Â»Â¿ at the start of column names, that’s the UTF-8 BOM (bytes EF BB BF) being incorrectly interpreted as Latin-1 characters. To fix this when reading CSV files, use pd.read_csv('file.csv', encoding='utf-8-sig') instead of the default encoding. If you’ve already loaded the data, you can rename the column with df.rename(columns={df.columns[0]: 'correct_name'}) or strip the BOM from all columns with df.columns = df.columns.str.replace('\\ufeff', '').\nWhen working with geospatial data, you might load a GeoPackage file that correctly displays international characters, but then find those characters get mangled when you drop the geometry and save as CSV. This happens because programs like Excel don’t properly handle UTF-8 files unless they have a BOM signature. The solution is to explicitly specify the encoding when saving: df.to_csv('output.csv', encoding='utf-8-sig', index=False). The -sig part adds the BOM that tells programs like Excel “this file is UTF-8 encoded.”\nThe key difference between utf-8 and utf-8-sig is the presence of a 3-byte signature at the beginning of the file. Plain utf-8 follows the Unicode standard exactly and produces smaller files, while utf-8-sig adds a BOM that acts as a hint to applications about the file’s encoding. Modern web applications and APIs typically expect plain UTF-8, while desktop applications like Excel often need the BOM to interpret international characters correctly.\nExcel causes problems when it encounters UTF-8 files without the BOM because it assumes the file uses Windows-1252 encoding (or your system’s default). When Excel tries to interpret UTF-8 bytes using the wrong encoding, single international characters get split and displayed as multiple garbled characters. For example, “Å” (stored as bytes C3 85 in UTF-8) gets interpreted as “Ã…” because Excel reads each byte separately using Windows-1252.\nFor most data analysis workflows, using utf-8-sig everywhere is the safest approach. While it adds 3 bytes to your files, this overhead is negligible, and you’ll avoid encoding issues when sharing data or opening files in different applications. Only use plain utf-8 when you’re specifically working in environments that expect it (like certain Unix command-line tools) or when you know the consuming application handles UTF-8 correctly without needing the BOM signature.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#docstrings",
    "href": "earth_economy_devstack/conventions.html#docstrings",
    "title": "Justin Andrew Johnson",
    "section": "Docstrings",
    "text": "Docstrings\nWe use “Google style docstrings” , which work well with Quarto.\nExample:\ndef fibonacci(n):\n    \"\"\"Generate the nth Fibonacci number.\n    \n    Args:\n        n (int): The position in the Fibonacci sequence.\n    \n    Returns:\n        int: The nth Fibonacci number.\n\n    Raises:\n        NameError: If n is not an int.\n    \n    Examples:\n        Basic usage:\n        ```python\n        fibonacci(5)  # Returns 5\n        fibonacci(10) # Returns 55\n        ```\n    \"\"\"",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#function-and-method-naming-conventions",
    "href": "earth_economy_devstack/conventions.html#function-and-method-naming-conventions",
    "title": "Justin Andrew Johnson",
    "section": "Function and Method Naming Conventions",
    "text": "Function and Method Naming Conventions\n\nFactory and Creation Functions\nmake\nReserved for factory functions or methods that create new instances (e.g., make_dataset, make_grid)\ncreate\nFor functions that generate new objects/files from scratch (e.g., create_empty_raster, create_new_project)\n\n\nFile Operations\nopen\nOpens file handles or connections, loads metadata but not full data (e.g., gdal.Open, open_database_connection)\nload\nReads entire data into memory from disk or remote source (e.g., load_dataset, load_model_weights)\nread\nGets data into memory from filepath or file-like object, often line-by-line or chunk-based (e.g., read_csv, read_lines)\nwrite\nOutputs data to disk or stream, often incremental (e.g., write_results, write_to_buffer)\nsave\nPersists complete objects/states to disk, typically all at once (e.g., save_model, save_checkpoint)\n\n\nData Manipulation\nextract\nPulls out specific portions from larger data structures (e.g., extract_features, extract_roi)\nexecute\nRuns commands, scripts, or processes (e.g., execute_query, execute_pipeline)\nconvert\nTransforms data from one format/type to another (e.g., convert_to_numpy, convert_crs)\n\n\nCollection Operations\nlist\nReturns collection of items, typically as Python list (e.g., list_files, list_available_datasets)\nremove\nDeletes from collections/memory structures (e.g., remove_duplicates, remove_from_cache)\ndelete\nPermanently removes from disk/database (e.g., delete_file, delete_record)\ndisplace\nIn-house term for renaming a path with e.g. a timestamp so you can write a new file into the old place (without eliminating the original). Has optional delete_on_exit functionality\nrename\nChanges name while keeping same location (e.g., rename_file, rename_column)\nreplace\nSubstitutes one value/object with another (e.g., replace_missing_values, replace_substring)\nmove\nRelocates to different location/container (e.g., move_file, move_to_archive)",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#directory-and-file-naming",
    "href": "earth_economy_devstack/conventions.html#directory-and-file-naming",
    "title": "Justin Andrew Johnson",
    "section": "Directory and File Naming",
    "text": "Directory and File Naming\n\nDirectory Conventions\ndir\nAvoid - ambiguous (directory or direction?). Use ‘directory’ instead. Note that it CAN be used as a suffix, like temp_dir. Function Names Exception: Use dir in function names (e.g., delete_dir(), create_dir()) for brevity, matching Unix heritage.\ndirectory\nPreferred over ‘dir’ for clarity (e.g., output_directory, working_directory)\nfolder\nAvoid using unless it’s user-facing documentation. Alternative to ‘directory’, more GUI-oriented.\ndirname\nPreferred in most cases. Matches stdlib conventions. Name of directory without path (e.g., “outputs” from “/home/user/outputs”)\ndir_name\nAvoid. Used when following strict PEP 8 in a codebase that emphasizes readability\ndir_path\nThe full directory path (e.g., “/home/user/documents”)\ndirectory_name\nAvoid. Too long.\n\n\nFile Naming\npath\nFull path to file/directory (e.g., input_path, config_path)\nfile_name\nFull filename with extension (e.g., “data.csv”, “image.tif”). Preferred over filename.\nfilename\nAvoid. Use file_name. However this is broadly used elsewhere.\nfile_root\nFilename without extension (e.g., “data” from “data.csv”)\nfileroot\nAvoid. Use file_root. However this is broadly used elsewhere.\nfile_extension\nFile suffix including dot (e.g., “.csv”, “.tif”)\nparent_directory\nOne level up in directory tree (e.g., os.path.dirname(path))\ngrandparent_path\nTwo levels up; consider using pathlib for clarity",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#variable-naming-conventions",
    "href": "earth_economy_devstack/conventions.html#variable-naming-conventions",
    "title": "Justin Andrew Johnson",
    "section": "Variable Naming Conventions",
    "text": "Variable Naming Conventions\n\nCounting and Size\nn_cols\nCommon shorthand for counts in scientific computing (e.g., n_samples, n_features)\nnum_cols\nMore explicit than n_, good for public APIs (e.g., num_iterations, num_bands)\nnumber_cols\nToo verbose; prefer n_ or num_ prefix\nshape\nDimensions tuple for arrays/matrices (e.g., array.shape returns (rows, cols))\nsize\nTotal number of elements or bytes (e.g., array.size, file_size)\n\n\nData Types\ndata_type\nPreferred option for most cases and consistency with numpy/pandas conventions\ndtype\nAvoid except where a library uses it explicitly, specifically numpy\ndatatype\nAvoid except where a library uses it explicitly\n\n\nGeospatial Conventions\ncell_size\nSpatial resolution of single raster cell (e.g., 30m for Landsat)\nres\nCommon abbreviation for resolution in geospatial contexts\nresolution\nFull word preferred in public APIs, documentation\nx_res\nHorizontal resolution (width of pixel in map units)\ny_res\nVertical resolution (height of pixel in map units)\nraster_info\nMetadata dict/object for gridded data (extent, projection, resolution)\nvector_info\nMetadata dict/object for feature data (geometry type, attribute schema)\n\n\nBounding Boxes\nbb\nCommon abbreviation for bounding box in geospatial code\nbounding_box\nPreferred for clarity\nbb_exact\nPyramids-specific notation that the bb aligns with some pyramidal ID raster. (preferably identified in the name, like bb_exact_30sec)\nbounding_box_min_max_notation\n[xmin, ymin, xmax, ymax]\nbounding_box_xy_notation\n[xmin, xmax, ymin, ymax]\ncr_widthheight\n[col, row, width, height] Specific usecase optimized for use with GDAL notation on bounding boxes.\n\n\nCoordinates\nlat\nStandard abbreviation for latitude\nlon\nStandard abbreviation for longitude (preferred over ‘long’)\nlon_vs_long\nAlways use ‘lon’; ‘long’ conflicts with Python built-in type\nlat_size\nHeight in degrees or number of latitude values\nlon_size\nWidth in degrees or number of longitude values",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#identifiers-and-indexing",
    "href": "earth_economy_devstack/conventions.html#identifiers-and-indexing",
    "title": "Justin Andrew Johnson",
    "section": "Identifiers and Indexing",
    "text": "Identifiers and Indexing\nindex\nPosition in sequence or database index (e.g., row_index, spatial_index). DO NOT USE for id columns in e.g. pandas DataFrames as the position can change.\nid\nUnique identifier; prefer ‘identifier’ or specific names (e.g., user_id, feature_id)\ncounter\nFor loop counters or accumulation (e.g., error_counter, iteration_counter). When lots of counters are being used, consider c_row, row in enumerate(rows) or c_col, col in enumerate(cols) to avoid confusion.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#data-processing",
    "href": "earth_economy_devstack/conventions.html#data-processing",
    "title": "Justin Andrew Johnson",
    "section": "Data Processing",
    "text": "Data Processing\nvalid\nBoolean or mask indicating valid/usable data points\nnot_valid\nPrefer ‘invalid’ or use boolean logic with ‘valid’\nmask\nBoolean array for filtering/selection (True where condition met)\nndv\nPreferred option over nodata, no_data, no_data_value, etc.\nnonzero\nElements/indices where value != 0 (e.g., numpy.nonzero())",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#standard-library-abbreviations",
    "href": "earth_economy_devstack/conventions.html#standard-library-abbreviations",
    "title": "Justin Andrew Johnson",
    "section": "Standard Library Abbreviations",
    "text": "Standard Library Abbreviations\narray\nGeneric numpy/array object (e.g., data_array, temperature_array)\ndf\nStandard abbreviation for pandas DataFrame. If you have many dfs, consider using df_ prefix (e.g., df_sales, df_customers)\ngdf\nStandard abbreviation for GeoPandas GeoDataFrame\ngpd\nStandard import alias for geopandas (import geopandas as gpd)\npd\nStandard import alias for pandas (import pandas as pd)\nnp\nStandard import alias for numpy (import numpy as np)",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#special-conventions",
    "href": "earth_economy_devstack/conventions.html#special-conventions",
    "title": "Justin Andrew Johnson",
    "section": "Special Conventions",
    "text": "Special Conventions\ndefault\nDefault parameter values (e.g., default_crs, default_timeout)\npaths_to_delete_at_exit\nClear naming for cleanup list; good practice for temp files\nuris_to_delete_at_exit\nOutdated. Use paths_to_delete_at_exit instead.\nplots_to_display_at_exit\nClear naming for deferred plotting; useful for batch processing\n\nPriority Indicators\ntodo\nSingle TODO for standard priority tasks\ntodoo\nDouble-O for lower priority items needing attention. The priority order for todo markers is usually opposite among coders.\ntodooo\nTriple-O for even lower issues that must be addressed\n\n\nNaming to Avoid\nglobals\nAvoid global variables; if necessary, use UPPER_CASE naming\ntemp\nNever use - ambiguous (temporary or temperature?)\ntemporary\nBetter than ‘temp’ - clear meaning for short-lived objects\ntemperature\nBetter than ‘temp’ - clear meaning for thermal data\n\n\nDebug Conventions\nprint_parentheses\nprint() with no space likely debug code to be removed\nprint_space_parentheses\nprint () with space indicates intentional output to keep",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#miscellaneous-conventions",
    "href": "earth_economy_devstack/conventions.html#miscellaneous-conventions",
    "title": "Justin Andrew Johnson",
    "section": "Miscellaneous Conventions",
    "text": "Miscellaneous Conventions\nuse_of_word_old\nPrefix ‘old_’ for previous versions during refactoring (e.g., old_algorithm)\ninfo\nGeneric metadata container (e.g., dataset_info, processing_info)\ndescribe\nStatistical summary function name (e.g., describe_dataset, pandas.describe())\ndesc\nCommon abbreviation for description in metadata/documentation\nrun_dir\nDirectory for current execution outputs (e.g., experiments, logs)\npostpending_array_etc_to_var_name\nGood practice to indicate type (e.g., temps_array vs temps_list)",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#file-type-specific",
    "href": "earth_economy_devstack/conventions.html#file-type-specific",
    "title": "Justin Andrew Johnson",
    "section": "File Type Specific",
    "text": "File Type Specific\nshapefile\nOnly use to specify that you’re explicitly using an ESRI shapefile vs eg geopackage.\ntiff\nNever use, except in GDAL Drivername GTiff",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#scenario-naming-conventions",
    "href": "earth_economy_devstack/conventions.html#scenario-naming-conventions",
    "title": "Justin Andrew Johnson",
    "section": "Scenario Naming Conventions",
    "text": "Scenario Naming Conventions\nScenarios are defined in the following nested structure:\n\nExogenous Assumptions Label (no hyphens) - e.g., SSP, GDP, population\nClimate Assumption Label (no hyphens) - which RCP\nModel Label (can have hyphens) - e.g., magpie, luh2-message\nCounterfactual Label - includes BAU as special counterfactual\n\nExample directory structure:\nssp2/rcp45/luh2-message/bau/filename_2050.tif \n\nFilename Conventions\nImplied: Directory structure defines all labels except year:\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif \nExplicit: Include all labels in filename:\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#utf-8-encoding-guidelines",
    "href": "earth_economy_devstack/conventions.html#utf-8-encoding-guidelines",
    "title": "Justin Andrew Johnson",
    "section": "UTF-8 Encoding Guidelines",
    "text": "UTF-8 Encoding Guidelines\nWhen working with international datasets:\n\nUse pd.read_csv('file.csv', encoding='utf-8-sig') for reading\nUse df.to_csv('output.csv', encoding='utf-8-sig', index=False) for writing\nThe -sig adds a BOM that helps Excel correctly interpret UTF-8",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#python-style-guidelines",
    "href": "earth_economy_devstack/conventions.html#python-style-guidelines",
    "title": "Justin Andrew Johnson",
    "section": "Python Style Guidelines",
    "text": "Python Style Guidelines\n\nDocstringsGeneral Style\n\nUse snake_case for variables and functions\nUse CamelCase for classes\nAvoid global variables\nKeep functions concise and focused on a single task",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#styleguide",
    "href": "earth_economy_devstack/conventions.html#styleguide",
    "title": "Justin Andrew Johnson",
    "section": "Styleguide",
    "text": "Styleguide\nWe follow pep8 style for Python with a few key departures. First, we allow more than 80 character-width lines. The limit is flexible but keep it what fits within 160 characters, with the exception of comments trailing a code line. For functions, only have 1 linebreak not 2 between functions. This is useful when you collapse/fold your functions, you can see more functions.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/conventions.html#unsorted",
    "href": "earth_economy_devstack/conventions.html#unsorted",
    "title": "Justin Andrew Johnson",
    "section": "UNSORTED",
    "text": "UNSORTED\nTODO, needs to be put in the right place in the documentation\nonly a projectflow module’s root dir can have the exact file run.py. This is used as a markerfile to identify the module’s root.",
    "crumbs": [
      "Methods",
      "Conventions"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html",
    "href": "earth_economy_devstack/gtappy.html",
    "title": "GTAPPy",
    "section": "",
    "text": "Step 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want.\n\n\n\n\nGetting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database.\n\n\n\nGTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression.\n\n\n\n\n\nWe need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\n\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\n\n\n\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\n\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\n\n\n\n\n\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\n\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#installation",
    "href": "earth_economy_devstack/gtappy.html#installation",
    "title": "GTAPPy",
    "section": "",
    "text": "Step 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "href": "earth_economy_devstack/gtappy.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "title": "GTAPPy",
    "section": "",
    "text": "Getting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#iterating-over-multiple-aggregations",
    "href": "earth_economy_devstack/gtappy.html#iterating-over-multiple-aggregations",
    "title": "GTAPPy",
    "section": "",
    "text": "GTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtappy.html#release-notes",
    "href": "earth_economy_devstack/gtappy.html#release-notes",
    "title": "GTAPPy",
    "section": "",
    "text": "We need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\n\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\n\n\n\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\n\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\n\n\n\n\n\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\n\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file.",
    "crumbs": [
      "GTAPPy"
    ]
  },
  {
    "objectID": "earth_economy_devstack/gtap_invest.html",
    "href": "earth_economy_devstack/gtap_invest.html",
    "title": "GTAP-InVEST",
    "section": "",
    "text": "GTAP-InVEST\nAlthough GTAP-InVEST is a part of the Earth-Economy Devstack, the GTAP-InVEST User Guide is hosted under the GTAP-InVEST home page.",
    "crumbs": [
      "GTAP-InVEST"
    ]
  },
  {
    "objectID": "earth_economy_devstack/how_to_contribute.html",
    "href": "earth_economy_devstack/how_to_contribute.html",
    "title": "Contributing code to the Earth Economy Devstack",
    "section": "",
    "text": "The Earth Economy Devstack and all of its published article code is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system git (https://git-scm.com/). git allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system git can vary and there generally is no ‘right’ or ‘wrong’ in collaborative software development.\nThere are two different ways to interact with a github repository: 1. making a fork; 2; working on a branch. This section describes both options, however internal members of TEEMs will use the fork method, using Pull Requests when their code is ready to incorporate back into the forked repository.\nWe also create and host a curated base_data. This data can be seamlessly downloaded using ProjectFlow within the devstack via the get_path() method. See the last section of this page for details on contributing data.\n\n\nUsing SEALS as an example, this section walks through how a new user could contribute a modification via a fork and pull request.\n\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for\n\ncode development or fixes\nsubmitting a pull request to the original SEALS repository.\n\n\n\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. The figure below illustrates the relationship of the original repo, your own user repo (made by forking) and then the local set of files cloned onto your machine.\n\nTo clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;, illustrated in the bottom two rows of the figure below. Eventually, your code can be incorporated back into the upstream repository via pull requests, discussed below.\n\n\n\n\nIt often happens that after you forked a repo, someone else makes a change that you need and you would like to update your fork with the latest changes from the original repository, This is easy if you haven’t made any changes, and just a bit harder if you have. To do so, follow these steps:\n1. Configure the Original Repository as a Remote\nIf you haven’t already, add the original repository as a remote. You typically name it upstream.\ngit remote add upstream &lt;URL of the original repository&gt;\n2. Fetch the Latest Changes from the Original Repository\nFetch the branches and their respective commits from the upstream repository. Commits to main will be stored in the local branch upstream/main.\ngit fetch upstream\n3. Merge (or Rebase) the Changes\n\nSwitch to your local main branch (or the branch you want to update).\ngit checkout main\nMerge the changes from upstream/main into your local main branch.\ngit merge upstream/main\nResolve any merge conflicts if they arise, commit the merge, and push the updated branch to your fork.\ngit push origin main\n\n4. Ensure Your Fork is Up-to-Date\nIf you have other branches, you may need to repeat the merge or rebase process for each branch.\nSummary of Commands\ngit remote add upstream &lt;URL of the original repository&gt;\ngit fetch upstream\ngit checkout main\ngit merge upstream/main  # or git rebase upstream/main\ngit push origin main  # use --force with rebase\nBy following these steps, you will incorporate the latest changes from the original repository into your fork.\n\n\n\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.\nHere is an example where you think some functions in seals_process_coarse_timeseries.py within the seals_dev project should be removed and relocated to a project-specific repository, rather than remaining in the core seals_dev repository. In your forked seals_dev repository:\n\nThis example shows that you have made changes in seals_process_coarse_timeseries.py. You need to “Stage Changes” (as indicated by the red circle) and “Commit Changes” before you can create a pull request.\n\nNext, commit and sync the changes to the forked repository.\n\nTo create a pull request, please go to your forked repository on GitHub.\n\nYou will see your commit message should match the information you typed in the commit box when you committed and synced the changes. Then, click on the Contribute button and select Open pull request.\n\nOn the pull request page, which appears on the upstream repository, you can add more information about the changes you made. Before clicking on the Create pull request button, test your changes again and ensure they pass all tests before proceeding to ensure that the changes work as expected.\n\nFinally, the pull request will appear in the upstream repository. The main developers will review your changes and, if approved, will merge them into the upstream repository.\n\n\n\nUsually Forking is sufficient for most uses. But, if for instance you are working on two different features simultaneously on your own fork, you might want to organize it with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\n\nSuppose you want to make a change to the hazelbean_dev repository. We will use this example to explain how we use Git Graph to make this contribution. First, let’s take a look at the Git Graph interface.\n\nFirst note the Repo dropdown box indicating we are looking at the hazelbean_dev repo. Here you could switch to other Repos that have been added to your workspace. The next thing to note are the three different branch tages (Blue, Magenta and Green boxes on the Git Graph tree view). These are the three branches that are currently in the hazelbean_dev repository. The blue branch is the main branch, the magenta branch is the develop branch and the green branch is a branch specific to the user and/or the feature. In this case, it is develop_justin branch. The lines and dots indicates the history of how files were edited and then how the different branches were merged back together after an edit. By default, when you clone a repository, you will be on the main branch. To switch to another branch, right-click the tag and select checkout branch. When you do this, the files on your harddrive will be changed by Git to match the status of the branch you just checked out. In the image below, we can see which is the current branch because it has an open-circle dot in the graph and the name is bold. Other things to notice is that after the name of the branch there is the word origin. This indicates that the branch is synced with the remote repository (named origin by convention).\nTo make a change, you will first want to create your own branch. First, make absolutely certain you currently have checked out the develop branch (unless you know why to do otherwise). We will use main branch only to contain working “releases”. Once develop is checked out, use the Command Pallate (ctrl-shift-p) and search for create branch.\n\nChoose the right repository among those loaded in your workspace, hazelbean_dev in this example.\n\nThen select it and give the new branch a name, either develop_&lt;your_name&gt; or feature_&lt;your_feature_name&gt;. Below we have created a new feature branch called feature_test branched off of the develop branch. It should look like this\n\nYou’ll notice that the tag is bold, indicating you have checked it out and it is your current branch. Also notice though that it does not have the tag origin after it, indicating that it is not synced with the remote repository. To sync it, you will need to push it to the remote repository. To do this, right-click the tag and select push branch. This will push the branch to the remote repository and it will be available to other users.\nAnother way to push the branch is using the Version Control tab in VS Code. Click the Version Control tab on the Lefthand bar. There you will see all of the repos you have loaded into our workspace. For hazelbean_dev, you will see it has a publish branch button. Clicking this will have the same effect as the push branch command in Git Graph.\n\nRegardless of how you pushed it to the remote repository, you will now see the branch has the ‘origin’ tag after it, indicating it is synced with the remote repository. It is now also the checked-out branch and so all changes you make will be made to this branch.\n\nNow, we’re going to make a small change to the code. In the VS Code file explorer, I will open up file_io.py in the hazelbean module and scroll to the function I want to edit.\n\nOn lines 778-779, you’ll see VS Code has greyed the variables out indicating they are not used, so I will remove them. Once gone, you’ll see a blue bar on the left side of the editor. This is Git and VS Code indicating to you you made a change. Blue indicates a modification where as green indicates a new addition and red indicates a deletion.\n\nTo illustrate a new addition, I will add a line at to space out our for loop. Once I do this, you’ll see the green bar on the left side of the editor. In the image you can also see that the file_io.py is now a different color and has an M next to it. This indicates that the file has been modified.\n\nAnother useful thing you can do is click on the Blue or Green edit bar to see more details, as below.\n\nHere, you can see the exact changes you made. Additionally, there are buttons at the top of this new box that let you revert the change to go back to how it was before.\nBefore we commit our changes, look at the Git Graph view. You’ll see that we now have a new grey line coming from the feature_test branch, showing that we have uncommitted changes. You could click on the uncommitted changes link to see the edits we made.\n\nWe are now going to commit our changes. To do this, go to the Version Control Tab. You will now see that there is a change listed under the hazelbean_dev repository. Click on the change to see the details. You will see a more detailed “diff editor” that lets you understand (or change) what was edited. To accept these changes, we will click the commit button. But first write a short commit message. Click Commit (but don’t yet click the next button to Sync changes). After committing, look back at the Git Graph view. You’ll see that the grey line is now gone and the blue feature_test tag is at the top of our commit tree along with our comitt message.\n\nNotice though that the tag for origin/feature_test is not yet up at the new location. This is because we have not yet pushed our changes to the remote repository. To do this, click the Sync button in the bottom right of the VS Code window. This will push your changes to the remote repository and update the tag to the new location, like this.\n\nYour code is now on GitHub and other contributors with access could check it out and try it. But, this code will be different than their code and if you made non-trivial changes, it could be hard to keep straight what is going on. To clarify this, we are going to merge our feature_test branch back into develop.\nTo do this, first you must make sure you have the most recent version of the develop branch. To do this, first we will use the command palette and search for git pull. This will pull the most recent changes from the remote repository and update your local develop branch. Next, we want to make sure that any changes in develop are merged with what we just did to our feature_test branch. If there were no changes, this is not strictly necessary, but it’s a good precation to take to avoid future merge conflicts. To do this, right click on the develop tag and select merge into current branch. A popup will ask you to confirm this, which you do want to (with the default options).\n\nNow that we know for sure we have the most up-to-date develop branch details, we can merge our new feature into the develop branch. However, we will protect the develop branch so that only features that pass unit tests can be merged in. Thus, you will make a pull request, as described above, to get me to merge your work in the develop branch. For completeness, here we discuss how one would to that. To do this, first right-click on the develop tag and select Checkout branch. Our git tree will now show we have develop checked out:\n\nWith develop checked out, now right click on feature_test and select merge into current branch. Select confirm in the popup box. Click Sync Changes in the Version Control Tab.\n\nthe feature_test and develop branches are now identical. You could now delete feature_test and nothing would be lost. Now, the develop branch is ready for a user to use and/or make new branches off of. # End",
    "crumbs": [
      "Methods",
      "How to Contribute"
    ]
  },
  {
    "objectID": "earth_economy_devstack/how_to_contribute.html#getting-started-with-forking",
    "href": "earth_economy_devstack/how_to_contribute.html#getting-started-with-forking",
    "title": "Contributing code to the Earth Economy Devstack",
    "section": "",
    "text": "Using SEALS as an example, this section walks through how a new user could contribute a modification via a fork and pull request.\n\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for\n\ncode development or fixes\nsubmitting a pull request to the original SEALS repository.\n\n\n\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. The figure below illustrates the relationship of the original repo, your own user repo (made by forking) and then the local set of files cloned onto your machine.\n\nTo clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;, illustrated in the bottom two rows of the figure below. Eventually, your code can be incorporated back into the upstream repository via pull requests, discussed below.\n\n\n\n\nIt often happens that after you forked a repo, someone else makes a change that you need and you would like to update your fork with the latest changes from the original repository, This is easy if you haven’t made any changes, and just a bit harder if you have. To do so, follow these steps:\n1. Configure the Original Repository as a Remote\nIf you haven’t already, add the original repository as a remote. You typically name it upstream.\ngit remote add upstream &lt;URL of the original repository&gt;\n2. Fetch the Latest Changes from the Original Repository\nFetch the branches and their respective commits from the upstream repository. Commits to main will be stored in the local branch upstream/main.\ngit fetch upstream\n3. Merge (or Rebase) the Changes\n\nSwitch to your local main branch (or the branch you want to update).\ngit checkout main\nMerge the changes from upstream/main into your local main branch.\ngit merge upstream/main\nResolve any merge conflicts if they arise, commit the merge, and push the updated branch to your fork.\ngit push origin main\n\n4. Ensure Your Fork is Up-to-Date\nIf you have other branches, you may need to repeat the merge or rebase process for each branch.\nSummary of Commands\ngit remote add upstream &lt;URL of the original repository&gt;\ngit fetch upstream\ngit checkout main\ngit merge upstream/main  # or git rebase upstream/main\ngit push origin main  # use --force with rebase\nBy following these steps, you will incorporate the latest changes from the original repository into your fork.\n\n\n\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.\nHere is an example where you think some functions in seals_process_coarse_timeseries.py within the seals_dev project should be removed and relocated to a project-specific repository, rather than remaining in the core seals_dev repository. In your forked seals_dev repository:\n\nThis example shows that you have made changes in seals_process_coarse_timeseries.py. You need to “Stage Changes” (as indicated by the red circle) and “Commit Changes” before you can create a pull request.\n\nNext, commit and sync the changes to the forked repository.\n\nTo create a pull request, please go to your forked repository on GitHub.\n\nYou will see your commit message should match the information you typed in the commit box when you committed and synced the changes. Then, click on the Contribute button and select Open pull request.\n\nOn the pull request page, which appears on the upstream repository, you can add more information about the changes you made. Before clicking on the Create pull request button, test your changes again and ensure they pass all tests before proceeding to ensure that the changes work as expected.\n\nFinally, the pull request will appear in the upstream repository. The main developers will review your changes and, if approved, will merge them into the upstream repository.\n\n\n\nUsually Forking is sufficient for most uses. But, if for instance you are working on two different features simultaneously on your own fork, you might want to organize it with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\n\nSuppose you want to make a change to the hazelbean_dev repository. We will use this example to explain how we use Git Graph to make this contribution. First, let’s take a look at the Git Graph interface.\n\nFirst note the Repo dropdown box indicating we are looking at the hazelbean_dev repo. Here you could switch to other Repos that have been added to your workspace. The next thing to note are the three different branch tages (Blue, Magenta and Green boxes on the Git Graph tree view). These are the three branches that are currently in the hazelbean_dev repository. The blue branch is the main branch, the magenta branch is the develop branch and the green branch is a branch specific to the user and/or the feature. In this case, it is develop_justin branch. The lines and dots indicates the history of how files were edited and then how the different branches were merged back together after an edit. By default, when you clone a repository, you will be on the main branch. To switch to another branch, right-click the tag and select checkout branch. When you do this, the files on your harddrive will be changed by Git to match the status of the branch you just checked out. In the image below, we can see which is the current branch because it has an open-circle dot in the graph and the name is bold. Other things to notice is that after the name of the branch there is the word origin. This indicates that the branch is synced with the remote repository (named origin by convention).\nTo make a change, you will first want to create your own branch. First, make absolutely certain you currently have checked out the develop branch (unless you know why to do otherwise). We will use main branch only to contain working “releases”. Once develop is checked out, use the Command Pallate (ctrl-shift-p) and search for create branch.\n\nChoose the right repository among those loaded in your workspace, hazelbean_dev in this example.\n\nThen select it and give the new branch a name, either develop_&lt;your_name&gt; or feature_&lt;your_feature_name&gt;. Below we have created a new feature branch called feature_test branched off of the develop branch. It should look like this\n\nYou’ll notice that the tag is bold, indicating you have checked it out and it is your current branch. Also notice though that it does not have the tag origin after it, indicating that it is not synced with the remote repository. To sync it, you will need to push it to the remote repository. To do this, right-click the tag and select push branch. This will push the branch to the remote repository and it will be available to other users.\nAnother way to push the branch is using the Version Control tab in VS Code. Click the Version Control tab on the Lefthand bar. There you will see all of the repos you have loaded into our workspace. For hazelbean_dev, you will see it has a publish branch button. Clicking this will have the same effect as the push branch command in Git Graph.\n\nRegardless of how you pushed it to the remote repository, you will now see the branch has the ‘origin’ tag after it, indicating it is synced with the remote repository. It is now also the checked-out branch and so all changes you make will be made to this branch.\n\nNow, we’re going to make a small change to the code. In the VS Code file explorer, I will open up file_io.py in the hazelbean module and scroll to the function I want to edit.\n\nOn lines 778-779, you’ll see VS Code has greyed the variables out indicating they are not used, so I will remove them. Once gone, you’ll see a blue bar on the left side of the editor. This is Git and VS Code indicating to you you made a change. Blue indicates a modification where as green indicates a new addition and red indicates a deletion.\n\nTo illustrate a new addition, I will add a line at to space out our for loop. Once I do this, you’ll see the green bar on the left side of the editor. In the image you can also see that the file_io.py is now a different color and has an M next to it. This indicates that the file has been modified.\n\nAnother useful thing you can do is click on the Blue or Green edit bar to see more details, as below.\n\nHere, you can see the exact changes you made. Additionally, there are buttons at the top of this new box that let you revert the change to go back to how it was before.\nBefore we commit our changes, look at the Git Graph view. You’ll see that we now have a new grey line coming from the feature_test branch, showing that we have uncommitted changes. You could click on the uncommitted changes link to see the edits we made.\n\nWe are now going to commit our changes. To do this, go to the Version Control Tab. You will now see that there is a change listed under the hazelbean_dev repository. Click on the change to see the details. You will see a more detailed “diff editor” that lets you understand (or change) what was edited. To accept these changes, we will click the commit button. But first write a short commit message. Click Commit (but don’t yet click the next button to Sync changes). After committing, look back at the Git Graph view. You’ll see that the grey line is now gone and the blue feature_test tag is at the top of our commit tree along with our comitt message.\n\nNotice though that the tag for origin/feature_test is not yet up at the new location. This is because we have not yet pushed our changes to the remote repository. To do this, click the Sync button in the bottom right of the VS Code window. This will push your changes to the remote repository and update the tag to the new location, like this.\n\nYour code is now on GitHub and other contributors with access could check it out and try it. But, this code will be different than their code and if you made non-trivial changes, it could be hard to keep straight what is going on. To clarify this, we are going to merge our feature_test branch back into develop.\nTo do this, first you must make sure you have the most recent version of the develop branch. To do this, first we will use the command palette and search for git pull. This will pull the most recent changes from the remote repository and update your local develop branch. Next, we want to make sure that any changes in develop are merged with what we just did to our feature_test branch. If there were no changes, this is not strictly necessary, but it’s a good precation to take to avoid future merge conflicts. To do this, right click on the develop tag and select merge into current branch. A popup will ask you to confirm this, which you do want to (with the default options).\n\nNow that we know for sure we have the most up-to-date develop branch details, we can merge our new feature into the develop branch. However, we will protect the develop branch so that only features that pass unit tests can be merged in. Thus, you will make a pull request, as described above, to get me to merge your work in the develop branch. For completeness, here we discuss how one would to that. To do this, first right-click on the develop tag and select Checkout branch. Our git tree will now show we have develop checked out:\n\nWith develop checked out, now right click on feature_test and select merge into current branch. Select confirm in the popup box. Click Sync Changes in the Version Control Tab.\n\nthe feature_test and develop branches are now identical. You could now delete feature_test and nothing would be lost. Now, the develop branch is ready for a user to use and/or make new branches off of. # End",
    "crumbs": [
      "Methods",
      "How to Contribute"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html",
    "href": "earth_economy_devstack/installation.html",
    "title": "Installation",
    "section": "",
    "text": "There are two types of installations. Basic, which just lets you be a user of Hazelbean (and soon SEALS), and then the Repository installation, which gets you up and running with the latest developer repositories. Also check out our video playlist for more context.\n\n\nHazelbean can be installed using condaforge following these steps. See theInstalling Git and Miniforge3 video for a walkthrough.\n\nInstall git: https://Git-scm.com/downloads\nInstall Miniforge3 from https://conda-forge.org/download/\n\nInstall just for your User Account\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\nCreate a new conda environment, activate it, and then install required packages with the following 3 mamba commands. The third command will take quite a while (10+ minutes). The conda init command may not be needed depending on how you configured your shell.\n\nconda init\nconda create -n environment_name\nconda activate environment_name\nmamba install hazelbean\n\n\nYou can now import hazelbean and it will have precompiled binaries already working for your operating system for geospatial operations using ProjectFlow!\n\n\n\nIf you want to run SEALS, edit Hazelbean, or run our global_invest models, you will need to have a C compiler and have editable installs of our git repositories. First, do the basic installation above and then follow these steps:\n\nInstall C/C++ compiler\n\nWindows: - Option 1: You could go to https://visualstudio.microsoft.com/visual-cpp-build-tools/ and select download build tools. - Option 2: Enter the following command in the Terminal: winget install Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;includeRecommended\" This will launch the build-tools installer (you could do this manually via the MS website if you want, but this ensures you get the right tools). - Option 3: Run the install.bat file in the Earth Economy Devestack repo’s root. This just runs the winget command above. - Mac:\nMac/Linux\n\nYou can use Xcode to compile the cython files. Most users will already have this installed but if not, follow the directions below.\n\nIf you don’t have Xcode, you can get it by running xcode-select --install in the Terminal. This command downloads and installs the Xcode Command Line Tools, which includes gcc and clang, the compilers needed to compile C/C++ code on macOS. This is somewhat analogous to the Visual Studio Build Tools on Windows.\n\n\n\nClone all desired repositories\n\nTo install one of our repositories, for example SEALS, you start by cloning the repo at https://github.com/jandrewjohnson/seals_dev.\n\nYou can clone it to whatever folder you want, but if you want to switch to a Workspace configuration later, we recommend installing it in /Users//Files// which using the SEALS example on my machine would be C:/Users/jajohns/Files/seals/seals_dev.\nFrom with your command prompt pointing at that directory, run your git command\n\ngit clone https://github.com/jandrewjohnson\n\n\nSupported repositories include the below\n\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/hazelbean_dev\n\nNOTE: Hazelbean is a little different than the other repos because you have already installed it via Condaforge and that might not need updating. However, if you want to have the very latest version of hazelbean and/or modify it, you will need to install directly from the repository. Note, however, that you should pip uninstall hazelbean before doing the pip install -e . command below.\n\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\n\n\nInstall the cloned repositories using an “editable” install with pip\n\nActivate the conda environment in this command prompt with the following command\n\nmamba activate environment_name\n\nRemove from your environment any previously installed devstack software, INCLUDING HAZELBEAN. It is still necessary to install hazelbean as a first step from the Basic install steps above to get all required dependencies.\n\nWith your environment activated, run e.g. pip uninstall hazelbean or similar for other previously installed models.\n\nNavigate in you command prompt to the directory where you cloned the additional repos. For the SEALS example, this would be C:\\Users\\jajohns\\Files\\seals\\seals_dev\nUse pip to install the repo you as an “editable install” with the following command\n\npip install -e .\n\n\n\nOne installed, the cloned repositories are all importable by python. For SEALS, you might want to start by poking around run_seals_standard.py.\n\n\n\n\nYou MUST have administrator rights to your computer.\nIf you’re using Windows PowerShell (instead of the Command Prompt and it isn’t working with Conda, you must initialize conda in powershell\n\nconda init powershell\n\nIf you don’t add conda to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3;\"\n(you can do for All Users, but you will need to manually set paths to conda)\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option). - Install in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html#basic-installation",
    "href": "earth_economy_devstack/installation.html#basic-installation",
    "title": "Installation",
    "section": "",
    "text": "Hazelbean can be installed using condaforge following these steps. See theInstalling Git and Miniforge3 video for a walkthrough.\n\nInstall git: https://Git-scm.com/downloads\nInstall Miniforge3 from https://conda-forge.org/download/\n\nInstall just for your User Account\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\nCreate a new conda environment, activate it, and then install required packages with the following 3 mamba commands. The third command will take quite a while (10+ minutes). The conda init command may not be needed depending on how you configured your shell.\n\nconda init\nconda create -n environment_name\nconda activate environment_name\nmamba install hazelbean\n\n\nYou can now import hazelbean and it will have precompiled binaries already working for your operating system for geospatial operations using ProjectFlow!",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html#repository-installation",
    "href": "earth_economy_devstack/installation.html#repository-installation",
    "title": "Installation",
    "section": "",
    "text": "If you want to run SEALS, edit Hazelbean, or run our global_invest models, you will need to have a C compiler and have editable installs of our git repositories. First, do the basic installation above and then follow these steps:\n\nInstall C/C++ compiler\n\nWindows: - Option 1: You could go to https://visualstudio.microsoft.com/visual-cpp-build-tools/ and select download build tools. - Option 2: Enter the following command in the Terminal: winget install Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;includeRecommended\" This will launch the build-tools installer (you could do this manually via the MS website if you want, but this ensures you get the right tools). - Option 3: Run the install.bat file in the Earth Economy Devestack repo’s root. This just runs the winget command above. - Mac:\nMac/Linux\n\nYou can use Xcode to compile the cython files. Most users will already have this installed but if not, follow the directions below.\n\nIf you don’t have Xcode, you can get it by running xcode-select --install in the Terminal. This command downloads and installs the Xcode Command Line Tools, which includes gcc and clang, the compilers needed to compile C/C++ code on macOS. This is somewhat analogous to the Visual Studio Build Tools on Windows.\n\n\n\nClone all desired repositories\n\nTo install one of our repositories, for example SEALS, you start by cloning the repo at https://github.com/jandrewjohnson/seals_dev.\n\nYou can clone it to whatever folder you want, but if you want to switch to a Workspace configuration later, we recommend installing it in /Users//Files// which using the SEALS example on my machine would be C:/Users/jajohns/Files/seals/seals_dev.\nFrom with your command prompt pointing at that directory, run your git command\n\ngit clone https://github.com/jandrewjohnson\n\n\nSupported repositories include the below\n\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/hazelbean_dev\n\nNOTE: Hazelbean is a little different than the other repos because you have already installed it via Condaforge and that might not need updating. However, if you want to have the very latest version of hazelbean and/or modify it, you will need to install directly from the repository. Note, however, that you should pip uninstall hazelbean before doing the pip install -e . command below.\n\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\n\n\nInstall the cloned repositories using an “editable” install with pip\n\nActivate the conda environment in this command prompt with the following command\n\nmamba activate environment_name\n\nRemove from your environment any previously installed devstack software, INCLUDING HAZELBEAN. It is still necessary to install hazelbean as a first step from the Basic install steps above to get all required dependencies.\n\nWith your environment activated, run e.g. pip uninstall hazelbean or similar for other previously installed models.\n\nNavigate in you command prompt to the directory where you cloned the additional repos. For the SEALS example, this would be C:\\Users\\jajohns\\Files\\seals\\seals_dev\nUse pip to install the repo you as an “editable install” with the following command\n\npip install -e .\n\n\n\nOne installed, the cloned repositories are all importable by python. For SEALS, you might want to start by poking around run_seals_standard.py.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/installation.html#common-problems",
    "href": "earth_economy_devstack/installation.html#common-problems",
    "title": "Installation",
    "section": "",
    "text": "You MUST have administrator rights to your computer.\nIf you’re using Windows PowerShell (instead of the Command Prompt and it isn’t working with Conda, you must initialize conda in powershell\n\nconda init powershell\n\nIf you don’t add conda to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3;\"\n(you can do for All Users, but you will need to manually set paths to conda)\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option). - Install in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "earth_economy_devstack/magpie.html",
    "href": "earth_economy_devstack/magpie.html",
    "title": "Magpie",
    "section": "",
    "text": "git clone -b develop https://github.com/magpiemodel/magpie.git magpie_develop\nHas input and output folders in gitignore\nMAgpie has a config folder\ndefault.cfg\nis something huge, so use a start script which takes lines from default and overwrites.\nThen in the non-ignored starts folder, has a test_runs.R"
  },
  {
    "objectID": "earth_economy_devstack/magpie.html#installation",
    "href": "earth_economy_devstack/magpie.html#installation",
    "title": "Magpie",
    "section": "",
    "text": "git clone -b develop https://github.com/magpiemodel/magpie.git magpie_develop\nHas input and output folders in gitignore\nMAgpie has a config folder\ndefault.cfg\nis something huge, so use a start script which takes lines from default and overwrites.\nThen in the non-ignored starts folder, has a test_runs.R"
  },
  {
    "objectID": "earth_economy_devstack/methods.html",
    "href": "earth_economy_devstack/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nThis section describes the methods for contributing to and using the overall devstack.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html",
    "href": "earth_economy_devstack/project_complexity.html",
    "title": "The stages of project complexity",
    "section": "",
    "text": "As an introduction and/or motivation for using the software in the EE Devstack, I would like to talk through the process that I went through as a PhD student, postdoc and staff researcher to answer progressively harder questions. I break these out into 6 stages below. Solving these challenges is what led to the creation of Hazelbean and many other software solutions.\n\n\nHere is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))\n\n\n\nThis is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad.\n\n\n\nThe coding approach in level 2 becomes intractable when there are lots of layers to consider. It’s also a pain to have to repeat code to do some common tasks, like loading the raster to a dataset and then to an array. This complexity level starts to apply good coding practices, such as defining helper functions like raster_to_array() below. Code is also made much shorter and more elegant by using loops. It minimizes the number of code statements, reduces bugs and scales better to long lists of input files.\nimport os\nimport numpy as np\nimport gdal\n\n# NOTE 1: Helper function defined\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# NOTE 2: Inputs put into an iterable\ninput_paths = [\n    'yield_per_cell_1.tif',\n    'yield_per_cell_2.tif',\n    'yield_per_cell_3.tif',\n    'yield_per_cell_4.tif',\n]\n\n# NOTE 3: Calculation happens in loops, recording results to an output object\nsummations = []\nfor raster_path in input_paths:\n    array = raster_to_array(raster_path)\n    summations.append(np.sum(array))\n\nprint('Sums of layers: ' + str(summations))\n\n\n\nBelow is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Unlike the other levels, do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment! Instead, I provide a better example of code below that does things better, in the Earth-Economy Devstack way,\n\n\nimport logging\nimport os\nimport csv\nimport math, time, random\nfrom osgeo import gdal, gdalconst\nimport numpy as np\n\n# NOTE 1: I started to pull in Cython (Python code compiled to C for speed) because my code was getting slow\nimport pyximport\npyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n\n# NOTE 2: I wrote my own Python Library (geoecon_utils), which went through several more \n# iterations (Numdal, Lol!), until it got finalized as hazelbean\nimport geoecon_utils.geoecon_utils as gu\nimport geoecon_utils.geoecon_cython_utils as gcu\n\n# NOTE 3: Logging becomes important to manage information input-output used by the developer\nlog_id = gu.pretty_time()\nLOGGER = logging.getLogger('ag_tradeoffs')\nLOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\nfile_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\nLOGGER.addHandler(file_handler)\n\n# NOTE 4: Defining inputs and outputs is now based on a workspace, from which everything\n# else is defined with relative paths. Scales better with inputs and to other users.\nworkspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\nc_1km_file = 'c_1km.tif'\nc_1km_uri = workspace + c_1km_file\nha_per_cell_5m_file = 'ha_per_cell_5m.tif'\nha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n\n# NOTE 5: Here's an example of using custom libraries to \nha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n# NOTE 6: Here we start to deal with conditional running of code that skips outputs if they have already been\n# created. This is often the first (and often most eficatious) optimization of code to run fast.\ndo_30s_resample = False\nif do_30s_resample:\n    # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n    desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n    c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n    gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n\n# NOTE 7: An here, we see an incredibly slow approach that seems intuitive but is wrong\n# because it is 1000x slower than correct vectorized calculations (which are provided by numpy)\narray = raster_to_array(c_30s_unscaled_uri)\nfor row in range(array.shape[0]):\n    for col in range(array.shape[1]):\n        if array[row, col] &lt; 0:\n            array[row, col] = -9999 # Set negative values to the no-data-value\n\n\n\n# For reference, the correct way would have been as below. We will introduce hazelbean utils (like hb.as_array() soon.\narray = hb.as_array(input_path)\narray = np.where(array &lt; 0, -9999, array)\n\n\n\n\nDepending on the size of the array, the numpy where command used above would fail with a MemoryError or something similar. Below you will the first way that I dealt with this (BAD CODE) and then the correct way. In either case, it is almost always true that the most likely solution to larger-than-memory situations is to apply your algorithm in chunks. We’ll do this in both cases.\n\n\nIn this code, I created a thing I made up, a “Tile Reference” which I implemented with geoecon_utils.Tr(). This returned a set of tiles, defined by their row, column, x_width and y_width. We used these to load just subsets of the array and do operations on the smaller thing.\n\nliteral_aggregation_to_5m_cell = False\nif literal_aggregation_to_5m_cell:\n    factor =  10\n    shape = (2160, 4320)\n    c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n    cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n    cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n    for tile in c_30s_tr.tr_frame:\n        tile_array = c_30s_tr.tile_to_array(tile)\n        print 'Aggregating tile', tile\n        for row in range(c_30s_tr.chunkshape[0] / factor):\n            for col in range(c_30s_tr.chunkshape[1] / factor):\n                cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n                cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n    cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n    print gu.desc(cell_sum_5m)\n    gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n\n\n\nThe better way is to use a function that builds in the tiling functionality. Eventually this will expand to multi-computer approaches, but for now, we’ll just use the local but parallelized hb.raster_calculator()\nhb.raster_calculator(this)\n\n\n\n\nThe final level of complexity we will discuss (before just using the Earth Economy Devstack approach) arises when the number of files that must be managed becomes a challenge both for performance reasons and the challenges of managing complexity.\nOne example where this comes up is when a computation requires writing tiles of output. In many big-data applications and in most of the very large datasets that are available online, the data are themselves stored in tiles. On the one hand, this is nice because it automatically suggests a chunk-by-chunk parallelization strategy. On the other hand, it quickly becomes challenging when, for instance, you want to look at an area of interest (AOI) that spans multiple tiles. There are a plethora of software solutions to deal with this, such as GDAL’s Virtual Raster (VRT) file type, but many of these have limitations.\nWhen the computation in question requires many complex steps which might be contingent on other intermediate products from adjacent tiles, even some of the most cutting-edge solutions that implement complex tiling architecture (like DASK with rioxarray) will not be sufficient. This was the challenge that arose when doing downscaling with the SEALS model, especially when the algorithm had to be trained on such tiles millions of times. The optimized algorithm in this complex data and computation dependency-tree situation required a new tool, which would also have to address all of the above challenges in project complexity.\nThis led to ProjectFlow, one of the key tools within the Earth Economy Devstack and a part of Hazelbean.",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-1-simple-question-answered-well",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-1-simple-question-answered-well",
    "title": "The stages of project complexity",
    "section": "",
    "text": "Here is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "This is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad.",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-3-starting-to-deal-with-generalization-reusing-code-and-shortening-scripts.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-3-starting-to-deal-with-generalization-reusing-code-and-shortening-scripts.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "The coding approach in level 2 becomes intractable when there are lots of layers to consider. It’s also a pain to have to repeat code to do some common tasks, like loading the raster to a dataset and then to an array. This complexity level starts to apply good coding practices, such as defining helper functions like raster_to_array() below. Code is also made much shorter and more elegant by using loops. It minimizes the number of code statements, reduces bugs and scales better to long lists of input files.\nimport os\nimport numpy as np\nimport gdal\n\n# NOTE 1: Helper function defined\ndef raster_to_array(raster_input_path):\n    ds = gdal.Open(yield_per_hectare_raster_path_1)\n    print(\"Reading \" + raster_input_path +'. This might take a while!')\n    array = ds.ReadAsArray()\n    return array\n\n# NOTE 2: Inputs put into an iterable\ninput_paths = [\n    'yield_per_cell_1.tif',\n    'yield_per_cell_2.tif',\n    'yield_per_cell_3.tif',\n    'yield_per_cell_4.tif',\n]\n\n# NOTE 3: Calculation happens in loops, recording results to an output object\nsummations = []\nfor raster_path in input_paths:\n    array = raster_to_array(raster_path)\n    summations.append(np.sum(array))\n\nprint('Sums of layers: ' + str(summations))",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-4-starting-to-deal-with-performance-and-generalization.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-4-starting-to-deal-with-performance-and-generalization.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "Below is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Unlike the other levels, do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment! Instead, I provide a better example of code below that does things better, in the Earth-Economy Devstack way,\n\n\nimport logging\nimport os\nimport csv\nimport math, time, random\nfrom osgeo import gdal, gdalconst\nimport numpy as np\n\n# NOTE 1: I started to pull in Cython (Python code compiled to C for speed) because my code was getting slow\nimport pyximport\npyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n\n# NOTE 2: I wrote my own Python Library (geoecon_utils), which went through several more \n# iterations (Numdal, Lol!), until it got finalized as hazelbean\nimport geoecon_utils.geoecon_utils as gu\nimport geoecon_utils.geoecon_cython_utils as gcu\n\n# NOTE 3: Logging becomes important to manage information input-output used by the developer\nlog_id = gu.pretty_time()\nLOGGER = logging.getLogger('ag_tradeoffs')\nLOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\nfile_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\nLOGGER.addHandler(file_handler)\n\n# NOTE 4: Defining inputs and outputs is now based on a workspace, from which everything\n# else is defined with relative paths. Scales better with inputs and to other users.\nworkspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\nc_1km_file = 'c_1km.tif'\nc_1km_uri = workspace + c_1km_file\nha_per_cell_5m_file = 'ha_per_cell_5m.tif'\nha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n\n# NOTE 5: Here's an example of using custom libraries to \nha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n# NOTE 6: Here we start to deal with conditional running of code that skips outputs if they have already been\n# created. This is often the first (and often most eficatious) optimization of code to run fast.\ndo_30s_resample = False\nif do_30s_resample:\n    # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n    desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n    c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n    gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n\n# NOTE 7: An here, we see an incredibly slow approach that seems intuitive but is wrong\n# because it is 1000x slower than correct vectorized calculations (which are provided by numpy)\narray = raster_to_array(c_30s_unscaled_uri)\nfor row in range(array.shape[0]):\n    for col in range(array.shape[1]):\n        if array[row, col] &lt; 0:\n            array[row, col] = -9999 # Set negative values to the no-data-value\n\n\n\n# For reference, the correct way would have been as below. We will introduce hazelbean utils (like hb.as_array() soon.\narray = hb.as_array(input_path)\narray = np.where(array &lt; 0, -9999, array)",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-5-dealing-with-larger-than-memory-data",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-5-dealing-with-larger-than-memory-data",
    "title": "The stages of project complexity",
    "section": "",
    "text": "Depending on the size of the array, the numpy where command used above would fail with a MemoryError or something similar. Below you will the first way that I dealt with this (BAD CODE) and then the correct way. In either case, it is almost always true that the most likely solution to larger-than-memory situations is to apply your algorithm in chunks. We’ll do this in both cases.\n\n\nIn this code, I created a thing I made up, a “Tile Reference” which I implemented with geoecon_utils.Tr(). This returned a set of tiles, defined by their row, column, x_width and y_width. We used these to load just subsets of the array and do operations on the smaller thing.\n\nliteral_aggregation_to_5m_cell = False\nif literal_aggregation_to_5m_cell:\n    factor =  10\n    shape = (2160, 4320)\n    c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n    cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n    cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n    for tile in c_30s_tr.tr_frame:\n        tile_array = c_30s_tr.tile_to_array(tile)\n        print 'Aggregating tile', tile\n        for row in range(c_30s_tr.chunkshape[0] / factor):\n            for col in range(c_30s_tr.chunkshape[1] / factor):\n                cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n                cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n    cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n    print gu.desc(cell_sum_5m)\n    gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n\n\n\nThe better way is to use a function that builds in the tiling functionality. Eventually this will expand to multi-computer approaches, but for now, we’ll just use the local but parallelized hb.raster_calculator()\nhb.raster_calculator(this)",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/project_complexity.html#project-complexity-level-6-systematic-file-management-file-management.",
    "href": "earth_economy_devstack/project_complexity.html#project-complexity-level-6-systematic-file-management-file-management.",
    "title": "The stages of project complexity",
    "section": "",
    "text": "The final level of complexity we will discuss (before just using the Earth Economy Devstack approach) arises when the number of files that must be managed becomes a challenge both for performance reasons and the challenges of managing complexity.\nOne example where this comes up is when a computation requires writing tiles of output. In many big-data applications and in most of the very large datasets that are available online, the data are themselves stored in tiles. On the one hand, this is nice because it automatically suggests a chunk-by-chunk parallelization strategy. On the other hand, it quickly becomes challenging when, for instance, you want to look at an area of interest (AOI) that spans multiple tiles. There are a plethora of software solutions to deal with this, such as GDAL’s Virtual Raster (VRT) file type, but many of these have limitations.\nWhen the computation in question requires many complex steps which might be contingent on other intermediate products from adjacent tiles, even some of the most cutting-edge solutions that implement complex tiling architecture (like DASK with rioxarray) will not be sufficient. This was the challenge that arose when doing downscaling with the SEALS model, especially when the algorithm had to be trained on such tiles millions of times. The optimized algorithm in this complex data and computation dependency-tree situation required a new tool, which would also have to address all of the above challenges in project complexity.\nThis led to ProjectFlow, one of the key tools within the Earth Economy Devstack and a part of Hazelbean.",
    "crumbs": [
      "Hazelbean",
      "Levels of Complexity"
    ]
  },
  {
    "objectID": "earth_economy_devstack/release_notes.html",
    "href": "earth_economy_devstack/release_notes.html",
    "title": "Release Notes",
    "section": "",
    "text": "Changed versioning to match the GTAP-InVEST project to which its development was associated. In this, we had:\n\n1.0 WWF, 141 regions, no AEZs\n2.0 ECN, 36 regions, 18 aez, endogenous LUC\n3.0 PNAS, 36 regions, 18 aez, endogenous LUC, new policies\n4.0 CWON, 50 regions, 18 aez, endogenous LUC, GTAP v11 data, recursive dynamic\n\nNew feature: 2-stage downscaling is now default (rather than implemented in the GTAP-InVEST code). Users can optionally give either (but at least one) coarse_projections_input_path or regional_projections input path. The two different downscaling scales are referred to as regional, where we assume a vector-file of regions that have corresponding LUC for different classes, or coarse, where we assume a raster file of LUC classes.\nAdded documentation of different sceanrio_definition.csv paramters:\n\nscenario_label: User-defined string to identify the scenario. Will be referenced (NYI) by automatic plotting calls according to baseline_reference_label or comparison_counterfactual_labels\nscenario_type: One of [‘baseline’, ‘bau’, ‘policy’].\n\nBaseline scenarios aren’t downscaled (because they have been observed), but they still need a line in the scenarios file to define which model defined them and in what year they have data.\nBAU scenarios are downscaled, relative either to the baseline sceanrio or the previous year if not the first year. Technically bau scenarios are no different than policy scenarios except insofar as they have different interpretation about if e.g. a policy is effective relative to BAU.\nPolicy scenarios are downscaled, relative to the baseline scenario or the previous year if not the first year and might be compared to BAU (NYI)\n\naoi: One of [‘global’, , or canonical_path to a gpkg vector file]\nexogenous_label: String representing some set of exogenous drivers, typically defined by the SSP database for population, TFP growth, GDP, etc. By convention, if its an SSP, format it as e.g. ssp3\nclimate_label: String representing the RCP in use. Format as e.g. rcp45\nmodel_label: String representing the model used to generate the scenario. Format as e.g. luh2-message.\ncounterfactual_label: String representing some counterfactual. For example, in the latest release, we have a new test file that has 3 counterfactuals:\n\nbau: defined just by the luh2 results\nbau_shift: shift the luh2 results by the values in regional_projections_path\njust_shift: downscale only the shift using a proportional coarse downscaling\n\nyears: space-delimited list of years that will be downscaled.\nbaseline_reference_label: scenario_label to which dowscaled scenarios might be compared (NYI)\nbase_years: space-delimited list of years that are in the observed data. If not calabrating the algorithm, these will just be a single year (which will be identical to key_base_year), but when calibrating, these are the years that define the training set. For future applications, these years could also be the validation/backcasting years\nkey_base_year: The final base year from which the model transitions from “oberved” to “projected” data.\ncomparison_counterfactual_labels: scenario_label to which policy scenarios might be compared (e.g., the bau), NYI.\ncoarse_projections_input_path: canonical path to a file that contains the coarse gridcells of how much each changing class will change. Typically this is a netcdf file with a layer for each year and each proportional LULC class as a variable, but there is great variation on HOW these are defined. This, the task tree must define a task that is capable of converting this file into the next step, which is geotiffs per class.\nlulc_src_label: There are two different correspondences currently idntified. The first is a correspondence between src and simplification labels. Currently implemented, src is usually ‘esa’\nlulc_simplification_label: Simplification label is what it gets simplified into, currently implemented to be ‘seals7’. IN PROCESS OF DEPRECATING. Use just the correspondence’s src and dst id column headers to define these.\nlulc_correspondence_path: Path to a correspondence file. Defines lulc src and lulc dst labels.\ncoarse_src_label: IN PROCESS OF DEPRECATING.\ncoarse_simplification_label: IN PROCESS OF DEPRECATING.\ncoarse_correspondence_path: Path to a correspondence of the coarse_gridded to lulc dst\nlc_class_varname: attempt at making netcdf work for all possible organizations. not sure if we should keep this.\ndimensions: attempt at making netcdf work for all possible organizations. not sure if we should keep this.\ntime_dim_adjustment: attempt at making netcdf work for all possible organizations. not sure if we should keep this.\ncalibration_parameters_source: file path to a csv file that contains the calibration parameters. Could also be directory i think for when there are zone-specific ones.\nbase_year_lulc_path: path to LULC used for the key_base_year.\nregional_projections_input_path: path to a region_id and region_label mapped to different class changes.\n\nIn simplifying lulc_src_label etc, we have clarified a few terms:\n\ncorrespondence: a one to many mapping with no extra information. Contains src_id, dst_id, src_label, dst_label at minimum No additional information is stored in this file. The src and dst labels are defined in the file columns. So for instance in esa_seals7_correspondnece.csv, the file name could have been anything, but the actual src_label and dst_label strings themselves are defined in the columns esa_id and seals7_id.\nstructured_mapping: contains multiple one to many relationships, but also requires that the first src listed is unique. Note that if you groupby, you have to drop the previously disaggregate label (or perhaps nyi list it in a curled-up dimension with cat-ears?).\nunstructured_mapping: like structured_mapping but doesn’t require that the first src listed is unique. If you take a structured_mapping and groupby on it, it will necessarily result in an unstructured_mapping.\n\nNOTE: The proposed changes to correspondences etc. haven’t been incorporated into the GTAP-InVEST code yet. do it. ## Update v0.5.0\n\nDownloading of base data now works.\n\n\n\nNow all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.\nIf no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file.",
    "crumbs": [
      "SEALS",
      "Release Notes"
    ]
  },
  {
    "objectID": "earth_economy_devstack/release_notes.html#update-v4.0.0",
    "href": "earth_economy_devstack/release_notes.html#update-v4.0.0",
    "title": "Release Notes",
    "section": "",
    "text": "Changed versioning to match the GTAP-InVEST project to which its development was associated. In this, we had:\n\n1.0 WWF, 141 regions, no AEZs\n2.0 ECN, 36 regions, 18 aez, endogenous LUC\n3.0 PNAS, 36 regions, 18 aez, endogenous LUC, new policies\n4.0 CWON, 50 regions, 18 aez, endogenous LUC, GTAP v11 data, recursive dynamic\n\nNew feature: 2-stage downscaling is now default (rather than implemented in the GTAP-InVEST code). Users can optionally give either (but at least one) coarse_projections_input_path or regional_projections input path. The two different downscaling scales are referred to as regional, where we assume a vector-file of regions that have corresponding LUC for different classes, or coarse, where we assume a raster file of LUC classes.\nAdded documentation of different sceanrio_definition.csv paramters:\n\nscenario_label: User-defined string to identify the scenario. Will be referenced (NYI) by automatic plotting calls according to baseline_reference_label or comparison_counterfactual_labels\nscenario_type: One of [‘baseline’, ‘bau’, ‘policy’].\n\nBaseline scenarios aren’t downscaled (because they have been observed), but they still need a line in the scenarios file to define which model defined them and in what year they have data.\nBAU scenarios are downscaled, relative either to the baseline sceanrio or the previous year if not the first year. Technically bau scenarios are no different than policy scenarios except insofar as they have different interpretation about if e.g. a policy is effective relative to BAU.\nPolicy scenarios are downscaled, relative to the baseline scenario or the previous year if not the first year and might be compared to BAU (NYI)\n\naoi: One of [‘global’, , or canonical_path to a gpkg vector file]\nexogenous_label: String representing some set of exogenous drivers, typically defined by the SSP database for population, TFP growth, GDP, etc. By convention, if its an SSP, format it as e.g. ssp3\nclimate_label: String representing the RCP in use. Format as e.g. rcp45\nmodel_label: String representing the model used to generate the scenario. Format as e.g. luh2-message.\ncounterfactual_label: String representing some counterfactual. For example, in the latest release, we have a new test file that has 3 counterfactuals:\n\nbau: defined just by the luh2 results\nbau_shift: shift the luh2 results by the values in regional_projections_path\njust_shift: downscale only the shift using a proportional coarse downscaling\n\nyears: space-delimited list of years that will be downscaled.\nbaseline_reference_label: scenario_label to which dowscaled scenarios might be compared (NYI)\nbase_years: space-delimited list of years that are in the observed data. If not calabrating the algorithm, these will just be a single year (which will be identical to key_base_year), but when calibrating, these are the years that define the training set. For future applications, these years could also be the validation/backcasting years\nkey_base_year: The final base year from which the model transitions from “oberved” to “projected” data.\ncomparison_counterfactual_labels: scenario_label to which policy scenarios might be compared (e.g., the bau), NYI.\ncoarse_projections_input_path: canonical path to a file that contains the coarse gridcells of how much each changing class will change. Typically this is a netcdf file with a layer for each year and each proportional LULC class as a variable, but there is great variation on HOW these are defined. This, the task tree must define a task that is capable of converting this file into the next step, which is geotiffs per class.\nlulc_src_label: There are two different correspondences currently idntified. The first is a correspondence between src and simplification labels. Currently implemented, src is usually ‘esa’\nlulc_simplification_label: Simplification label is what it gets simplified into, currently implemented to be ‘seals7’. IN PROCESS OF DEPRECATING. Use just the correspondence’s src and dst id column headers to define these.\nlulc_correspondence_path: Path to a correspondence file. Defines lulc src and lulc dst labels.\ncoarse_src_label: IN PROCESS OF DEPRECATING.\ncoarse_simplification_label: IN PROCESS OF DEPRECATING.\ncoarse_correspondence_path: Path to a correspondence of the coarse_gridded to lulc dst\nlc_class_varname: attempt at making netcdf work for all possible organizations. not sure if we should keep this.\ndimensions: attempt at making netcdf work for all possible organizations. not sure if we should keep this.\ntime_dim_adjustment: attempt at making netcdf work for all possible organizations. not sure if we should keep this.\ncalibration_parameters_source: file path to a csv file that contains the calibration parameters. Could also be directory i think for when there are zone-specific ones.\nbase_year_lulc_path: path to LULC used for the key_base_year.\nregional_projections_input_path: path to a region_id and region_label mapped to different class changes.\n\nIn simplifying lulc_src_label etc, we have clarified a few terms:\n\ncorrespondence: a one to many mapping with no extra information. Contains src_id, dst_id, src_label, dst_label at minimum No additional information is stored in this file. The src and dst labels are defined in the file columns. So for instance in esa_seals7_correspondnece.csv, the file name could have been anything, but the actual src_label and dst_label strings themselves are defined in the columns esa_id and seals7_id.\nstructured_mapping: contains multiple one to many relationships, but also requires that the first src listed is unique. Note that if you groupby, you have to drop the previously disaggregate label (or perhaps nyi list it in a curled-up dimension with cat-ears?).\nunstructured_mapping: like structured_mapping but doesn’t require that the first src listed is unique. If you take a structured_mapping and groupby on it, it will necessarily result in an unstructured_mapping.\n\nNOTE: The proposed changes to correspondences etc. haven’t been incorporated into the GTAP-InVEST code yet. do it. ## Update v0.5.0\n\nDownloading of base data now works.",
    "crumbs": [
      "SEALS",
      "Release Notes"
    ]
  },
  {
    "objectID": "earth_economy_devstack/release_notes.html#update-v0.4.0",
    "href": "earth_economy_devstack/release_notes.html#update-v0.4.0",
    "title": "Release Notes",
    "section": "",
    "text": "Now all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.\nIf no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file.",
    "crumbs": [
      "SEALS",
      "Release Notes"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_magpie_tutorial.html",
    "href": "earth_economy_devstack/seals_magpie_tutorial.html",
    "title": "Applying SEALS to downscale MAgPIE outputs",
    "section": "",
    "text": "The Model of Agricultural Production and Its Impact on the Environment (MAgPIE) is a global food and land system modelling framework. It uses a wide range of spatially-explicit biophysical and socioeconomic information to model global land-use dynamics throughout the 21st century. MAgPIE operates at two spatial scales that include a coarse scale of twelve model regions with similar socioeconomic characteristics and a subregional level that combines biophysical information, such as suitability for agricultural production (crop yield potential, irrigation water requirements, travel time to urban markets etc.), to determine the cost-effectiveness of different land-use activities. Both these resolutions, however, are typically too coarse to assess how global land-use dynamics could drive landscape-scale changes and associated changes in biodiversity and ecosystems service supply, which is a key sustainability challenge.\nThe Spatial Economic Allocation Landscape Simulator (SEALS) model can therefore be applied to spatially allocate projected land-use dynamics to spatial scale that is relevant for assessing landscape and biodiversity change. SEALS uses empirically calibrated adjacency relationships, physical suitability and conversion eligibility information at a spatial resolution of 300 x 300 Meter to allocate land use activities across space.\nIn the following this tutorial describes how both the MAgPIE and SEALS models can be linked using some of the functionalities provided by MAgPIE and SEALS to processes model outputs (MAgPIE) or inputs (SEALS) respectively. This tutorial therefore assumes some familiarity with the MAgPIE model - in particular how to set up model runs. Basic tutorials on how to run the MAgPIE model can be found here https://magpiemodel.github.io/tutorials/. A full documentation of the MAgPIE model (release version 4.7.2) can be accessed under https://rse.pik-potsdam.de/doc/magpie/4.7.2/ (substitute the version number 4.7.2 with any newer release version number to get the latest documentation).\n\n\n\nThe MAgPIE model provides several output scripts that can be executed after a model run and are stored under scripts/output in the MAgPIE folder. Two scripts are particularly relevant for providing MAgPIE outputs to SEALS: the extra/disaggregation.R script and the extra/reportMAgPIE2SEALS.R script. The extra/disaggregation.R script disaggregates coarse-resolution land-use projections to a spatial resolution of 0.5 degrees and is run by default after each model run. The extra/reportMAgPIE2SEALS.R script, on the other hand, generates a NetCDF file that can readily be used by SEALS but is not run by default.\nThe output scripts can be selected and executed via the command line. Therefore, on the command prompt, navigate to the MAgPIE model folder and use the following command:\nRscript output.R\nOn the command line you are now asked for which of the model runs you would like to do the postprocessing. You can choose all, select a specific run, or run the postprocessing for a selection of runs by choosing search by the pattern.\n\nAfter this you can select the output script that you want to execute. Choose extra.\n\nNext select reportMAgPIE2SEALS.\n\nFinally, choose the run submission type, e.g. Direct execution.\n\nAll outputs are written to a subfolder in the output folder within the MAgPIE directory. This subfolder (or run folder) is created automatically at the beginning of a model run and its name is a combination of the run title and date (For further information see https://magpiemodel.github.io/tutorials/t06-changingconfig). Before you run reportMAgPIE2SEALS, please make sure that the script extra/disaggregation.R was executed without error. If the script extra/disaggregation.R was completed successfully, the run folder should contain the file cell.land_0.5_share.mz. Once the script extra/reportMAgPIE2SEALS.R has finished the run folder should contain a file named cell.land_0.5_SEALS_&lt;run title&gt;.nc. This file contains spatially-explicit information on the share of total land per grid cell used for different land-use activities (cropland, pasture, forest, urban etc.) projected until 2050, which can now be used as an input for SEALS.\n\n\n\nIf you are familiar with setting up a start script for MAgPIE runs (see https://magpiemodel.github.io/tutorials/t07-startscript), an alternative way of generating MAgPIE outputs for SEALS is by adding \"extra/reportMAgPIE2SEALS\" to the setting\ncfg$output &lt;- c(\"output_check\", \"extra/disaggregation\", \"rds_report\")\nso that it reads\ncfg$output &lt;- c(\"output_check\", \"extra/disaggregation\", \"rds_report\", \"extra/reportMAgPIE2SEALS\")\nAs the order of the vector determines the order of execution, please make sure that \"extra/reportMAgPIE2SEALS\" is called after \"extra/disaggregation\".\n\n\n\n\nAfter generating the output cell.land_0.5_SEALS_&lt;run title&gt;.nc, the following describes how this information can be used as input for the SEALS model.\nFirst, we need to relate the land use types of the MAgPIE model to the land cover types used by the SEALS model. While MAgPIE separates seven different main land types (cropland, grassland, primary forest, secondary forest, forestry, non-forest vegetation & urban), the SEALS model only allocates changes for 5 main land types (urban, cropland, grassland, forest & other natural vegetation). Below a mapping between the different land types is shown.\nsrc_id,dst_id,src_label,dst_label,src_description,dst_description\n6,1,urban,urban,urban land,\n1,2,crop,cropland,cropland,\n2,3,past,grassland,grassland,\n3,4,primforest,forest,primary forest,\n4,4,secdforest,forest,secondary forest,\n5,4,forestry,forest,forestry,\n7,5,other,othernat,non-forest vegetation,\nCopy and paste this mapping into a CSV file and save it, e.g. under base_data/seals/default_inputs/magpie_seals7_correspondence.csv. The folder base_data/seals/default_inputs also contains other mappings such as a mapping betweehn LUH2v2 land types and SEALS. This mapping can now be used to set up the scenario_definitions.csv. The scenario_definitions.csv provides SEALS with all necessary information on the input data (or MAgPIE outputs), including the path to the MAgPIE outputs. The scenario_definitions.csv also allows the user to set up multiple SEALS runs (e.g. in the case of different scenario runs) in one go. One way to better understand the scenario_definitions.csv is to run SEALS on the test data without further modification. This will automatically generate a scenario_definitions.csv in a test project folder. This scenario_definitions.csv can then be tailored to your project needs.\nThe following shows how an example scenario_definitions_tutorial.csv is created that provides relevant information for a set of different MAgPIE test runs. Assume we have created land use projections for three different test scenarios: a reference ‘business-as-usual’ scenario (weeklyTests_SSP2EU-Ref), a low ambition mitigation scenario, in which current nationally determined contributions (NDCs) under the Paris agreement for the land sector are fulfilled (weeklyTests_SSP2EU-NDC) and a highly ambitious mitigation scenario in line with the 1.5 degree target (weeklyTests_SSP2EU-PkBudg650). For each of the scenarios, we have created gridded land-use projections in a format that can be read by SEALS using the output script extra/reportMAgPIE2SEALS.R (see above).\nHere, the scenario_definitions_tutorial.csv contains a line for each scenario specified under scenario_label. Note that the Baseline always needs to be defined as well. In the years column you can specify the years for which SEALS should do a downscaling. Multiple years can be separated by a space.\n\nSet the path to the gridded MAgPIE outputs in the column coarse_projections_input_path\n\nThe lulc_correspondence_path gives the path to a mapping between ESA CCI land cover classes and the aggregated land cover types used in SEALS.\n\nThe default mapping in SEALS is stored under base_data\\seals\\default_inputs\\esa_seals7_correspondence.csv. For MAgPIE-SEALS couplings, however, it is recommended to use a mapping that classifies “crop_natural_mosaic” as “othernat” and “natural_crop_mosaic” as “forest” to better align the total cropland extent of the ESA map with the cropland initialisation in MAgPIE. That there is an overestimation of the cropland extent in the ESA CCI data is a well-know issue. Thus, the lines in esa_seals7_correspondence.csv\n6,,30,2,crop_natural_mosaic,cropland\n7,,40,2,natural_crop_mosaic,cropland\nare changed to\n6,,30,2,crop_natural_mosaic,othernat\n7,,40,2,natural_crop_mosaic,forest\nin esa_seals7_correspondence_mosaic-natural.csv.\nNext, we set the path to the mapping between MAgPIE land use types to SEALS land cover types, as defined above\n\nWe also specify a set of regressors used during downscaling. Tables of trained model coefficients used by SEALS can be found under base_data/seals/default_inputs. Here we can also align some model constraints between MAgPIE and SEALS. If, for example, compliance with currently implemented land policies such as protected areas is assumed in MAgPIE, these constraints also need to be reflected during the downscaling. We here therefore add an additional multiplicative regressor that precludes expansion of agricultural land (cropland and grassland) in legally protected areas as reported by the WDPA database, which is in line with the assumption in MAgPIE.\n\nSimilarly, the same functionality can also be used to specify policy scenarios, such as the expansion of protected areas in line with the 30x30 goal of the Global Biodiversity Framework. The seals/static_regressors/magpie_30by30.tif contains both currently protected areas from the WDPA database and areas that might be protected under a global 30x30 protection scenario.\n\nFinally, the path to the regressor used during the downscaling must also be added to the scenario_definitions.csv file, while the base land-cover data base_year_lulc_path must refer to the same year as defined in the Baseline case.\n\nSave the scenario_definitions_tutorial.csv under ./input in your SEALS project folder and don’t forget to add it to your SEALS run file and you are good to go.",
    "crumbs": [
      "SEALS",
      "Magpie Tutorial"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_magpie_tutorial.html#overview",
    "href": "earth_economy_devstack/seals_magpie_tutorial.html#overview",
    "title": "Applying SEALS to downscale MAgPIE outputs",
    "section": "",
    "text": "The Model of Agricultural Production and Its Impact on the Environment (MAgPIE) is a global food and land system modelling framework. It uses a wide range of spatially-explicit biophysical and socioeconomic information to model global land-use dynamics throughout the 21st century. MAgPIE operates at two spatial scales that include a coarse scale of twelve model regions with similar socioeconomic characteristics and a subregional level that combines biophysical information, such as suitability for agricultural production (crop yield potential, irrigation water requirements, travel time to urban markets etc.), to determine the cost-effectiveness of different land-use activities. Both these resolutions, however, are typically too coarse to assess how global land-use dynamics could drive landscape-scale changes and associated changes in biodiversity and ecosystems service supply, which is a key sustainability challenge.\nThe Spatial Economic Allocation Landscape Simulator (SEALS) model can therefore be applied to spatially allocate projected land-use dynamics to spatial scale that is relevant for assessing landscape and biodiversity change. SEALS uses empirically calibrated adjacency relationships, physical suitability and conversion eligibility information at a spatial resolution of 300 x 300 Meter to allocate land use activities across space.\nIn the following this tutorial describes how both the MAgPIE and SEALS models can be linked using some of the functionalities provided by MAgPIE and SEALS to processes model outputs (MAgPIE) or inputs (SEALS) respectively. This tutorial therefore assumes some familiarity with the MAgPIE model - in particular how to set up model runs. Basic tutorials on how to run the MAgPIE model can be found here https://magpiemodel.github.io/tutorials/. A full documentation of the MAgPIE model (release version 4.7.2) can be accessed under https://rse.pik-potsdam.de/doc/magpie/4.7.2/ (substitute the version number 4.7.2 with any newer release version number to get the latest documentation).",
    "crumbs": [
      "SEALS",
      "Magpie Tutorial"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_magpie_tutorial.html#create-magpie-outputs-for-seals-by-using-output-scripts",
    "href": "earth_economy_devstack/seals_magpie_tutorial.html#create-magpie-outputs-for-seals-by-using-output-scripts",
    "title": "Applying SEALS to downscale MAgPIE outputs",
    "section": "",
    "text": "The MAgPIE model provides several output scripts that can be executed after a model run and are stored under scripts/output in the MAgPIE folder. Two scripts are particularly relevant for providing MAgPIE outputs to SEALS: the extra/disaggregation.R script and the extra/reportMAgPIE2SEALS.R script. The extra/disaggregation.R script disaggregates coarse-resolution land-use projections to a spatial resolution of 0.5 degrees and is run by default after each model run. The extra/reportMAgPIE2SEALS.R script, on the other hand, generates a NetCDF file that can readily be used by SEALS but is not run by default.\nThe output scripts can be selected and executed via the command line. Therefore, on the command prompt, navigate to the MAgPIE model folder and use the following command:\nRscript output.R\nOn the command line you are now asked for which of the model runs you would like to do the postprocessing. You can choose all, select a specific run, or run the postprocessing for a selection of runs by choosing search by the pattern.\n\nAfter this you can select the output script that you want to execute. Choose extra.\n\nNext select reportMAgPIE2SEALS.\n\nFinally, choose the run submission type, e.g. Direct execution.\n\nAll outputs are written to a subfolder in the output folder within the MAgPIE directory. This subfolder (or run folder) is created automatically at the beginning of a model run and its name is a combination of the run title and date (For further information see https://magpiemodel.github.io/tutorials/t06-changingconfig). Before you run reportMAgPIE2SEALS, please make sure that the script extra/disaggregation.R was executed without error. If the script extra/disaggregation.R was completed successfully, the run folder should contain the file cell.land_0.5_share.mz. Once the script extra/reportMAgPIE2SEALS.R has finished the run folder should contain a file named cell.land_0.5_SEALS_&lt;run title&gt;.nc. This file contains spatially-explicit information on the share of total land per grid cell used for different land-use activities (cropland, pasture, forest, urban etc.) projected until 2050, which can now be used as an input for SEALS.\n\n\n\nIf you are familiar with setting up a start script for MAgPIE runs (see https://magpiemodel.github.io/tutorials/t07-startscript), an alternative way of generating MAgPIE outputs for SEALS is by adding \"extra/reportMAgPIE2SEALS\" to the setting\ncfg$output &lt;- c(\"output_check\", \"extra/disaggregation\", \"rds_report\")\nso that it reads\ncfg$output &lt;- c(\"output_check\", \"extra/disaggregation\", \"rds_report\", \"extra/reportMAgPIE2SEALS\")\nAs the order of the vector determines the order of execution, please make sure that \"extra/reportMAgPIE2SEALS\" is called after \"extra/disaggregation\".",
    "crumbs": [
      "SEALS",
      "Magpie Tutorial"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_magpie_tutorial.html#setting-up-seals-to-downscale-magpie-outputs",
    "href": "earth_economy_devstack/seals_magpie_tutorial.html#setting-up-seals-to-downscale-magpie-outputs",
    "title": "Applying SEALS to downscale MAgPIE outputs",
    "section": "",
    "text": "After generating the output cell.land_0.5_SEALS_&lt;run title&gt;.nc, the following describes how this information can be used as input for the SEALS model.\nFirst, we need to relate the land use types of the MAgPIE model to the land cover types used by the SEALS model. While MAgPIE separates seven different main land types (cropland, grassland, primary forest, secondary forest, forestry, non-forest vegetation & urban), the SEALS model only allocates changes for 5 main land types (urban, cropland, grassland, forest & other natural vegetation). Below a mapping between the different land types is shown.\nsrc_id,dst_id,src_label,dst_label,src_description,dst_description\n6,1,urban,urban,urban land,\n1,2,crop,cropland,cropland,\n2,3,past,grassland,grassland,\n3,4,primforest,forest,primary forest,\n4,4,secdforest,forest,secondary forest,\n5,4,forestry,forest,forestry,\n7,5,other,othernat,non-forest vegetation,\nCopy and paste this mapping into a CSV file and save it, e.g. under base_data/seals/default_inputs/magpie_seals7_correspondence.csv. The folder base_data/seals/default_inputs also contains other mappings such as a mapping betweehn LUH2v2 land types and SEALS. This mapping can now be used to set up the scenario_definitions.csv. The scenario_definitions.csv provides SEALS with all necessary information on the input data (or MAgPIE outputs), including the path to the MAgPIE outputs. The scenario_definitions.csv also allows the user to set up multiple SEALS runs (e.g. in the case of different scenario runs) in one go. One way to better understand the scenario_definitions.csv is to run SEALS on the test data without further modification. This will automatically generate a scenario_definitions.csv in a test project folder. This scenario_definitions.csv can then be tailored to your project needs.\nThe following shows how an example scenario_definitions_tutorial.csv is created that provides relevant information for a set of different MAgPIE test runs. Assume we have created land use projections for three different test scenarios: a reference ‘business-as-usual’ scenario (weeklyTests_SSP2EU-Ref), a low ambition mitigation scenario, in which current nationally determined contributions (NDCs) under the Paris agreement for the land sector are fulfilled (weeklyTests_SSP2EU-NDC) and a highly ambitious mitigation scenario in line with the 1.5 degree target (weeklyTests_SSP2EU-PkBudg650). For each of the scenarios, we have created gridded land-use projections in a format that can be read by SEALS using the output script extra/reportMAgPIE2SEALS.R (see above).\nHere, the scenario_definitions_tutorial.csv contains a line for each scenario specified under scenario_label. Note that the Baseline always needs to be defined as well. In the years column you can specify the years for which SEALS should do a downscaling. Multiple years can be separated by a space.\n\nSet the path to the gridded MAgPIE outputs in the column coarse_projections_input_path\n\nThe lulc_correspondence_path gives the path to a mapping between ESA CCI land cover classes and the aggregated land cover types used in SEALS.\n\nThe default mapping in SEALS is stored under base_data\\seals\\default_inputs\\esa_seals7_correspondence.csv. For MAgPIE-SEALS couplings, however, it is recommended to use a mapping that classifies “crop_natural_mosaic” as “othernat” and “natural_crop_mosaic” as “forest” to better align the total cropland extent of the ESA map with the cropland initialisation in MAgPIE. That there is an overestimation of the cropland extent in the ESA CCI data is a well-know issue. Thus, the lines in esa_seals7_correspondence.csv\n6,,30,2,crop_natural_mosaic,cropland\n7,,40,2,natural_crop_mosaic,cropland\nare changed to\n6,,30,2,crop_natural_mosaic,othernat\n7,,40,2,natural_crop_mosaic,forest\nin esa_seals7_correspondence_mosaic-natural.csv.\nNext, we set the path to the mapping between MAgPIE land use types to SEALS land cover types, as defined above\n\nWe also specify a set of regressors used during downscaling. Tables of trained model coefficients used by SEALS can be found under base_data/seals/default_inputs. Here we can also align some model constraints between MAgPIE and SEALS. If, for example, compliance with currently implemented land policies such as protected areas is assumed in MAgPIE, these constraints also need to be reflected during the downscaling. We here therefore add an additional multiplicative regressor that precludes expansion of agricultural land (cropland and grassland) in legally protected areas as reported by the WDPA database, which is in line with the assumption in MAgPIE.\n\nSimilarly, the same functionality can also be used to specify policy scenarios, such as the expansion of protected areas in line with the 30x30 goal of the Global Biodiversity Framework. The seals/static_regressors/magpie_30by30.tif contains both currently protected areas from the WDPA database and areas that might be protected under a global 30x30 protection scenario.\n\nFinally, the path to the regressor used during the downscaling must also be added to the scenario_definitions.csv file, while the base land-cover data base_year_lulc_path must refer to the same year as defined in the Baseline case.\n\nSave the scenario_definitions_tutorial.csv under ./input in your SEALS project folder and don’t forget to add it to your SEALS run file and you are good to go.",
    "crumbs": [
      "SEALS",
      "Magpie Tutorial"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_quickstart.html",
    "href": "earth_economy_devstack/seals_quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "Quickstart\nThe simplest way to run SEALS is to clone the repository and then open run_test_standard.py in your preferred editor. Then, update the values in ENVIRONMENT SETTINGS near the top of run_test_standard.py for your local computer (ensuring this points to directories you have write-access for and is not a virtual/cloud directory).\nFinally, run seals via the command line with the command python run_test_standard.py in the appropriate directory. If configured correctly, SEALS will download the required data at runtime (assuming you point to a data bucket you have access to). The default bucket is publicly available, though other configurations are provided for alternate uses.",
    "crumbs": [
      "SEALS",
      "Quickstart"
    ]
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#getting-set-up",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#getting-set-up",
    "title": "Justin Andrew Johnson",
    "section": "Getting set up",
    "text": "Getting set up\n\n\nMake sure you have followed all of the steps in the installation page.\n\nIn particular, Clone the SEALS and Hazelbean repositories in the correct location, as described here\nYou will know you’ve got them installed correctly if your VS Code Explorer tab shows the repositories without an error message (Figure 1)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#explore-the-seals-code",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#explore-the-seals-code",
    "title": "Justin Andrew Johnson",
    "section": "Explore the SEALS code",
    "text": "Explore the SEALS code\n\n\nIn the VS Code Explorer tab, navigate to your seals_dev directory (Figure 1)\n\nQuick note about file organization\n\nThe root directory of seals_dev contains more than just the seals library, such as directories for scripts, images, etc.\nThe library itself is in the seals subdirectory seals_dev/seals which may seem redundant but is necessary for the way Python imports work.\nIf you inspect the seals directory, you will see an __init__.py file. This make Python able to import the directory as a package.\n\nYou will also see a seals_main.py file. This is where most of the actual logic of seals is."
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#run-files",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#run-files",
    "title": "Justin Andrew Johnson",
    "section": "Run files",
    "text": "Run files\n\n\n\n\nOne does not simply run a main.py (Figure 1)\n\nInstead, we’re going to have a “run file” that sets up the code and then runs the seals_main.py file\nOpen up the run_test_standard.py file in the seals directory (Figure 2)\n\nWe will setup this file and then finally run it (in debug mode via the launch configs in the earth_economy_devstack repository)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#setting-up-the-run-file",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#setting-up-the-run-file",
    "title": "Justin Andrew Johnson",
    "section": "Setting up the run file",
    "text": "Setting up the run file\n::: r-fit-text - The run file begins with standard python imports - Then in the if __name__ == '__main__': block, we define the project directory and initialize the project flow object - The reason for putting it in this block is so that you don’t accidentally run the code when you import the file in another script - This file then creates a ProjectFlow objected, assigned to the variable p. - Python is an object-oriented programming langage - The hb.ProjectFlow() defines a class, which is like a recipe for an object - When we call it, it generates on object of that class, which we assign to the variable p\nimport os, sys\nimport seals_utils\nimport seals_initialize_project\nimport hazelbean as hb\nimport pandas as pd\n\nmain = ''\nif __name__ == '__main__':\n:::\n\n## Directories and project name\n\n::: r-fit-text\n-   SEALS (and the EE Devstack) assumes (or softly requires) that you put all code and data somewhere relative to the user's home directory `os.path.expanduser('~')`\n    -   Can put it in sub-directories with `extra_dirs = ['Files', 'seals', 'projects']`\n-   If you followed the EE method, you will have already created the `seals` directory at `&lt;user_dir&gt;/Files/seals`\n    -   In the `seals` directory, your code is in `seals_dev`\n    -   In the `seals` directory, you also will have a `projects` directory\n        -   This is created automatically if its not there\n        -   All data and outputs will be saved in this directory\n            -   As a best practice, you should not save data in the `seals_dev` directory\n-   Given the directory structure above, p.project_name will also be use\n\n:::\n\n\n\n\n# files that already exist. \np.user_dir = os.path.expanduser('~')        \n::: r-fit-text\np.extra_dirs = ['Files', 'seals', 'projects']\np.project_name = 'test_examples'\np.project_name = p.project_name + '_' + hb.pretty_time() # If don't you want to recreate everything each time, comment out this line.\n:::\n\n# Based on the paths above, set the project_dir. All files will be created in this directory.\np.project_dir = os.path.join(p.user_dir, os.sep.join(p.extra_dirs), p.project_name)\np.set_project_dir(p.project_dir)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#using-objects",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#using-objects",
    "title": "Justin Andrew Johnson",
    "section": "Using Objects",
    "text": "Using Objects\n\n\nThe p object we created will organize input variables (called attributes when assigned to an object)\n\nLike this: p.attribute_name = 'ItsName'\n\nThe p object also has functions tied specificially to it (called methods when assigned to an object)\n\nSuch as: p.validate_name()\nMethods operate on the object that defined it\n\nSo validate_name() is specifically looking at the p object, often doing something to the p object, like fixing value of p.attribute_name"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#scenario-definitions-csv",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#scenario-definitions-csv",
    "title": "Justin Andrew Johnson",
    "section": "Scenario definitions CSV",
    "text": "Scenario definitions CSV\n\n\nThe scenario_definitions file specifies what defines the many different scenarios you want to run\n\nEach row will be one scenario\nEach time the model runs a new scenario, it will update its attributes based on this row\n\nIf you haven’t run SEALS yet, you won’t have a scenario_defintions file, so it will download the default one on the first run\n\n    p.scenario_definitions_filename = 'test_standard_scenarios.csv' \n    p.scenario_definitions_path = os.path.join(p.input_dir, p.scenario_definitions_filename)\n    seals_initialize_project.initialize_scenario_definitions(p)"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#scenario-types",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#scenario-types",
    "title": "Justin Andrew Johnson",
    "section": "Scenario types",
    "text": "Scenario types\n\nScenario type determines if it is historical (baseline) or future (anything else) as well as what the scenario should be compared against. I.e., Policy minus BAU.\n\np.scenario_type = 'bau'"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#automatically-downloading-data",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#automatically-downloading-data",
    "title": "Justin Andrew Johnson",
    "section": "Automatically downloading data",
    "text": "Automatically downloading data\n\n\nThis computing stack also uses hazelbean to automatically download needed data at run time.\n\nIn the code block below, we set p.base_data_dir to a location where we want to store lots of very large files.\nHazelbean will look here for certain files that are necessary and will download them from a cloud bucket if they are not present. T\nThis also lets you use the same base data across different projects.\n\nThe final directory has to be named base_data to match the naming convention on the google cloud bucket.\n\np.base_data_dir = os.path.join('Files/base_data')"
  },
  {
    "objectID": "earth_economy_devstack/seals_walkthrough_marked.html#running-the-model",
    "href": "earth_economy_devstack/seals_walkthrough_marked.html#running-the-model",
    "title": "Justin Andrew Johnson",
    "section": "Running the model",
    "text": "Running the model\n\nAfter doing the above steps, you should be ready to run run_test_seals.py. Upon starting, SEALS will report the “task tree” of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.\nOnce the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:\n[THIS IS NOT THE CORRECT IMAGE]"
  },
  {
    "objectID": "gtap_invest/index.html",
    "href": "gtap_invest/index.html",
    "title": "GTAP-InVEST Home",
    "section": "",
    "text": "GTAP-InVEST is an Earth-Economy model that calculates how economic activity affects the environment, and vice versa, on global to local scales. GTAP-InVEST can assess:\n\nWhat “Nature Smart” policies are good for both the economy and the environment?\nWhat impacts and dependencies link the environment and the economy?\nWhat happens to the economy when ecosystems become degraded?\nHow can we produce enough food while not undermining our critical natural capital?\nWhere is land-use change (e.g., deforestation) most likely to happen and what drives this?\nHow does climate change affect natural capital and the value it provides (via ecosystem services).\n\nThe figure below illustrates how GTAP-InVEST connects economic models with environmental models.\n\nAlternatively, see a recent presentation I gave at the Chilean Central Bank’s Annual Conference below or this Youtube playlist: Earth-Economy Modeling Playlist.\n\n\nOn a more technical level, GTAP-InVEST is a global, earth-economy model that integrates a computable general equilibrium (CGE) model from the Global Trade Analysis Project (GTAP), a land-use change (LUC) model, and a ecosystem services model, InVEST (Integrated Valuation of Ecosystem Services and Tradeoffs), provided by the Natural Capital Project. It has most recently been documented in The Proceedings of the National Academy of Sciences in Johnson et al. (2023), https://www.pnas.org/doi/10.1073/pnas.2220401120. This page is the GTAP-InVEST project site, which describes the model in general terms. See also the GTAP-InVEST User Guide or the download page of results from the PNAS article. Additionally, we are hosting a live, visualization tool at https://mygeohub.org/tools/gtapinvest. \nGTAP-InVEST is open-source and can be accessed at github.com/jandrewjohnson/gtap_invest. However, this version of GTAP-InVEST relies on two proprietary elements: one must have a GTAP database license for version 10+ and one must have a GEMPACK license to access the CGE solver we used. See https://www.gtap.agecon.purdue.edu/ for access to these resources.\nGTAP-InVEST originated from several academic papers, including:\n\n“The Economic Case for Nature” (Johnson et al. 2021)\n“Projected landscape-scale repercussions of global action for climate and biodiversity protection” (von Jeetze et al. 2023, Nature Communications, https://www.nature.com/articles/s41467-023-38043-1)\n“Closing yield gap is crucial to avoid potential surge in global carbon emissions” (Suh et al. 2020, Global Environmental Change, https://doi.org/10.1016/j.gloenvcha.2020.102100)\n“Global Future: Assessing The Global Economic Impacts of Environmental Change to Support Policy-Making” (Roxburg et al. 2020, World Wildlife Fund, https://www.gtap.agecon.purdue.edu/resources/res_display.asp?RecordID=5987).\n\nThe GTAP-InVEST model is written in Python, C, R, and GEMPACK. Running the model requires a considerable amount of technical skill and involves multiple different programming languages and paradigms. Future versions of GTAP-InVEST will aim to simplify this software complexity, but for now, because this is the first “hard-linked” version of a global ecosystem service and general equilibrium model, there still exist many suboptimal design choices that were necessary in “gluing together” all of these models. Python is used as the main glue of the model while each of the preexisting models are coded in their default language. This section describes how to get the combined model software running.\nThe three building blocks of the model, documented in more detail below in the User Guide, are:\n\nThe GTAP-AEZ (Agroecological Zones) model, which calculates equilibrium prices, quantities, GDP, trade and many other economic variables, and most notably for this exercise, endogenously calculates how land is used in different sectors and how natural land is brought into economic use through agricultural, pasture and managed forest expansion.\nThe Spatial Economic Allocation Landscape Simulator (SEALS) model, which downscales regional estimates of land-use change from GTAP-AEZ to a high enough resolution (300m) to run ecosystem service models.\nInVEST, which takes the downscaled land-use, land-cover maps from SEALS to calculate changes in ecosystem service provision. These ecosystem service changes are then further expressed as shocks to the economy and used as inputs into a second run of GTAP-AEZ, which calculates how losses of ecosystem services affect economic performance.\n\n\nFor the most up-to-date documentation, please see the dynamically-updated User Guide. If you would like to analyze the raw results of the PNAS paper, please see full results from PNAS paper."
  },
  {
    "objectID": "gtap_invest/user_guide/es_impact.html",
    "href": "gtap_invest/user_guide/es_impact.html",
    "title": "6. Connecting InVEST outputs to GTAP inputs",
    "section": "",
    "text": "6. Connecting InVEST outputs to GTAP inputs\nIn the previous section, we described how we calculated the biophysical outputs of the InVEST model. These results on their own provide potentially useful results on the provision of ecosystem services. However, these biophysical changes alone are not enough to identify the impact on the economy. This section walks through the calculations for converting the raw InVEST biophysical outputs to Region-AEZ specific, factor-neutral productivity shocks.\n\n6.1. How Region-AEZ aggregated shocks are inputted into GTAP\nIn GTAP-InVEST, there are three primary ways in which changing environmental conditions enter the CGE model. These include shifting the land supply curve directly to reflect a change in production, implementing a land-augmenting or land-reducing technological changes, or shifting the production function via a factor-neutral productivity shock.\nIn this section, we describe the third type of shock, which was used for the pollination linkage. We implement this as a factor-neutral (also referred to as Hicks’-neutral) productivity shock, which changes the efficiency coefficient  in each impacted production function, scaled uniquely for each region  and production activity in the following production function:\n\nWe solve the system of equations in the GTAP model (documented extensively in the GTAP user resources) for equilibrium values of prices and quantities where supply equals demand in all markets and all other regions and sectors can be affected by the change. This interconnection of markets shows one of the important advances captured by using a general equilibrium approach: changes in one component of the model will change equilibrium production and input usage, leading to different overall levels of change in the economy depending on exactly which sector in which region is affected.\nFor the pollination shock, the key parameter then is . To identify this parameter, we applied the following algorithm to process outputs from the InVEST Pollinator Sufficiency model\n\nCalculate the total value in 2014 USD of crop production on each 10 arcsecond grid-cell.\n\nFor each grid-cell and for each agricultural production activity , multiply the production tonnage (Monfreda et al. 2008) by the price per ton of that crop, specific to the country the in which the grid-cell is located. These prices are produced from the FAOSTAT database, on which we applied the following missing-data procedure:\n\nIf a price was not available for a crop in a given country for the year 2000, use the 10-year moving average.\nIf 1.b.i cannot be calculated because of missing data, use the continental average price (or its moving average if needed).\nIf 1.b.ii cannot be calculated due to missing data, use the global average price.\n\nResample each activity-specific crop value map from 5 arcminutes to 10 arcseconds (to match the LULC map) using bilinear interpolation.\nAggregate all crop-specific production values from 1.b to get the total value of crop production in each 10 arcsecond grid-cell.\nCalculate the maximum loss of value for each crop in each grid-cell that would occur if zero pollination habitat existed. To get this, we multiplied the production value from 1.b by 1 minus the pollinator dependence of each crop, as identified in Klein et al. 2013.\nAggregate all crop-specific maximum production loss figures from 3 to get the total maximum loss of crop production value.\nCalculate the proportion of maximum value lost for a specific scenario by using the pollination sufficiency map outputted by the InVEST Pollination model for that scenario (see Sharp et al. 2020), categorizing all values greater than 0.3 as having sufficient pollination (threshold chosen based on the approach used in Chaplin-Kramer et al, 2019). In these grid-cells, assign them the value 1, indicating no loss from degraded pollination habitat. For all grid-cells in which pollination dependence is below 0.3, assign them the value 1-(1/.3)*pollionation_sufficiency, which scales the pollination dependence variable so that it is 1.0 right at the threshold but falls linearly to 0 when pollination sufficiency approaches zero.\nCalculate the crop production value lost for each scenario by multiplying the aggregate maximum cropland value lost (4) with the proportion of max value lost specific to that scenario (5).\nFor each AEZ-Region  (n=341), calculate the percent change in cropland value lost in the scenario minus the baseline value lost in 2014. Note that this means that we are only considering newly-lost crop production value and how it compares to the loss of value from pollinators already included in the baseline.\nScale the value in 7 so that it can be aggregated from the AEZ-Region to the Region (n=36) level. Assign this scaled value to the five pollination-dependent production activities (variables gro, ocr, osd, pfb, v_f). This identifies the factor-neutral productivity shock  for each activity and region.\nRerun the full GTAP-model but with the values for  as defined above.\n\n\nThis approach improves upon existing economic models that incorporate pollination. Specifically, past models quantified the contribution of pollination by multiplying the pollination yield-dependency ratio by the value of output for each crop (Gallai, Salles, Settele, & Vaissière, 2009; Lautenbach, Seppelt, Liebscher, & Dormann, 2012). This would provide a proxy estimate of value lost in a static case, but it does not incorporate how changes in production methods or factor usage would lead to different production choices. Other approaches employ CGE methods, such as in Bauer and Wing (2016), but these consider only the complete loss of pollination (and thereby do not consider any degree of spatial dependence between pollinators and pollination-dependent crops). Our approach improves on the literature by incorporating spatially explicit information on which land areas will experience loss in pollinator habitat and calculates the losses from changes in crop pollination, specifying how these losses arise from relevant scenarios of economic growth.\nOne important difference with our model from the CGE analyses of pollination impact discussed above is that our model goes beyond just calculating some arbitrarily large shock on pollination services, but instead calculates how a specific land-use change, calculated by GTAP-AEZ and represented with a high-resolution LULC map, leads to production effects. Calculating the spatially heterogenous impacts of a changed landscape generate informative results. Figure S.6.1.1 presents one example of this, showing how the precise configuration of where the land-use change happens can have a large effect on the productivity shock.\n\nFigure S.6.1: Complexity of landscape affects the degree of pollination service:\n\n\n6.2. Carbon Storage and Timber provision\nWe used the InVEST carbon storage model for both estimating carbon sequestration (this was subsequently valued using the social cost of carbon) and to estimate changes in productivity in the forestry sector. The InVEST carbon storage and sequestration model works by specifying carbon storage levels present in each of four carbon pools (above-ground, below-ground, soil, and dead matter) specific to each LULC class (see the land-use change downscaling section in this SI for specific LULC classes used, along with their parameters). We use parameters for the carbon storage model taken directly from the IPCC Tier 1 method (Ruesch and Gibbs 2008). The base InVEST model is intended to run for a single ecofloristic region, using carbon pool parameters specific to that region (Sharp et al. 2020). To run this globally, we developed separate carbon-pool tables for each of the approximately 125 carbon zones, where each carbon zone is defined as the unique combination of ecofloristic region, continent, and whether the forest is a frontier forest as specified by the IPCC (as in Ruesch and Gibbs 2008). To develop these tables, we built on work from Suh et al. (in submission), which recategorized ESA LULC data into seven functional types. We extended the classes considered to include carbon storage values for agriculture.\nFor the forestry sector, we used the outputs of the carbon storage model to calculate the productivity impacts in each region. Specifically, we calculated the percentage change in carbon storage on land identified by the GTAP-AEZ model as used for forestry. We averaged these estimates for each AEZ-Region to find the average change of biomass on potentially-harvestable land. We implemented this as a factor-neutral productivity shock for the forestry sector of each region according to the average percentage change for in that region. Using carbon storage as a proxy for biomass means that we are including more detail than approaches that are based strictly on forest area (e.g., Baldos et al. 2020), accounting for variation in ecofloristic region, frontier forest status, and other factors, based on field-estimates of aboveground biomass as summarized in Ruesch and Gibbs (2008). However, this proxy approach is very simple compared to detailed modeling of forest harvest. We were not able to compute such forestry models at the global scale, but forthcoming work from several research teams may make this possible for future applications.\n\n\n6.3. Marine Fisheries\nThe InVEST marine fisheries model has been deprecated, and moreover, it would not have been able to produce global estimates. To model the ecosystem changes in marine fisheries, we use outputs from the FISH-MIP program within Intersector Impact Model Intercomparison Project (ISIMIP, isimip.org). We use results from the EcoOcean and BOATS models, based on the GFDL-ESM2M and IPSL climate reanalysis (following the methods documented in Tittensor et al. 2018). The models we used are global food-web models that incorporate both climate change and human pressures on a global, 0.5-degree grid and outputs results for many (51 in the EcoOcean model) trophic and taxonomic groups with the age structure of the populations included. This model run assumed no ocean acidification and excluded diazotrophic fish species (per the FISH-MIP guidelines). These results are reported in figure S.6.3, reproduced with permission from Johnson et al. 2020, showing the biomass density of commercial species under the baseline 2011 condition and under the 2050 BAU.\n We chose to pair three of the FISH-MIP scenarios with our report scenarios as follows: first, our BAU scenario uses the FISH-MIP RCP8.5 and SSP5 scenario with BAU levels of fishing. To calculate the specific shocks given to GTAP, we extracted the total catch biomass (TCB) variable from the FISH-MIP database (hosted under the ISI-MIP data portal at www.isimip.org), which provide monthly and yearly observations of gridded total biomass of catchable, commercially valued species. For the BAU and SP scenarios, we defined the shock as the percentage change in TCB in each of the GTAP zones (augmented to include their 200-mile nautical claims). These shocks, defined in the supplemental data for this article, were applied as a factor-neutral productivity shock, scaling the TFP term up or down each regions’ marine fisheries production function. Note that results for marine fisheries were calculated by joining Marine Exclusive Economic regions with their associated country. We chose to report these results aggregated onto land regions for visual clarity, though the results themselves are calculated on the marine zones.\n[1] https://www.regimeshifts.org/about\n[2] R&D spending data from high-income countries is taken from Heisey and Fuglie (2018). For developing countries, we rely on data from two sources. Data after 1981 is available from the Agricultural Science and Technology Indicators database (https://www.asti.cgiar.org/), and we obtain data before 1981 from Pardey and Roseboom (1989) and Pardey, Roseboom, and Anderson (1991). Our R&D spending data for Eastern Europe and the Soviet Union come from Judd, Boyce, and Evenson (1991).",
    "crumbs": [
      "6. Connecting InVEST outputs to GTAP inputs"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtapv7_v11_sectors.html",
    "href": "gtap_invest/user_guide/gtapv7_v11_sectors.html",
    "title": "Sectors in v11",
    "section": "",
    "text": "Sectors in v11\nDescription (Detailed Sector Breakdown)\n\n\n\n\n\n\n\n\n\nid\nlabel\nName\nDescription\n\n\n\n\n1\npdr\nPaddy rice\nRice: seed, paddy (not husked)\n\n\n2\nwht\nWheat\nWheat: seed, other\n\n\n3\ngro\nCereal grains nec\nOther Grains: maize (corn), sorghum, barley, rye, oats, millets, other cereals\n\n\n4\nv_f\nVegetables, fruit, nuts\nVeg & Fruit: vegetables, fruit and nuts, edible roots and tubers, pulses\n\n\n5\nosd\nOil seeds\nOil Seeds: oil seeds and oleaginous fruit\n\n\n6\nc_b\nSugar cane, sugar beet\nCane & Beet: sugar crops\n\n\n7\npfb\nPlant-based fibers\nFibres crops\n\n\n8\nocr\nCrops nec\nOther Crops: stimulant; spice and aromatic crops; forage products; plants and parts of plants used primarily in perfumery, pharmacy, or for insecticidal, fungicidal or similar purposes; beet seeds (excluding sugar beet seeds) and seeds of forage plants; natural rubber in primary forms or in plates, sheets or strip, living plants; cut flowers and flower buds; flower seeds, unmanufactured tobacco; other raw vegetable materials nec\n\n\n9\nctl\nBovine cattle, sheep and goats, horses\nCattle: bovine animals, live, other ruminants, horses and other equines, bovine semen\n\n\n10\noap\nAnimal products nec\nOther Animal Products: swine; poultry; other live animals; eggs of hens or other birds in shell, fresh; reproductive materials of animals; natural honey; snails, fresh, chilled, frozen, dried, salted or in brine, except sea snails; edible products of animal origin n.e.c.; hides, skins and furskins, raw; insect waxes and spermaceti, whether or not refined or coloured\n\n\n11\nrmk\nRaw milk\nRaw milk\n\n\n12\nwol\nWool, silk-worm cocoons\nWool: wool, silk, and other raw animal materials used in textile\n\n\n13\nfrs\nForestry\nForestry: forestry, logging and related service activities\n\n\n14\nfsh\nFishing\nFishing: hunting, trapping and game propagation including related service activities, fishing, fish farms; service activities incidental to fishing\n\n\n15\ncoa\nCoal\nCoal: mining and agglomeration of hard coal, lignite and peat\n\n\n16\noil\nOil\nOil: extraction of crude petroleum, service activities incidental to oil and gas extraction excluding surveying (part)\n\n\n17\ngas\nGas\nGas: extraction of natural gas, service activities incidental to oil and gas extraction excluding surveying (part)\n\n\n18\noxt\nOther Extraction (formerly omn Minerals nec)\nOther Mining Extraction (formerly omn): mining of metal ores; other mining and quarrying\n\n\n19\ncmt\nBovine meat products\nCattle Meat: fresh or chilled; meat of buffalo, fresh or chilled; meat of sheep, fresh or chilled; meat of goat, fresh or chilled; meat of camels and camelids, fresh or chilled; meat of horses and other equines, fresh or chilled; other meat of mammals, fresh or chilled; meat of mammals, frozen; edible offal of mammals, fresh, chilled or frozen\n\n\n20\nomt\nMeat products nec\nOther Meat: meat of pigs, fresh or chilled; meat of rabbits and hares, fresh or chilled; meat of poultry, fresh or chilled; meat of poultry, frozen; edible offal of poultry, fresh, chilled or frozen; other meat and edible offal, fresh, chilled or frozen; preserves and preparations of meat, meat offal or blood; flours, meals and pellets of meat or meat offal, inedible; greaves\n\n\n21\nvol\nVegetable oils and fats\nVegetable Oils: margarine and similar preparations; cotton linters; oil-cake and other residues resulting from the extraction of vegetable fats or oils; flours and meals of oil seeds or oleaginous fruits, except those of mustard; vegetable waxes, except triglycerides; degras; residues resulting from the treatment of fatty substances or animal or vegetable waxes; animal fats\n\n\n22\nmil\nDairy products\nMilk: dairy products\n\n\n23\npcr\nProcessed rice\nProcessed Rice: semi- or wholly milled, or husked\n\n\n24\nsgr\nSugar\nSugar and molasses\n\n\n25\nofd\nFood products nec\nOther Food: prepared and preserved fish, crustaceans, molluscs and other aquatic invertebrates; prepared and preserved vegetables, pulses and potatoes; prepared and preserved fruits and nuts; wheat and meslin flour; other cereal flours; groats, meal and pellets of wheat and other cereals; other cereal grain products (including corn flakes); other vegetable flours and meals; mixes and doughs for the preparation of bakers’ wares; starches and starch products; sugars and sugar syrups n.e.c.; preparations used in animal feeding; lucerne (alfalfa) meal and pellets; bakery products; cocoa, chocolate and sugar confectionery; macaroni, noodles, couscous and similar farinaceous products; food products n.e.c.\n\n\n26\nb_t\nBeverages and tobacco products\nBeverages and Tobacco products\n\n\n27\ntex\nTextiles\nManufacture of textiles\n\n\n28\nwap\nWearing apparel\nManufacture of wearing apparel\n\n\n29\nlea\nLeather products\nManufacture of leather and related products\n\n\n30\nlum\nWood products\nLumber: manufacture of wood and of products of wood and cork, except furniture; manufacture of articles of straw and plaiting materials\n\n\n31\nppp\nPaper products, publishing\nPaper & Paper Products: includes printing and reproduction of recorded media\n\n\n32\np_c\nPetroleum, coal products\nPetroleum & Coke: manufacture of coke and refined petroleum products\n\n\n33\nchm\nChemical products\nManufacture of chemicals and chemical products\n\n\n34\nbph\nBasic pharmaceutical products\nManufacture of pharmaceuticals, medicinal chemical and botanical products\n\n\n35\nrpp\nRubber and plastic products\nManufacture of rubber and plastics products\n\n\n36\nnmm\nMineral products nec\nManufacture of other non-metallic mineral products\n\n\n37\ni_s\nFerrous metals\nIron & Steel: basic production and casting\n\n\n38\nnfm\nMetals nec\nNon-Ferrous Metals: production and casting of copper, aluminium, zinc, lead, gold, and silver\n\n\n39\nfmp\nMetal products\nManufacture of fabricated metal products, except machinery and equipment\n\n\n40\nele\nComputer, electronic and optical products\nManufacture of computer, electronic and optical products\n\n\n41\neeq\nElectrical equipment\nManufacture of electrical equipment\n\n\n42\nome\nMachinery and equipment nec\nManufacture of machinery and equipment n.e.c.\n\n\n43\nmvh\nMotor vehicles and parts\nManufacture of motor vehicles, trailers and semi-trailers\n\n\n44\notn\nTransport equipment nec\nManufacture of other transport equipment\n\n\n45\nomf\nManufactures nec\nOther Manufacturing: includes furniture\n\n\n46\nely\nElectricity\nElectricity; steam and air conditioning supply\n\n\n47\ngdt\nGas manufacture, distribution\nGas manufacture, distribution\n\n\n48\nwtr\nWater\nWater supply; sewerage, waste management and remediation activities\n\n\n49\ncns\nConstruction\nConstruction: building houses factories offices and roads\n\n\n50\ntrd\nTrade\nWholesale and retail trade; repair of motor vehicles and motorcycles\n\n\n51\nafs\nAccommodation, Food and service activities\nAccommodation, Food and service activities\n\n\n52\notp\nTransport nec\nLand transport and transport via pipelines\n\n\n53\nwtp\nWater transport\nWater transport\n\n\n54\natp\nAir transport\nAir transport\n\n\n55\nwhs\nWarehousing and support activities\nWarehousing and support activities\n\n\n56\ncmn\nCommunication\nInformation and communication\n\n\n57\nofi\nFinancial services nec\nOther Financial Intermediation: includes auxiliary activities but not insurance and pension funding\n\n\n58\nins\nInsurance (formerly isr)\nInsurance (formerly isr): includes pension funding, except compulsory social security\n\n\n59\nrsa\nReal estate activities\nReal estate activities\n\n\n60\nobs\nBusiness services nec\nOther Business Services nec\n\n\n61\nros\nRecreational and other services\nRecreation & Other Services: recreational, cultural and sporting activities, other service activities; private households with employed persons (servants)\n\n\n62\nosg\nPublic Administration and defense\nOther Services (Government): public administration and defense; compulsory social security, activities of membership organizations n.e.c., extra-territorial organizations and bodies\n\n\n63\nedu\nEducation\nEducation\n\n\n64\nhht\nHuman health and social work activities\nHuman health and social work\n\n\n65\ndwe\nDwellings\nDwellings: ownership of dwellings (imputed rents of houses occupied by owners)",
    "crumbs": [
      "Sectors in v11"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/gtap_details.html",
    "href": "gtap_invest/user_guide/gtap_details.html",
    "title": "4. GTAP model details",
    "section": "",
    "text": "4. GTAP model details\nThe GTAP (Global Trade Analysis Project) model is a multi-commodity, multiregional computable general equilibrium (CGE) model that tracks bilateral trade flows between all countries in the world and explicitly models the consumption and production for all commodities of each national economy (Corong et al., 2017; Hertel, 1997). The standard version of the model is a comparative static CGE model that shows differences between possible states of the global economy for a given year – with and without policy or with respect to base year and future year. At the core of the GTAP model is an input-output accounting framework that accounts for all sources and uses of each market good and economic endowment. Figure S.4.1 is a stylized framework of the GTAP model and summarizes the key flows across economic agents in the model.\n\nFigure S.4.1: A stylized framework of the GTAP model. Source: the figure is adapted from Corong et al. 2017\nConsumption framework. The model has a single representative household for each region (the Regional household). The red line in Figure S.4.1 represents income flows to the household. The household receives all gross factor payments net of the capital depreciation allowance (VOA - Payments of Factors of Production), plus the receipts from all indirect taxes (including Export and Import Taxes – XTAX, MTAX). Regional income is distributed across three broad categories—Private Household, Government Expenditures, and Savings— by maximizing a top-level Cobb-Douglas utility function. Saving is a unitary good, while Private Household and Government Expenditures utilize sub-level utility functions to determine consumption of each domestic product (Value of Domestic purchases of Private Household and Government - VDPA, VDGA - black lines in Figure S.4.1) and imported commodities (Value of Imported purchases of Private Household and Government - VIPA, VIGA - blue lines in Figure B.1). The sub-level utility function for the Private Household is based on a constant differences of elasticities (CDE) function (Hanoch, 1975). This function is less demanding to solve than the flexible functional forms and permits calibration of income elasticities and own-price elasticities independently, and importantly, it is non-homothetic. The sub-utility function for public expenditure is based on a constant elasticity (CES) utility function (Arrow et al., 1961).\nProduction framework. GTAP uses nested CES functions to model producer behavior for each region. At the top level of the production framework, producers combine aggregate value-added and intermediate inputs, according to a single CES function. Sub-level CES functions produce aggregated value-added from each primary factor commodities and aggregated intermediate input from each purchased inputs. Factors of production, or endowments, are of three types: perfectly mobile (e.g., labor and capital), partially mobile or sluggish (e.g., land) and sector-specific factors (natural resources). Each purchased input can be sourced either domestically or internationally, and this is modelled using another sub-level CES function (Value of Domestic and Imports of Firms – VDFA and VIFA in Figure S.4.1.).\nInternational trade. The most notable restriction on trade in the GTAP model is that commodity sourcing is at the border: for each product, all domestic agents (i.e., Private Household, Government, Producers) in an economy use the same mix of imports from different countries, though each agent chooses their own combination of import and domestic product. There is also a two-level system of substitution between products from different sources - an import-domestic top-level CES function above an import-import sub-level CES function. Trade flows generate supply and demand for international transport services, and this is accounted for in the model. There is also no international trade in primary factors in the standard version of GTAP.\nWe use the GTAP Database Version 10 for year 2014 (Aguiar et al., 2019). It represents globally consistent data on consumption, production, and international trade (including transportation and protection data), energy data and CO2 emissions for 140 regions and 57 commodities. These regions and commodities are aggregated into 37 regions and 17 commodity groups. The GTAP Data Base is composed of Input Output Tables statistics, which are mainly contributed by members of the GTAP Network. The GTAP 10 Database includes separate IO tables for 121 individual countries representing 98 percent of global gross domestic product and 92 percent of the world’s population. Key value flows in the database include both input-output flows within each region, bilateral international trade flows, capital stock and savings information, international transports costs, domestic input and output subsidies, export subsidies and import tariffs, as well as revenue flows from taxes and tariffs. Most flows are measured at both tax-free and tax-paid prices (i.e., taxes are implicitly accounted for). Key behavioral parameters provided with the GTAP Data Base include the source-substitution or Armington elasticities (used to differentiate goods by country or origin), the factor substitution elasticities, the factor transformation elasticities affecting the sluggish factors, the investment parameters, and the parameters governing the consumer demand elasticities. The first three sets of parameters are taken from external sources, while the rest are calibrated from the database.\nThe standard GTAP model is implemented using the GEMPACK (General Equilibrium Modelling PACKage) suite of economic modeling software (Harrison & Pearson, 1996). GEMPACK is distributed by The Centre of Policy Studies Knowledgebase at Victoria University, Melbourne, Australia (https://www.copsmodels.com/gempack.htm). Following the standard for the GEMPACK program, all equations of the GTAP model are recorded not in levels (e.g., million USD), but in percentage change form. Due to non-linearities in formulae and update equations– which result in changes in the underlying shares and price elasticities– the solution requires non-linear methods. The GTAP model can be run via command line as well as the Windows-based RunGTAP tool. RunGTAP is a visual interface to various GEMPACK programs and allows the user to run simulations interactively in a Windows environment using the GTAP general equilibrium model. No previous knowledge of the GEMPACK language or programming skills is necessary to use the program. Results and complementary information for further analysis are also provided in a Windows environment and can be accessed interactively. RunGTAP also has several add-on tools that can be helpful to users. The welfare decomposition tool permits the user to break down the regional equivalent variation metric into its component parts, including changes due to allocative efficiency, terms of trade, improved technology, and endowments. The systematic sensitivity analysis tool allows uncertainty analysis in the model shocks and parameters, thereby generating both mean and standard deviations of model output. Finally, the subtotals tool utilizes numerical integration techniques in order to exactly decompose changes in the model outputs as sums of the contributions made by the change in each exogenous variable (Harrison, Horridge & Pearson, 2000). The subtotals are particularly useful in understanding the key drivers of model outcomes. All the input files are binary header array (HAR) files, to keep the size of the files small. The HAR files are designed to work with the GEMPACK program. There is also a GAMS version of the standard GTAP model and software exist for readily converting these HAR files to the General Algebraic Modeling System (GAMS) data exchange file (GDX) format, as well as to CSV files.\nCapital, skilled labor, and unskilled labor are perfectly mobile in the GTAP model. This report assumes perfect labor mobility, which means that labor can move across sectors but not across skill types. In general, perfect mobility implies that returns to each factor will be equated across all sectors. Therefore, there is a single economy-wide price for each mobile factor (capital, skilled and unskilled labor), with market equilibrium determined by setting aggregate demand equal to (exogenous) supply. Land is partially mobile (or sluggish in GTAP terminology). The supply of aggregate land to individual activities is less than perfectly elastic, as there is a transformation frontier (Constant Elasticity of Transformation) that moderates the movement of the land across activities. This results in sector-differentiated land prices for each land using sector. The economy-wide price of each land type is then calculated as the CET aggregate price of each land factor.\n\n4.1. Productivity growth from agricultural R&D\nIncreasing public agricultural R&D investments is one of the key levers policy makers can use to alter the trajectory of agricultural productivity. Following Baldos et al. (2020), GTAP models implements R&D policy by modelling the linkages between the flow of R&D spending, the stock of accumulated knowledge capital, and subsequent total factor (TFP) productivity growth (Alston et al., 2011; Griliches, 1979; P. Heisey et al., 2011; Huffman, 2009). TFP captures the rise in total output given all inputs used in agricultural production, unlike crop yields, which ignores the role of other farm inputs (i.e., total output per area of land input used). The historical national R&D spending data are based on Fuglie (2018), who complied data on public agricultural R&D expenditures from the literature[2] starting from the 1960s worldwide and from the 1930s for some developed countries, measured in 2005 Purchasing Power Parity (PPP) dollars. Equations 1 to 3 summarize the key linkages under this framework.\n\nStarting with Eq.1, R&D expenditure at time t (XD_t ) contributes to R&D stock in years t+1 (RD_t+1 ) through t + L, where \\beta_RD,i is the R&D lag weight at period i and total lag length L is the number of years R&D contributes to productivity until it fully depreciates (Appendix Figure S.4.1). Following the structure of the R&D lag weights, initially R&D spending at time  contributes little to knowledge capital stock, but its effect builds over time as technology arising from that research is developed and is disseminated to farmers. Eventually, the effects peak when technology is fully disseminated, but then wanes due to technology obsolescence. Following Alston et al. (2011), this process is modeled by imposing a gamma distribution for the R&D lag weights (Equation 2). We utilize separate R&D lag distributions for developing and developed regions, calibrated according to lag structures suggested in the literature (Alston et al. 2010). Specifically, we impose a lag length that spans 50 years for developed countries, which are assumed to be on the productivity frontier.  Peak impacts of R&D spending on knowledge stocks (and productivity) occur after 26 years (\\sigma, \\lambda = (0.90, 0.70)). For developing countries, we impose a total lag length of 35 years, with peak effects at year 10 (\\sigma, \\lambda = (0.80, 0.75)).\n\nFigure S.4.1: R&D Lag Weights used to convert R&D spending to R&D knowledge stocks across different years Note: figure shows the R&D lag weights used to convert R&D spending to R&D knowledge stocks across different years. The value of the weights is zero at Year 0 (investment year), increases as time progress, and eventually goes to zero. The shorter lag length for developing countries reflects their ability to focus more on adaptive R&D, borrowing from global knowledge capital to close existing yield gaps.  The longer lag structure for developed countries reflects a greater focus on discovery R&D to push out the global science and technology frontier. We define the total lag length as 50 years following Alston et al (2010).\nThe growth in knowledge capital stocks is linked to growth in agricultural total factor productivity (TFP) via elasticities that describe the percent rise in TFP given a 1 percent rise in knowledge capital stock (see Equation 3). Using the empirical estimates in the literature as a guide (Fuglie 2018), we assign the R&D stock–to–TFP elasticities for each world region (Appendix Table 1). The values of these parameters reflect generally lower capacity of R&D systems in developing regions (where the value of the R&D elasticity ranges from 0.18 to 0.23) compared to developed countries (where ranges from 0.23 to 0.30). Lower elasticity values imply larger increases in R&D stocks – achieved via greater R&D spending growth – are required in order to raise TFP sufficiently to offset the adverse effects of climate change. It is consistent with the lower research intensities (less R&D spending relative to the value of agricultural output) in developing countries (Pardey et al. 2016). It is important to note that we only consider climate adaptation driven by public R&D investments. We exclude private and international R&D spending for which the contribution to global spending pool has grown steadily in recent years (Beintema et al., 2012). We also abstract from the potential for additional technological spillovers across regions in the context of climate adaptation.\nTable S.4.1: Annual growth rates of key economic variables over 2014-2030. Note: Compounded annual growth rates are calculated from total changes over 2014-30 in Capital Stock, Real GDP, Population, Unskilled Labor and Skilled Labor from Econmap (v2.4) (Fouré et al 2013) based on SSP2 scenario. Regional productivity growth is used to target changes in regional Real GDP. Sector specific productivity growth for Crops, Ruminants and Non-Ruminants are based on per annum growth rates from Ludena et al (2007) over 2001-40. Due to lack of estimates, global agricultural productivity growth from Ludena et al (2007) is imposed on the managed forestry sector. A 2% productivity growth is imposed on Manufactures sector to reflect the productivity gap between Manufactures and Service sectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegions\nAnnual growth rates of key economic drivers over 2014-2030 (in %)\n\n\n\n\n\n\n\n\n\nCapital Stock\nReal GDP\nPopulation\nUnskilled Labor\nSkilled Labor\nTotal Factor Productivity\n\n\n\n\n\n\n\n\n\n\n\nCrops\nRuminant\nNon-ruminants\n\n\nArgentina\n3.38\n2.79\n0.61\n0.94\n1.41\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nBangladesh\n3.66\n3.91\n0.80\n1.27\n0.94\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrazil\n3.26\n2.62\n0.57\n0.81\n1.94\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Amer\n3.94\n3.53\n0.86\n1.47\n1.21\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanada\n2.62\n2.82\n0.96\n0.72\n0.92\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\n6.82\n6.07\n-0.07\n-0.30\n1.62\n0.81\n1.68\n3.66\n\n\n\n\n\n\n\n\n\n\n\n\n\nColombia\n2.61\n1.96\n0.97\n1.39\n2.16\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nRest of E Asia\n6.69\n5.70\n1.04\n1.02\n0.29\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nEgypt\n5.58\n5.21\n1.42\n2.13\n1.76\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthiopia\n4.34\n3.73\n1.60\n2.40\n-0.04\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nEU\n1.96\n1.81\n0.25\n-0.12\n1.17\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndonesia\n3.45\n3.05\n0.57\n1.51\n4.53\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndia\n5.08\n5.43\n1.08\n1.69\n1.64\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nJapan\n1.54\n1.41\n-0.32\n-0.28\n0.84\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nKorea\n4.24\n3.58\n0.02\n0.17\n0.85\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nMorroco\n3.13\n3.31\n0.41\n0.89\n2.35\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nMadagascar\n0.59\n2.50\n2.18\n3.03\n-2.39\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nM East N Africa\n3.47\n2.30\n1.35\n1.47\n1.54\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nMexico\n3.52\n2.82\n0.64\n0.98\n0.60\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nMalaysia\n4.68\n3.73\n1.28\n1.49\n1.23\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nNigeria\n7.00\n6.47\n2.20\n3.02\n4.68\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nOceania\n3.01\n3.14\n1.36\n1.47\n-0.46\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nC Asia\n4.14\n4.63\n0.07\n0.02\n0.64\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Europe\n2.62\n1.90\n0.62\n0.22\n1.22\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nPakistan\n4.70\n3.59\n1.56\n2.01\n1.24\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhilippines\n5.11\n4.94\n1.42\n2.22\n0.86\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoland\n4.04\n3.89\n-0.06\n-0.44\n1.29\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nRest of S Asia\n4.11\n3.80\n2.10\n2.59\n0.28\n0.54\n0.83\n1.94\n\n\n\n\n\n\n\n\n\n\n\n\n\nRest of SE Asia\n4.56\n3.45\n0.61\n0.78\n2.38\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nRussia\n3.40\n3.42\n-0.12\n-0.67\n-1.21\n0.78\n0.30\n1.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nS America\n3.54\n2.65\n0.78\n1.34\n1.13\n0.35\n0.84\n2.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nSS Africa\n4.38\n4.77\n2.08\n2.83\n1.98\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurkey\n4.76\n4.41\n0.88\n1.26\n1.31\n0.25\n-0.17\n-0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nUSA\n2.12\n1.77\n0.71\n0.40\n-0.53\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\nVietnam\n5.37\n4.59\n0.61\n0.54\n2.77\n-0.37\n-0.70\n2.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nAngola+DRC\n6.89\n5.93\n2.60\n3.46\n3.05\n0.51\n0.32\n-0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth Africa\n3.67\n3.37\n0.62\n1.26\n1.57\n0.64\n0.15\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable S.4.2: Historical and Projected Average Annual Growth Rate in R&D Spending. Note: Subsidy Repurposing includes R&D spending under Baseline 2021-2030 in addition to savings from subsidy removal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nAverage annual growth rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Data\nFuture Projections\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline\nSubsidy Repurposing 100 percent\nSubsidy Repurposing 50 percent\nSubsidy Repurposing 20 percent\nSubsidy Repurposing 10 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1991-2000\n2001-2010\n2011-2020\n2021-2030\n2021-2030\n2021-2030\n2021-2030\n2021-2030\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral America\n-3.7 percent\n4.0 percent\n3.2 percent\n3.6 percent\n62.1 percent\n22.1 percent\n15.3 percent\n12.4 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth America\n-0.3 percent\n2.7 percent\n3.5 percent\n3.6 percent\n7.8 percent\n5.5 percent\n4.4 percent\n4.1 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth Asia\n6.9 percent\n5.4 percent\n4.0 percent\n3.6 percent\n20.3 percent\n9.8 percent\n5.9 percent\n4.3 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth East Asia\n3.4 percent\n1.6 percent\n3.4 percent\n3.6 percent\n35.6 percent\n17.3 percent\n13.3 percent\n11.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorth East Asia\n6.9 percent\n11.5 percent\n4.7 percent\n3.6 percent\n27.2 percent\n13.0 percent\n8.5 percent\n6.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nWest Asia\n3.9 percent\n2.6 percent\n3.7 percent\n3.6 percent\n17.0 percent\n11.3 percent\n9.4 percent\n8.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorth Africa\n2.5 percent\n3.0 percent\n4.7 percent\n3.6 percent\n17.0 percent\n11.3 percent\n9.4 percent\n8.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSub-Saharan Africa\n0.7 percent\n2.8 percent\n3.4 percent\n3.6 percent\n23.5 percent\n14.9 percent\n12.6 percent\n11.8 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth Africa\n1.4 percent\n-1.4 percent\n1.1 percent\n1.5 percent\n4.4 percent\n2.8 percent\n2.0 percent\n1.7 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nOceania\n1.2 percent\n-1.6 percent\n1.2 percent\n1.5 percent\n4.6 percent\n3.1 percent\n2.3 percent\n2.0 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanada/USA\n1.4 percent\n-0.2 percent\n0.6 percent\n1.5 percent\n10.0 percent\n5.2 percent\n2.9 percent\n2.0 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nJapan/Korea\n2.2 percent\n1.3 percent\n1.3 percent\n1.5 percent\n10.5 percent\n5.7 percent\n3.4 percent\n2.5 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nWestern Europe\n0.6 percent\n1.3 percent\n1.4 percent\n1.5 percent\n16.4 percent\n7.8 percent\n4.2 percent\n2.8 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransition Regions\n10.9 percent\n5.8 percent\n3.6 percent\n3.6 percent\n381.6 percent\n12.6 percent\n7.9 percent\n5.9 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\nWORLD\n1.9 percent\n2.8 percent\n2.7 percent\n2.8 percent\n32.3 percent\n10.0 percent\n6.7 percent\n5.4 percent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. Extending GTAP to include a physical representation of land via GTAP-AEZ\nThe base GTAP CGE excludes several components necessary to link to environmental models. As discussed above, we chose to augment the base model with specific agro-ecological zones (AEZs) in order to specify biophysical production parameters, make explicit the use of environmental resources (e.g., water), and endogenize use-differentiated land resources (for forestry, agriculture and other land-intensive sectors). Inclusion of these extra factors exposes the key variables in the economy to which our fully spatialized GTAP-InVEST model can connect. The underlying GTAP-AEZ model is based on the formulation in Lee (2005), updated by Tahierpour et al. (2013) and Baldos (2016). We further updated the model for this paper to the newest GTAP database (Version 10) with greater country-level and AEZ-level disaggregation. The underlying equations in GTAP and GTAP-AEZ are well documented throughout the literature, so we do not present them in full.\n\n\n4.3. Example GTAP configuration file\nThe configuration below is for the BAU Ecosystem Service adjustment run. See the code repository at github.com/jandrewjohnson/gtap_investfor all configuration files. These configuration files are called iteratively by the python code using a batch file that replaces the &lt;cmf&gt; tag with the name of the scenario being run.\nstart with mmnz = 200000000;  ! Assign largest starting memory allocation (max is 2100000000)\nMA48 increase_MMNZ = veryfast; ! If largest memory is not enough, allow\nCPU = yes;\nNDS = yes;\nExtrapolation accuracy file = NO;\naux files = \"GTAPAEZ\";\n! Input files\nfile GTAPSETS = ..\\gtp1414\\sets.har;\nfile GTAPDATA = ..\\work\\2021_30_BAU_noES.upd;\nfile GTAPPARM = ..\\gtp1414\\default.prm;\nfile GTAPSUPP = ..\\gtp1414\\&lt;cmf&gt;_SUPP.har;\n! Output files\nfile GTAPSUM          = ..\\work\\&lt;cmf&gt;_sum.har;\nUpdated file GTAPDATA = ..\\work\\&lt;cmf&gt;.upd;\nSolution file         = ..\\work\\&lt;cmf&gt;.sl4;\nVerbal Description =&lt;cmf&gt; ;\nlog file  =  ..\\work\\&lt;cmf&gt;.log;\nMethod = Euler;\nSteps = 2 4 6;\nautomatic accuracy = yes;\naccuracy figures = 4;\naccuracy percent = 90;\nminimum subinterval length =  0.0001;\nminimum subinterval fails = stop;\naccuracy criterion = Both;\nsubintervals =5;\nexogenous\npop\npsaveslack pfactwld\nprofitslack incomeslack endwslack\ncgdslack tradslack\nams atm atf ats atd\naosec aoreg\navasec avareg\nafcom afsec afreg afecom afesec afereg\naoall afall afeall aoall2 aoall3 aoall4\nau dppriv dpgov dpsave\nto_1 to_2 !to\n!EC change for revenue neutral scenario\ntfijr\ntfreg\n!End: EC change for revenue neutral scenario\ntp tm tms tx txs\nqo(\"UnSkLab\",REG)\nqo(\"SkLab\",REG)\nqo(\"Capital\",REG)\nqo(\"NatRes\",REG)\ntfm tfd;\nExogenous p_slacklandr;\nExogenous p_ECONLAND  = zero value on    file ..\\gtp1414\\basedata.har header \"MAXL\" ;\nExogenous p_slackland = nonzero value on file ..\\gtp1414\\basedata.har header \"MAXL\" ;\nExogenous p_LANDCOVER_L(AEZ_COMM,\"UNMNGLAND\",REG);\nExogenous c_MAX_LAND;\nRest Endogenous ;\n!===========\n! Shocks\n!===========\n! Ecosystem services shocks\n! (1) Fishery and Forestry shocks\nShock aoall2(\"fishery\",REG)    = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"fishery\";\nShock aoall2(\"forestsec\",REG)  = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"forestsec\";\n! (2) Pollination shocks from InVEST : Pollination Collapse\nShock aoall3(\"cotton\",REG)     = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"cotton\";\nShock aoall3(\"crsgrns\",REG)    = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"crsgrns\";\nShock aoall3(\"fruitveg\",REG)   = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"fruitveg\";\nShock aoall3(\"oilsds\",REG)     = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"oilsds\";\nShock aoall3(\"othercrps\",REG)  = file ..\\shocks\\ALLSHOCKS.har header \"BAES\" slice \"othercrps\";\n!===========\n! Subtotal\n!===========\nSubtotal aoall2(\"fishery\",REG)  = fish;\nSubtotal aoall2(\"forestsec\",REG)= forest;\nSubtotal aoall3(\"crsgrns\",REG) aoall3(\"fruitveg\",REG) aoall3(\"oilsds\",REG) aoall3(\"cotton\",REG) aoall3(\"othercrps\",REG)  = polli;",
    "crumbs": [
      "4. GTAP model details"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/invest_details.html",
    "href": "gtap_invest/user_guide/invest_details.html",
    "title": "5. InVEST model details",
    "section": "",
    "text": "5. InVEST model details\nThe InVEST model suite is thoroughly documented in its user guide (Sharp et al. 2020). This source identifies how each of the 24 ecosystem service models are calculated, including how to obtain input data. However, the models officially included in InVEST do not always work at the global scale. Thus, we use modified versions of them that have been rewritten to run in parallel at the global scale. These modified versions of the models were first documented in Kim et al. (2018) and more fully in Chaplin-Kramer et al. (2019). In this section, we summarize the modifications made to enable global coverage.\nFor the pollination model, the global version assumed pollinators are fully abundant on any natural land covers (specifically classes 50 to 180 in our underlying ESACCI LULC database) and that all pollinators had the same relative efficacy and distance decay parameters for their pollination activity. Region-specific applications of InVEST often calibrate the model based on which species of pollinators are present and what their pollination efficiency is (which we did not do). We then calculated the proportional area of natural land covers around every plot of agricultural land (classes 10-20). In brief, the model estimates pollinator sufficiency based on the abundance of pollinator species given the two limiting resources on the landscape: pollinator nesting habitat and pollinator nectar sources (provided here by pollination-dependent crops). We calculated this globally following Chaplin-Kramer et al. (2019), which provided a global map of pollination sufficiency. This was an input to the algorithm described in SI Section 6 that connects InVEST biophysical outputs to GTAP inputs.\nWe perform the carbon storage calculations by applying a custom version of the InVEST carbon storage model. This version uses a globally-extended lookup table for IPCC Tier-1 (Ruesch and Gibbs 2008) values of carbon content defined for regions breaking out each continent, ecofloristic region and frontier forest status. The default InVEST model takes as an input a biophysical table with one of these regions.",
    "crumbs": [
      "5. InVEST model details"
    ]
  },
  {
    "objectID": "gtap_invest/user_guide/release_notes.html",
    "href": "gtap_invest/user_guide/release_notes.html",
    "title": "Release Notes",
    "section": "",
    "text": "Unzipped gtapv7-aez-rd_26Mar24.zip to C:_data_releases_aez_rd\nConsidering changing hyphens to underscores in the file names\n\nREJECTED to follow the gtap model naming convention, so we are staying with gtapv7-aez-rd but everything else is underscore\n\nREQUIRES manually adding a new dir “out” in the “gtapv7_aez_rd” dir\nThe first run did not find results.map, so I copied this manually from the last release into /mod dir\nAdded a _just_mapping.bat file to run the mapping only but this required configuring the directory to not include cd ..\nThere are now two time-modes for running:\n\nRD and Singleshot\nOne can’t simply run the singleshot because then it will get the 2049-2050 percentage change for GDP POP etc.\nSo, we prepared an additional set of projections that are not recursive. These are prepended with D.\nset DGDPPC=DGP2\nset DPOP=DPO2\nset DLAB=DLB2\n\nset GDPPC=OGP2\nset POP=POP2\nset LAB=LAB2\nTo switch from the default RD mode to SS, just replace GDPPC with DGDPPC, POP with DPOP, and LAB with DLAB in the .bat file\nNOTE: GDP is exogenous in BAU to match projections, but then we swap it in any policy shock to endogenously determine GDP.\n\n\n\n\n\nCMF files cannot change any values at run time, so instead we are going to create a new harfile with the new values\n\nTo do this in ViewHAR with the example of MAXL, right click on MAXL and select “Clone”.\n\nIt’s hard to edit in viewhar, so copy to clipboard, then excel, then edit the values, then paste back in for the modified har.\nIn the CMF file, then call"
  },
  {
    "objectID": "gtap_invest/user_guide/release_notes.html#section",
    "href": "gtap_invest/user_guide/release_notes.html#section",
    "title": "Release Notes",
    "section": "",
    "text": "Unzipped gtapv7-aez-rd_26Mar24.zip to C:_data_releases_aez_rd\nConsidering changing hyphens to underscores in the file names\n\nREJECTED to follow the gtap model naming convention, so we are staying with gtapv7-aez-rd but everything else is underscore\n\nREQUIRES manually adding a new dir “out” in the “gtapv7_aez_rd” dir\nThe first run did not find results.map, so I copied this manually from the last release into /mod dir\nAdded a _just_mapping.bat file to run the mapping only but this required configuring the directory to not include cd ..\nThere are now two time-modes for running:\n\nRD and Singleshot\nOne can’t simply run the singleshot because then it will get the 2049-2050 percentage change for GDP POP etc.\nSo, we prepared an additional set of projections that are not recursive. These are prepended with D.\nset DGDPPC=DGP2\nset DPOP=DPO2\nset DLAB=DLB2\n\nset GDPPC=OGP2\nset POP=POP2\nset LAB=LAB2\nTo switch from the default RD mode to SS, just replace GDPPC with DGDPPC, POP with DPOP, and LAB with DLAB in the .bat file\nNOTE: GDP is exogenous in BAU to match projections, but then we swap it in any policy shock to endogenously determine GDP.\n\n\n\n\n\nCMF files cannot change any values at run time, so instead we are going to create a new harfile with the new values\n\nTo do this in ViewHAR with the example of MAXL, right click on MAXL and select “Clone”.\n\nIt’s hard to edit in viewhar, so copy to clipboard, then excel, then edit the values, then paste back in for the modified har.\nIn the CMF file, then call"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am an Associate Professor in Applied Economics at the University of Minnesota. I also work closely with the Natural Capital Project, at the University of Minnesota and Stanford University. My research focuses on how the economy affects the environment, and vice versa, on global to local scales. Specifically, I connect models of ecosystem services (how natural capital provides valuable services to humans) with general equilibrium economic models. To do this, I write open-source software (Python, R and C/C++) that uses big data (mostly from satellites) with economic modeling and machine-learning/AI techniques. I recently co-founded NatCap TEEMs, a research center in Applied Economics and a partner of Natural Capital Project.\nI now focus mostly on the GTAP-InVEST model, which links the Global Trade Analysis Project (GTAP) out of Purdue University with the Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) model from the Natural Capital Project. The project aims to build strong quantitative evidence on how changes in ecosystem services affect economic performance at the macroeconomic level. I also continue development on the SEALS model, a land-use change prediction model, incorporating AI/ML techniques able to predict change globally at a fine resolution (10 to 300 meter). SEALS has been connected to multiple models, including GTAP, MAgPIE, GLOBIOM, and other Integrated Assessment models. See my research page for more information and additional projects.\nIn addition to ecosystem services, I also research food security, climate change, agent-based modeling, AI/machine-learning and agricultural management in developing countries. In my spare time, I am a mountain biker, rock climber and board game designer."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "For an up-to-date listing, see my Google Scholar profile.\n*Authorship notes: Student/advisee names in underline. Shared lead-author is noted with an asterisk.\nJohnson, J. A., Baldos, U. L., Corong, E., Hertel, T., Polasky, P., Cervigni, R., Roxburgh, T., Ruta, G., Salemi, C., and Thakrar, S. “Investing in Nature Can Improve Equity and Economic Returns.” Proceedings of the National Academy of Sciences 120, no. 27 (July 4, 2023): e2220401120. https://doi.org/10.1073/pnas.2220401120.\nKim, H., Peterson, G. D., Cheung, W. W., Ferrier, S., Alkemade, R., Arneth, A., … Johnson, J. A., …  & Pereira, H. M. (2023). Towards a better future for biodiversity and people: Modelling Nature Futures. Global Environmental Change, 82, 102681.\nvon Jeetze, P. J., Weindl, I., Johnson, J. A., Borrelli, P., Panagos, P., Molina Bacca, E. J., … & Popp, A. (2023). Projected landscape-scale repercussions of global action for climate and biodiversity protection. Nature Communications, 14(1), 2515.\nCisneros-Pineda, A., Dukes, J. S., Johnson, J. A., Brouder, S., Ramankutty, N., Corong, E., & Chaudhary, A. (2023). The missing markets link in global-to-local-to-global analyses of biodiversity and ecosystem services. Environmental Research Letters, 18(4), 041003.\nKamarajugedda, S. A., Johnson, J. A., McDonald, R., & Hamel, P. (2023). Carbon storage and sequestration in Southeast Asian urban clusters under future land cover change scenarios (2015–2050). Frontiers in Environmental Science, 11, 1105759.\nIsbell, F., Balvanera, P., Mori, A. S., He, J. S., Bullock, J. M., Regmi, G. R., … Johnson, J. A., … & Palmer, M. S. (2023). Expert perspectives on global biodiversity loss and its drivers and impacts on people. Frontiers in Ecology and the Environment, 21(2), 94-103.\nJohnson, J. A., Brown, M. E., Corong, E., Dietrich, J. P., Henry, R., Jeetze, P. J. V., … & Williams, D. R. (2023). The meso scale as a frontier in interdisciplinary modeling of sustainability from local to global scales. Environmental Research Letters.\nChaplin-Kramer, R., Neugarten, R. A., Sharp, R. P., Collins, P. M., Polasky, S., Hole, D., … Johnson, J. A., …  & Watson, R. A. (2023). Mapping the planet’s critical natural assets. Nature Ecology & Evolution, 7(1), 51-61.\nJohnson, J. A., & Salemi, C. (2022). Agents on a Landscape: Simulating Spatial and Temporal Interactions in Economic and Ecological Systems. Frontiers in Ecology and Evolution, 524.\nRunge, C. F., Johnson, J. A., Nelson, E., & Redish, A. D. (2022). A neuroscience-based analysis of impacts of disaster memory on economic valuation. Journal of Neuroscience, Psychology, and Economics.\nWeiskopf, Sarah R., Bonnie JE Myers, Maria Isabel Arce-Plata, Julia L. Blanchard, Simon Ferrier, Elizabeth A. Fulton, Mike Harfoot et al. “A Conceptual Framework to Integrate Biodiversity, Ecosystem Function, and Ecosystem Service Models.” BioScience (2022).\nWeiskopf, S. R., Harmáčková, Z., Johnson, C. G., Londoño-Murcia, M. C., Miller, B. W., Myers, B. J., … Johnson, J. A., … & Rosa, I. M. (2022). Increasing the uptake of ecological model results in policy decisions to improve biodiversity outcomes. Environmental Modelling & Software, 105318.\nJohnson, J. A., Kennedy, C. M., Oakleaf, J. R., Baruch-Mordo, S., Polasky, S., & Kiesecker, J. (2021). Energy matters: Mitigating the impacts of future land expansion will require managing energy and extractive footprints. Ecological Economics, 187, 107106.\nBagstad, K. J., Ingram, J. C., Shapiro, C. D., La Notte, A., Maes, J., Vallecillo, S., … Johnson, J.A., … & Voigt, B. (2021). Lessons learned from development of natural capital accounts in the United States and European Union. Ecosystem Services, 52, 101359.\nMandle, L., Shields-Estrada, A., Chaplin-Kramer, R., Mitchell, M. G., Bremer, L. L., Gourevitch, J. D., … Johnson, J. A., … & Ricketts, T. H. (2021). Increasing decision relevance of ecosystem service science. Nature Sustainability, 4(2), 161-169.\nSuh, S., Johnson, J. A., Tambjerg, L., Sim, S., Broeckx-Smith, S., Reyes, W., & Chaplin-Kramer, R. (2020). Closing yield gap is crucial to avoid potential surge in global carbon emissions. Global Environmental Change, 63, 102100.\nJohnson, J. A., Jones, S., Wood, S., Chaplin-Kramer, R., Hawthorne, P., Mulligan, M., Pennington, D., DeClerck, F. (2019). Mapping Ecosystem Services to Human Well-being: a toolkit to support integrated landscape management for the SDGs. Ecological Applications.\nChaplin-Kramer, R., Sharp, R. P., Weil, C., Bennett, E. M., Pascual, U., Arkema, K. Johnson, J.A., … & Hamann, M. (2019). Global modeling of nature’s contributions to people. Science, 366(6462), 255-258.\nOakleaf, J. R., Kennedy, C. M., Baruch-Mordo, S., Gerber, J. S., West, P. C., Johnson, J. A., & Kiesecker, J. (2019). Mapping global development potential for renewable energy, fossil fuels, mining and agriculture sectors. Scientific Data, 6(1), 101.\nSmith, W. K., Nelson, E., Johnson, J. A., Polasky, S., Milder, J. C., Gerber, J. S., … & Arbuthnot, M. (2019). Voluntary sustainability standards could significantly reduce detrimental impacts of global agriculture. Proceedings of the National Academy of Sciences, 116(6), 2130-2137.\nKeeler, B. L., Hamel, P., McPhearson, T., Hamann, M. H., Donahue, M. L., Prado, K. A. M., … Johnson, J. A., … & Guerry, A. D. (2019). Social-ecological and technological factors moderate the value of urban nature. Nature Sustainability, 2(1), 29.\nBoyd, J. W., Bagstad, K. J., Ingram, J. C., Shapiro, C. D., Adkins, J. E., Casey, C. F., … Johnson, J. A., … & Hass, J. L. (2018). The Natural Capital Accounting Opportunity: Let’s Really Do the Numbers. BioScience, 68(12), 940-943.\nKim, H., Rosa, I., Alkemade, R., Leadley, P., Hurtt, G., Popp, A., … Johnson, J. A., … & Caton, E. (2018). A protocol for an intercomparison of biodiversity and ecosystem services models using harmonized land-use and climate scenarios. Geoscientific Model Development, 11(11), 4537-4562.\nWood, S. L., Jones, S. K., Johnson, J. A. (shared lead-author), Brauman, K. A., Chaplin-Kramer, R., Fremier, A., … & Mulligan, M. (2018). Distilling the role of ecosystem services in the Sustainable Development Goals. Ecosystem services, 29, 70-82.\nSonter, L. J., Johnson, J. A., Nicholson, C. C., Richardson, L. L., Watson, K. B., & Ricketts, T. H. (2017). Multi-site interactions: Understanding the offsite impacts of land use change on the use and supply of ecosystem services. Ecosystem Services, 23, 158-164.\nJohnson, J. A., Runge, C. F., Senauer, B., & Polasky, S. (2016). Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification. Land Economics, 92(4), 579-592.\nCarlson, K., Gerber, J., Mueller, N., Herrero, M., MacDonald, G., Brauman, K., Havlik, P., O’Connell, C., Johnson, J.A., Saatchi, S., West, P. (2016). Greenhouse gas emissions intensity of global croplands. Nature Climate Change.\nVogl, A. L., Dennedy-Frank, P. J., Wolny, S., Johnson, J. A., Hamel, P., Narain, U., & Vaidya, A. (2016). Managing forest ecosystem services for hydropower production. Environmental Science & Policy, 61, 221-229.\nPolasky, S., Bryant, B., Hawthorne, P., Johnson, J. A., Keeler, B., & Pennington, D. (2015). Inclusive wealth as a metric of sustainable development. Annual Review of Environment and Resources, 40, 445-466.\nChaplin-Kramer, R., Sharp, R.P., Mandle, L., Sim, S., Johnson, J. A., Butnar, I., i Canals, L.M., Eichelberger, B.A., Ramler, I., Mueller, C. and McLachlan, N. (2015). Spatial patterns of agricultural expansion determine impacts on biodiversity and carbon storage. Proceedings of the National Academy of Sciences, 112(24), 7402-7407.\nJohnson, J. A, Runge, C. F., Senauer, B., Foley, J., & Polasky, S. (2014). Global agriculture and carbon trade-offs. Proceedings of the National Academy of Sciences, 111(34), 12342-12347.\nRunge, C. F., & Johnson, J. A. (2014). Are we in this together? Risk bearing and collective action. Journal of Natural Resources Policy Research, 6(1), 71-76.\nRunge, C. F., Sheehan, J. J., Senauer, B., Foley, J., Gerber, J., Johnson, J. A., … & Runge, C. P. (2012). Assessing the comparative productivity advantage of bioenergy feedstocks at different latitudes. Environmental Research Letters, 7(4), 045906.\nRunge, C. F., Johnson, J. A, & Runge, C. P. (2011). Better milk than cola: soft drink taxes and substitution effects. Choices, 26(3), 3."
  },
  {
    "objectID": "publications/index.html#peer-reviewed-journal-publications",
    "href": "publications/index.html#peer-reviewed-journal-publications",
    "title": "Publications",
    "section": "",
    "text": "For an up-to-date listing, see my Google Scholar profile.\n*Authorship notes: Student/advisee names in underline. Shared lead-author is noted with an asterisk.\nJohnson, J. A., Baldos, U. L., Corong, E., Hertel, T., Polasky, P., Cervigni, R., Roxburgh, T., Ruta, G., Salemi, C., and Thakrar, S. “Investing in Nature Can Improve Equity and Economic Returns.” Proceedings of the National Academy of Sciences 120, no. 27 (July 4, 2023): e2220401120. https://doi.org/10.1073/pnas.2220401120.\nKim, H., Peterson, G. D., Cheung, W. W., Ferrier, S., Alkemade, R., Arneth, A., … Johnson, J. A., …  & Pereira, H. M. (2023). Towards a better future for biodiversity and people: Modelling Nature Futures. Global Environmental Change, 82, 102681.\nvon Jeetze, P. J., Weindl, I., Johnson, J. A., Borrelli, P., Panagos, P., Molina Bacca, E. J., … & Popp, A. (2023). Projected landscape-scale repercussions of global action for climate and biodiversity protection. Nature Communications, 14(1), 2515.\nCisneros-Pineda, A., Dukes, J. S., Johnson, J. A., Brouder, S., Ramankutty, N., Corong, E., & Chaudhary, A. (2023). The missing markets link in global-to-local-to-global analyses of biodiversity and ecosystem services. Environmental Research Letters, 18(4), 041003.\nKamarajugedda, S. A., Johnson, J. A., McDonald, R., & Hamel, P. (2023). Carbon storage and sequestration in Southeast Asian urban clusters under future land cover change scenarios (2015–2050). Frontiers in Environmental Science, 11, 1105759.\nIsbell, F., Balvanera, P., Mori, A. S., He, J. S., Bullock, J. M., Regmi, G. R., … Johnson, J. A., … & Palmer, M. S. (2023). Expert perspectives on global biodiversity loss and its drivers and impacts on people. Frontiers in Ecology and the Environment, 21(2), 94-103.\nJohnson, J. A., Brown, M. E., Corong, E., Dietrich, J. P., Henry, R., Jeetze, P. J. V., … & Williams, D. R. (2023). The meso scale as a frontier in interdisciplinary modeling of sustainability from local to global scales. Environmental Research Letters.\nChaplin-Kramer, R., Neugarten, R. A., Sharp, R. P., Collins, P. M., Polasky, S., Hole, D., … Johnson, J. A., …  & Watson, R. A. (2023). Mapping the planet’s critical natural assets. Nature Ecology & Evolution, 7(1), 51-61.\nJohnson, J. A., & Salemi, C. (2022). Agents on a Landscape: Simulating Spatial and Temporal Interactions in Economic and Ecological Systems. Frontiers in Ecology and Evolution, 524.\nRunge, C. F., Johnson, J. A., Nelson, E., & Redish, A. D. (2022). A neuroscience-based analysis of impacts of disaster memory on economic valuation. Journal of Neuroscience, Psychology, and Economics.\nWeiskopf, Sarah R., Bonnie JE Myers, Maria Isabel Arce-Plata, Julia L. Blanchard, Simon Ferrier, Elizabeth A. Fulton, Mike Harfoot et al. “A Conceptual Framework to Integrate Biodiversity, Ecosystem Function, and Ecosystem Service Models.” BioScience (2022).\nWeiskopf, S. R., Harmáčková, Z., Johnson, C. G., Londoño-Murcia, M. C., Miller, B. W., Myers, B. J., … Johnson, J. A., … & Rosa, I. M. (2022). Increasing the uptake of ecological model results in policy decisions to improve biodiversity outcomes. Environmental Modelling & Software, 105318.\nJohnson, J. A., Kennedy, C. M., Oakleaf, J. R., Baruch-Mordo, S., Polasky, S., & Kiesecker, J. (2021). Energy matters: Mitigating the impacts of future land expansion will require managing energy and extractive footprints. Ecological Economics, 187, 107106.\nBagstad, K. J., Ingram, J. C., Shapiro, C. D., La Notte, A., Maes, J., Vallecillo, S., … Johnson, J.A., … & Voigt, B. (2021). Lessons learned from development of natural capital accounts in the United States and European Union. Ecosystem Services, 52, 101359.\nMandle, L., Shields-Estrada, A., Chaplin-Kramer, R., Mitchell, M. G., Bremer, L. L., Gourevitch, J. D., … Johnson, J. A., … & Ricketts, T. H. (2021). Increasing decision relevance of ecosystem service science. Nature Sustainability, 4(2), 161-169.\nSuh, S., Johnson, J. A., Tambjerg, L., Sim, S., Broeckx-Smith, S., Reyes, W., & Chaplin-Kramer, R. (2020). Closing yield gap is crucial to avoid potential surge in global carbon emissions. Global Environmental Change, 63, 102100.\nJohnson, J. A., Jones, S., Wood, S., Chaplin-Kramer, R., Hawthorne, P., Mulligan, M., Pennington, D., DeClerck, F. (2019). Mapping Ecosystem Services to Human Well-being: a toolkit to support integrated landscape management for the SDGs. Ecological Applications.\nChaplin-Kramer, R., Sharp, R. P., Weil, C., Bennett, E. M., Pascual, U., Arkema, K. Johnson, J.A., … & Hamann, M. (2019). Global modeling of nature’s contributions to people. Science, 366(6462), 255-258.\nOakleaf, J. R., Kennedy, C. M., Baruch-Mordo, S., Gerber, J. S., West, P. C., Johnson, J. A., & Kiesecker, J. (2019). Mapping global development potential for renewable energy, fossil fuels, mining and agriculture sectors. Scientific Data, 6(1), 101.\nSmith, W. K., Nelson, E., Johnson, J. A., Polasky, S., Milder, J. C., Gerber, J. S., … & Arbuthnot, M. (2019). Voluntary sustainability standards could significantly reduce detrimental impacts of global agriculture. Proceedings of the National Academy of Sciences, 116(6), 2130-2137.\nKeeler, B. L., Hamel, P., McPhearson, T., Hamann, M. H., Donahue, M. L., Prado, K. A. M., … Johnson, J. A., … & Guerry, A. D. (2019). Social-ecological and technological factors moderate the value of urban nature. Nature Sustainability, 2(1), 29.\nBoyd, J. W., Bagstad, K. J., Ingram, J. C., Shapiro, C. D., Adkins, J. E., Casey, C. F., … Johnson, J. A., … & Hass, J. L. (2018). The Natural Capital Accounting Opportunity: Let’s Really Do the Numbers. BioScience, 68(12), 940-943.\nKim, H., Rosa, I., Alkemade, R., Leadley, P., Hurtt, G., Popp, A., … Johnson, J. A., … & Caton, E. (2018). A protocol for an intercomparison of biodiversity and ecosystem services models using harmonized land-use and climate scenarios. Geoscientific Model Development, 11(11), 4537-4562.\nWood, S. L., Jones, S. K., Johnson, J. A. (shared lead-author), Brauman, K. A., Chaplin-Kramer, R., Fremier, A., … & Mulligan, M. (2018). Distilling the role of ecosystem services in the Sustainable Development Goals. Ecosystem services, 29, 70-82.\nSonter, L. J., Johnson, J. A., Nicholson, C. C., Richardson, L. L., Watson, K. B., & Ricketts, T. H. (2017). Multi-site interactions: Understanding the offsite impacts of land use change on the use and supply of ecosystem services. Ecosystem Services, 23, 158-164.\nJohnson, J. A., Runge, C. F., Senauer, B., & Polasky, S. (2016). Global Food Demand and Carbon-Preserving Cropland Expansion under Varying Levels of Intensification. Land Economics, 92(4), 579-592.\nCarlson, K., Gerber, J., Mueller, N., Herrero, M., MacDonald, G., Brauman, K., Havlik, P., O’Connell, C., Johnson, J.A., Saatchi, S., West, P. (2016). Greenhouse gas emissions intensity of global croplands. Nature Climate Change.\nVogl, A. L., Dennedy-Frank, P. J., Wolny, S., Johnson, J. A., Hamel, P., Narain, U., & Vaidya, A. (2016). Managing forest ecosystem services for hydropower production. Environmental Science & Policy, 61, 221-229.\nPolasky, S., Bryant, B., Hawthorne, P., Johnson, J. A., Keeler, B., & Pennington, D. (2015). Inclusive wealth as a metric of sustainable development. Annual Review of Environment and Resources, 40, 445-466.\nChaplin-Kramer, R., Sharp, R.P., Mandle, L., Sim, S., Johnson, J. A., Butnar, I., i Canals, L.M., Eichelberger, B.A., Ramler, I., Mueller, C. and McLachlan, N. (2015). Spatial patterns of agricultural expansion determine impacts on biodiversity and carbon storage. Proceedings of the National Academy of Sciences, 112(24), 7402-7407.\nJohnson, J. A, Runge, C. F., Senauer, B., Foley, J., & Polasky, S. (2014). Global agriculture and carbon trade-offs. Proceedings of the National Academy of Sciences, 111(34), 12342-12347.\nRunge, C. F., & Johnson, J. A. (2014). Are we in this together? Risk bearing and collective action. Journal of Natural Resources Policy Research, 6(1), 71-76.\nRunge, C. F., Sheehan, J. J., Senauer, B., Foley, J., Gerber, J., Johnson, J. A., … & Runge, C. P. (2012). Assessing the comparative productivity advantage of bioenergy feedstocks at different latitudes. Environmental Research Letters, 7(4), 045906.\nRunge, C. F., Johnson, J. A, & Runge, C. P. (2011). Better milk than cola: soft drink taxes and substitution effects. Choices, 26(3), 3."
  },
  {
    "objectID": "publications/index.html#peer-reviewed-books-and-reports",
    "href": "publications/index.html#peer-reviewed-books-and-reports",
    "title": "Publications",
    "section": "Peer-Reviewed Books and Reports",
    "text": "Peer-Reviewed Books and Reports\nJohnson, J. A., Giovanni Ruta, Uris Baldos, Raffaello Cervigni, Shun Chonabayashi, Erwin Corong, Olga Gavryliuk et al. “The Economic Case for Nature.” World Bank Flagship Report. (2021). https://openknowledge.worldbank.org/handle/10986/35882\nJohnson, J. A., Baldos, U., Hertel, T., Liu, J., Nootenboom, C., Polasky, S., and Roxburgh, T. (2020). Global Futures: modelling the global economic impacts of environmental change to support policy-making. World Wildlife Fund Technical Paper. https://www.gtap.agecon.purdue.edu/resources/res_display.asp?RecordID=6186\nRoxburgh, T., Ellis, K., Johnson, J. A., Baldos, U. L., Hertel, T., Nootenboom, C., & Polasky, S. (2020). Global Future: Assessing The Global Economic Impacts of Environmental Change to Support Policy-Making. World Wildlife Fund Report. https://www.gtap.agecon.purdue.edu/resources/res_display.asp?RecordID=5987"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software",
    "section": "",
    "text": "Software\nPage for indexing all software."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "I currently teach:\n\nBig Data Methods in Economics (Ph.D., APEC8222)\nIntroduction to Economics (Undergrad, APEC1101)\nEnvironment and Natural Resource Economics (Ph.D., APEC8601)\nEnvironmental Economics (Undergrad, APEC 3611w)"
  },
  {
    "objectID": "teaching/index.html#teaching-overview",
    "href": "teaching/index.html#teaching-overview",
    "title": "Justin Andrew Johnson",
    "section": "",
    "text": "I currently teach:\n\nBig Data Methods in Economics (Ph.D., APEC8222)\nIntroduction to Economics (Undergrad, APEC1101)\nEnvironment and Natural Resource Economics (Ph.D., APEC8601)\nEnvironmental Economics (Undergrad, APEC 3611w)"
  },
  {
    "objectID": "teaching/index.html#teaching-philosophy",
    "href": "teaching/index.html#teaching-philosophy",
    "title": "Justin Andrew Johnson",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nMy philosophy for teaching economics is that it should be an interactive experience that exposes students to the core principles of decision making and economic growth, builds students' competence with quantitative and computational methods, and prepares them for future research or jobs. I believe students learn best when they address directly the ways in which economics is connected to underlying moral questions about how a society should be organized. This allows the student to build a better sense of how economics can illuminate policy debates or decision making with analysis techniques and quantitative methods. I then build on this understanding to increase students' mastery of the mathematical, theoretical and computational skills necessary to enable future use of economic models.\nThe reason I emphasize interactive experience is that I find relating abstract economic concepts to specific policy questions or real-life experiences dramatically increases the intuitive (and then eventually, methodological) skills a student can obtain. An example of this I used in the past is conducting game-theory based exercises in class in which students compete to have the highest semester-running score through a series of games. After each version of the game we play, I engage the students in a discussion of why they played the strategy they did and award extra points for describing how their strategy is based on the economic theory. I find that these discussions cement previously discussed lecture material very well. Another interactive method I use is conducting mock-negotiations or debates. For example, in a course I taught on the economics of climate change, each student represented a country and we held an international climate negotiation. Students immediately connected with this approach and were motivated enough to use relatively sophisticated integrated assessment models to support their negotiations with economic analysis. Both of these examples also used new communication and web technologies to improve interactivity and feedback."
  },
  {
    "objectID": "teaching/index.html#teaching-experience",
    "href": "teaching/index.html#teaching-experience",
    "title": "Justin Andrew Johnson",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nI was an adjunct professor and visiting lecturer at St. Olaf College, Minnesota, where I taught undergraduate courses, including Principles of Economics (which covered both microeconomics and macroeconomics) and the Economics of Climate Change. I successfully proposed teaching the latter course at St. Olaf College and prepared an entirely new curriculum of course materials that the economics department still uses. I had extremely positive student evaluations (see justinandrewjohnson.com/teaching for a comprehensive set of evaluation forms). In my role as Senior Scientist at The University of Minnesota, I also guest lectured many times for undergraduate, masters and Ph.D. level courses. In addition to traditional course instruction, I launched a short-course for faculty and staff at The Institute on the Environment entitled \"Python Programming and Big Data for Sustainability Science and Environmental Economics.\""
  },
  {
    "objectID": "teaching/index.html#new-course-development",
    "href": "teaching/index.html#new-course-development",
    "title": "Justin Andrew Johnson",
    "section": "New Course Development",
    "text": "New Course Development\nAs discussed above, I have experience creating and teaching new courses. I want to continue this, proposing the development of new courses. An example of this is proposing a new course on data science that uses the R statistical package and Python programming language to build new economic models based on large online data sources and satellite imagery (an extension of the course I proposed in my current position)."
  },
  {
    "objectID": "teaching/principles_of_microeconomics.html",
    "href": "teaching/principles_of_microeconomics.html",
    "title": "Principles of Microeconomics",
    "section": "",
    "text": "Lecture: Tuesday/Thursday from 1:30 pm - 2:45 pm in Ruttan Hall B35\nDiscussion:\n002 Friday, 9:35 am - 10:25 am in Ruttan Hall 143 003 Friday, 10:40 am - 11:30 am in Ruttan Hall 143 004 Friday, 11:45 am - 12:35 pm in Ruttan Hall 143 005 Friday, 12:50 pm - 1:40 pm in Ruttan Hall 143"
  },
  {
    "objectID": "teaching/principles_of_microeconomics.html#timelocation",
    "href": "teaching/principles_of_microeconomics.html#timelocation",
    "title": "Principles of Microeconomics",
    "section": "",
    "text": "Lecture: Tuesday/Thursday from 1:30 pm - 2:45 pm in Ruttan Hall B35\nDiscussion:\n002 Friday, 9:35 am - 10:25 am in Ruttan Hall 143 003 Friday, 10:40 am - 11:30 am in Ruttan Hall 143 004 Friday, 11:45 am - 12:35 pm in Ruttan Hall 143 005 Friday, 12:50 pm - 1:40 pm in Ruttan Hall 143"
  }
]